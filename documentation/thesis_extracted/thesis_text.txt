
ISLAMIC AZAD UNIVERSITYMASHHAD BRANCHFaculty of Engineering- Department of ComputerThesis for receiving «M.Sc.» degree on Artificial IntelligenceSubject:Distance Metric Learning for dimensionality reduction and performance improvement based on the structural neighborhoodsThesis Advisor:Mohammad Hossein Moattar Ph.D.Consulting Advisor:Yahya Forghani Ph.D.By:Mostafa Razavi GhodsSummer 2017ISLAMIC AZAD UNIVERSITYMASHHAD BRANCHFaculty of Engineering- Department of ComputerThesis for receiving «M.Sc.» degree on Artificial IntelligenceSubject:Distance Metric Learning for dimensionality reduction and performance improvement based on the structural neighborhoodsBy:Mostafa Razavi GhodsApproved by:Assoc. Prof. Dr. Mohammad Hossein Moattar:(Thesis Advisor)…………………………………………Assoc. Prof. Dr. Yahya Forghani:…………………………………………Assoc. Prof. Dr. Reza Godaz:…………………………………………AbstractDistance metric learning can be viewed as one of the fundamental interests in pattern recognition and machine learning, which plays a pivotal role on the performance of many learning methods. One of the effective methods in learning such a metric is to learn it from a set of labeled training samples. The issue of data imbalance is the most important challenge of the recent methods. This research tries not only to preserve the local structures but also covers the issue of imbalanced datasets. To do this, the proposed method first tries to extract a low dimensional manifold from the input data. Then, it learns the local neighborhood structures and the relationship of the data points in the ambient space based on the adjacencies of the same data points on the embedded low dimensional manifold. Using the local neighborhood relationships extracted from the manifold space, the proposed method learns the distance metric in a way which minimizes the distance between similar data and maximizes their distance from the dissimilar data points. The evaluations of the proposed method on numerous datasets from the UCI repository of machine learning, and also the KDDCup98 dataset as the most imbalance dataset, justify the supremacy of the proposed approach in comparison with other approaches especially when the imbalance factor is high. Keywords: Distance metric learning; Imbalanced data; Manifold learning; Mahalanobis distance; Locally Linear Embedding (LLE).Table of contents TOC \o "1-3" \h \z \u Abstract PAGEREF _Toc84811874 \h iiiTable of contents PAGEREF _Toc84811875 \h ivList of figures PAGEREF _Toc84811876 \h viiList of tables PAGEREF _Toc84811877 \h viiiChapter 1: Introduction PAGEREF _Toc84811878 \h 111.1Background and aims PAGEREF _Toc84811879 \h 111.2Statement of the problem PAGEREF _Toc84811880 \h 121.3Importance and necessity of doing this research PAGEREF _Toc84811881 \h 131.4A summary of the proposed method PAGEREF _Toc84811882 \h 141.5Thesis structure PAGEREF _Toc84811883 \h 14Chapter 2: Literature review PAGEREF _Toc84811884 \h 162.1Introduction PAGEREF _Toc84811885 \h 162.2Definitions of the problem and literature review (dimensionality reduction and manifold learning problems) PAGEREF _Toc84811886 \h 162.2.1Dimensionality reduction PAGEREF _Toc84811887 \h 162.3Main features of distance learning algorithms PAGEREF _Toc84811888 \h 172.3.1Learning scope PAGEREF _Toc84811889 \h 182.3.2Form of metric PAGEREF _Toc84811890 \h 182.3.3Dimensionality reduction PAGEREF _Toc84811891 \h 192.4A brief on distance metric learning PAGEREF _Toc84811892 \h 192.5Mahalanobis Distance Metric PAGEREF _Toc84811893 \h 192.6Distance metric learning algorithms PAGEREF _Toc84811894 \h 212.7Unsupervised distance metric learning approaches PAGEREF _Toc84811895 \h 212.7.1Principle Component Analysis (PCA) PAGEREF _Toc84811896 \h 222.7.2Nonlinear PCA PAGEREF _Toc84811897 \h 232.7.3Autoencoder PAGEREF _Toc84811898 \h 232.7.4Locality Preserving Projections (LPP) PAGEREF _Toc84811899 \h 252.7.5Laplacian Embedding (LE) PAGEREF _Toc84811900 \h 272.7.6Locally Linear embedding (LLE) PAGEREF _Toc84811901 \h 272.7.7Isometric feature mapping (Isomap) PAGEREF _Toc84811902 \h 282.8Supervised distance metric learning approaches PAGEREF _Toc84811903 \h 292.8.1Linear discriminant analysis (LDA) PAGEREF _Toc84811904 \h 292.8.2Relevant Component Analysis (RCA) PAGEREF _Toc84811905 \h 302.8.3Information Theoretic Metric Learning (ITML) PAGEREF _Toc84811906 \h 312.8.4Neighborhood Component Analysis (NCA) PAGEREF _Toc84811907 \h 322.8.5Discriminative Least Squares Regression (DLSR) PAGEREF _Toc84811908 \h 322.9Semi-supervised distance metric learning PAGEREF _Toc84811909 \h 342.9.1Laplacian Regularized Metric Learning (LRML) PAGEREF _Toc84811910 \h 342.9.2Constraint Margin Maximization (CRM) PAGEREF _Toc84811911 \h 352.10Summarization and conclusion of this chapter PAGEREF _Toc84811912 \h 36Chapter 3: Methodology PAGEREF _Toc84811913 \h 383.1Introduction PAGEREF _Toc84811914 \h 383.2Proposed method PAGEREF _Toc84811915 \h 383.2.1Data neighborhood structures after distance metric learning PAGEREF _Toc84811916 \h 403.3The objective of distance metric learning PAGEREF _Toc84811917 \h 423.4Advantages of the proposed method PAGEREF _Toc84811918 \h 43Chapter 4: Experiments PAGEREF _Toc84811919 \h 444.1Introduction PAGEREF _Toc84811920 \h 444.2Dataset PAGEREF _Toc84811921 \h 444.3Evaluation criteria PAGEREF _Toc84811922 \h 454.4Evaluation scenarios and experimental results PAGEREF _Toc84811923 \h 454.5Representation of the data after reduction PAGEREF _Toc84811924 \h 504.6Evaluations on the KDD data PAGEREF _Toc84811925 \h 52Chapter 5: Conclusion and Future Work PAGEREF _Toc84811926 \h 585.1Introduction PAGEREF _Toc84811927 \h 585.2Future work PAGEREF _Toc84811928 \h 58References PAGEREF _Toc84811929 \h 60Appendix PAGEREF _Toc84811930 \h 67Results of the SVM (the remainder) PAGEREF _Toc84811931 \h 67Sensitivity PAGEREF _Toc84811932 \h 67Specificity PAGEREF _Toc84811933 \h 69results of the k-NN+S PAGEREF _Toc84811934 \h 71Accuracy PAGEREF _Toc84811935 \h 71Sensitivity PAGEREF _Toc84811936 \h 74Specificity PAGEREF _Toc84811937 \h 76Comparison of the average runtime of the k-NN, k-NN+S, and SVM PAGEREF _Toc84811938 \h 78List of figures TOC \h \z \c "Figure" Figure ‎21. An example of Manifold learning while maintaining a structural neighborhood and manifold learning with lower dimensions embedded in the original space with higher dimensions  [35]. PAGEREF _Toc84773575 \h 17Figure ‎22. Diagram of an Autoencoder  [47]. PAGEREF _Toc84773576 \h 24Figure ‎31. Overall process of the proposed method. PAGEREF _Toc84773577 \h 39Figure ‎32. The local patch consisting of the dissimilar neighbors. PAGEREF _Toc84773578 \h 39Figure ‎33. The local neighborhoods after distance metric learning with the proposed approach. PAGEREF _Toc84773579 \h 40Figure ‎34. The representation of data points after manifold embedding and similarity calculation. PAGEREF _Toc84773580 \h 40Figure ‎41. Data distribution visualization after the reduction to a new 2D space using different approaches. PAGEREF _Toc84773581 \h 52Figure ‎01. Comparison of the averagre accuracy of the proposed method on 10-fold settings based on the neighborhoods on the manifold learnt by autoenocders and DLSR on the vehicle dataset using the k-NN+S classifier. PAGEREF _Toc84773582 \h 71List of tables TOC \h \z \c "Table" Table ‎21. Unspervised distance metric learning algorithms. PAGEREF _Toc84810028 \h 22Table ‎22. Supervised distnace metric learning algorithms. PAGEREF _Toc84810029 \h 29Table ‎23. Semi-supervised distance metric learning algorithms. PAGEREF _Toc84810030 \h 34Table ‎41. The properties of the datasets. PAGEREF _Toc84810031 \h 44Table ‎42. Accuracy comparison between different approaches versus the proposed using 10-fold cross validation and 7-NN classifier with (d,r) indicating the best latent dimensionality and the rank of the approach, respectively (AE denotes auto-encoder approach). PAGEREF _Toc84810032 \h 46Table ‎43. Sensitivity comparison between different approaches versus the proposed using 10-fold cross validation and 7-NN classifier with (d,r) indicating the best dimensionality and the rank of the approach, respectively (AE denotes auto-encoder approach). PAGEREF _Toc84810033 \h 47Table ‎44. Specificity comparison between different approaches versus the proposed using 10-fold cross validation and 7-NN classifier with (d,r) indicating the best dimensionality and the rank of the approach, respectively (AE denotes auto-encoder approach). PAGEREF _Toc84810034 \h 48Table ‎45. Accuracy comparison between different approaches versus the proposed using 10-fold cross validation and SVM classifier with (d,r) indicating the best latent dimensionality and the rank of the approach, respectively (AE denotes auto-encoder approach). PAGEREF _Toc84810035 \h 49Table ‎46. The average confusion matrix of the 10-fold cross validation using DLSR approach on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 28). PAGEREF _Toc84810036 \h 53Table ‎47. The average confusion matrix of the 10-fold cross validation using the proposed PCA+DSLR dimension reduction on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 10). PAGEREF _Toc84810037 \h 53Table ‎48. The average confusion matrix of the 10-fold cross validation using the proposed LDA+DLSR dimension reduction on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 28). PAGEREF _Toc84810038 \h 53Table ‎49. The average confusion matrix of the 10-fold cross validation using the proposed MDS+DLSR dimension reduction on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 1). PAGEREF _Toc84810039 \h 54Table ‎410. The average confusion matrix of the 10-fold cross validation using the proposed Isomap+DLSR dimension reduction on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 1). PAGEREF _Toc84810040 \h 54Table ‎411. The average confusion matrix of the 10-fold cross validation using the proposed LLE+DLSR dimension reduction on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 1). PAGEREF _Toc84810041 \h 54Table ‎412. The average confusion matrix of the 10-fold cross validation using the proposed KPCA+DLSR dimension reduction on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 1). PAGEREF _Toc84810042 \h 55Table ‎413. The average confusion matrix of the 10-fold cross validation using the proposed Autoencoder+DLSR approach on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 1). PAGEREF _Toc84810043 \h 55Table ‎414. The average confusion matrix of the 10-fold cross validation using the proposed Autoencoder+DLSR approach on KDD dataset using the SVM classifier for the best latent dimensionality (i.e. 9). PAGEREF _Toc84810044 \h 55Table ‎415. A comparison between the accuracy of the proposed method and some other recent works on different classes of the KDD based on the 10-fold cross validation. PAGEREF _Toc84810045 \h 57Table ‎01. Comparison of the results for the sensitivity criterion using the SVM classifier. PAGEREF _Toc84810046 \h 67Table ‎02. Ranking of the results for the sensitivity criterion based on the average of the 10-fold cross-validation. PAGEREF _Toc84810047 \h 68Table ‎03. Comparison of the results for the specificity criterion using the SVM classifier. PAGEREF _Toc84810048 \h 69Table ‎04. Ranking of the results for the specificity criterion based on the average of the 10-fold cross-validation. PAGEREF _Toc84810049 \h 70Table ‎05. Comparison of the results for the accuracy criterion using the 7-NN classifier. PAGEREF _Toc84810050 \h 72Table ‎06. Ranking of the results for the accuracy criterion based on the average of the 10-fold cross-validation using the 7-NN classifier. PAGEREF _Toc84810051 \h 73Table ‎07. Comparison of the results for the sensitivity criterion using the 7-NN classifier. PAGEREF _Toc84810052 \h 74Table ‎08. Ranking of the results for the sensitivity criterion based on the average of the 10-fold cross-validation using the 7-NN classifier. PAGEREF _Toc84810053 \h 75Table ‎09. Comparison of the results for the specificity criterion using the 7-NN classifier. PAGEREF _Toc84810054 \h 76Table ‎010. Ranking of the results for the specificity criterion based on the average of the 10-fold cross-validation using the 7-NN classifier. PAGEREF _Toc84810055 \h 77Table ‎011. Comparison of the average execution time per test data on 10-fold over different datasets between the three classification methods k-NN, k-NN + S, and SVM in the proposed similarity space. PAGEREF _Toc84810056 \h 78IntroductionBackground and aimsDistance metric learning (DML) for many years has been considered as one the main research interests in works which try to define the similarity and dissimilarity criteria between patterns. Distance metric learning approaches are employed to define an appropriate metric which can reflect the similarity and the dissimilarity of the data points with respect to the application in which they are used. The goal of distance metric learning is to find a real-valued metric function of data pairs under which the data pair with the same label are as close and the data pair from different classes are as far as possible. In this work, the main goal is to learn a function which can transform the input data onto the learned manifold with the least possible amount of changes in the relative distance of data-points from the same class  ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1073/pnas.0809777106","ISBN":"1091-6490 (Electronic)\\r0027-8424 (Linking)","ISSN":"00401951","PMID":"19342485","abstract":"The need for appropriate ways to measure the distance or similarity between data is ubiquitous in machine learning, pattern recognition and data mining, but handcrafting such good metrics for specific problems is generally difficult. This has led to the emergence of metric learning, which aims at automatically learning a metric from data and has attracted a lot of interest in machine learning and related fields for the past ten years. This survey paper proposes a systematic review of the metric learning literature, highlighting the pros and cons of each approach. We pay particular attention to Mahalanobis distance metric learning, a well-studied and successful framework, but additionally present a wide range of methods that have recently emerged as powerful alternatives, including nonlinear metric learning, similarity learning and local metric learning. Recent trends and extensions, such as semi-supervised metric learning, metric learning for histogram data and the derivation of generalization guarantees, are also covered. Finally, this survey addresses metric learning for structured data, in particular edit distance learning, and attempts to give an overview of the remaining challenges in metric learning for the years to come.","author":[{"dropping-particle":"","family":"Bellet","given":"Aurélien","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Habrard","given":"Amaury","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Sebban","given":"Marc","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"arXiv preprint arXiv:1306.6709","id":"ITEM-1","issued":{"date-parts":[["2013"]]},"note":"NULL","page":"57","title":"A Survey on Metric Learning for Feature Vectors and Structured Data","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=b8c018b9-4ef6-4b3d-9c36-41fd7acbb268"]}],"mendeley":{"formattedCitation":"[1]","plainTextFormattedCitation":"[1]","previouslyFormattedCitation":"[1]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[1].The application of the distance metric learning the in pattern recognition includes algorithms such as k-means, k-nearest neighbors and kernel-based algorithms such as support vector machines (SVMs) ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TPAMI.2002.1033219","author":[{"dropping-particle":"","family":"Domeniconi","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Peng","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Gunopulos","given":"D","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Pattern Analysis and Machine Intelligence","id":"ITEM-1","issue":"9","issued":{"date-parts":[["2002"]]},"note":"cited By 206","page":"1281-1285","title":"Locally adaptive metric nearest-neighbor classification","type":"article-journal","volume":"24"},"uris":["http://www.mendeley.com/documents/?uuid=d5d422c6-09cb-4c4f-a3e8-b523817a1d08"]},{"id":"ITEM-2","itemData":{"author":[{"dropping-particle":"","family":"Yang","given":"L","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Jin","given":"R","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Sukthankar","given":"R","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Liu","given":"Y","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Proceedings of the National Conference on Artificial Intelligence","id":"ITEM-2","issued":{"date-parts":[["2006"]]},"note":"cited By 59","page":"543-548","title":"An efficient algorithm for local distance metric learning","type":"paper-conference","volume":"1"},"uris":["http://www.mendeley.com/documents/?uuid=5f956f29-3e42-4d91-945e-d200aa163a36"]},{"id":"ITEM-3","itemData":{"author":[{"dropping-particle":"","family":"Domeniconi","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Gunopulos","given":"D","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-3","issued":{"date-parts":[["2002"]]},"note":"cited By 21","title":"Adaptive nearest neighbor classification using support vector machines","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=6c661769-afac-4dbc-ae2e-ce81c009d3f6"]},{"id":"ITEM-4","itemData":{"author":[{"dropping-particle":"","family":"Zhang","given":"Z","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Kwok","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Yeung","given":"D","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Proceedings of the IJCAI","id":"ITEM-4","issued":{"date-parts":[["0"]]},"note":"cited By 1","title":"Parametric distance metric learning with label information","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=7ec32585-79db-42d9-8294-fd4883ebdb49"]},{"id":"ITEM-5","itemData":{"DOI":"10.1109/TNN.2010.2042729","author":[{"dropping-particle":"","family":"Schneider","given":"P","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Bunte","given":"K","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Stiekema","given":"H","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Hammer","given":"B","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Villmann","given":"T","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Biehl","given":"M","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Neural Networks","id":"ITEM-5","issue":"5","issued":{"date-parts":[["2010"]]},"note":"cited By 39","page":"831-840","title":"Regularization in matrix relevance learning","type":"article-journal","volume":"21"},"uris":["http://www.mendeley.com/documents/?uuid=5d1ebe68-18b6-49f5-abb3-03b63137332e"]},{"id":"ITEM-6","itemData":{"DOI":"10.1109/TPAMI.2007.1096","author":[{"dropping-particle":"","family":"Tao","given":"D","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Li","given":"X","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Wu","given":"X","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Maybank","given":"S J","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Pattern Analysis and Machine Intelligence","id":"ITEM-6","issue":"10","issued":{"date-parts":[["2007"]]},"note":"cited By 730","page":"1700-1715","title":"General tensor discriminant analysis and Gabor features for gait recognition","type":"article-journal","volume":"29"},"uris":["http://www.mendeley.com/documents/?uuid=e5c5ca98-937f-4fc3-a396-9121441222e3"]},{"id":"ITEM-7","itemData":{"author":[{"dropping-particle":"","family":"Weinberger","given":"K Q","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Saul","given":"L K","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Proceedings of the 25th International Conference on Machine Learning","id":"ITEM-7","issued":{"date-parts":[["2008"]]},"note":"cited By 161","page":"1160-1167","title":"Fast solvers and efficient implementations for distance metric learning","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=c14a4df4-714f-41c6-a142-7369fce9a419"]},{"id":"ITEM-8","itemData":{"DOI":"10.1109/IJCNN.2011.6033542","author":[{"dropping-particle":"","family":"Mu","given":"Y","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Ding","given":"W","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Tao","given":"D","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Stepinski","given":"T F","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Proceedings of the International Joint Conference on Neural Networks","id":"ITEM-8","issued":{"date-parts":[["2011"]]},"note":"cited By 11","page":"2487-2494","title":"Biologically inspired model for crater detection","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=04630d83-da08-4351-a1be-65f441ee875f"]}],"mendeley":{"formattedCitation":"[2]–[9]","plainTextFormattedCitation":"[2]–[9]","previouslyFormattedCitation":"[2]–[9]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[2]–[9]. Distance metric learning approaches can be categorized into three classes of: fully-supervised, unsupervised, and semi supervised methods. In fully-supervised learning, the ultimate goal is to use the class discriminative information between the data-pairs in order to keep all data within a class as close and the data from different classes as diverse as possible. Zhang et al. ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TKDE.2008.212","author":[{"dropping-particle":"","family":"Zhang","given":"T","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Tao","given":"D","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Li","given":"X","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Yang","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Knowledge and Data Engineering","id":"ITEM-1","issue":"9","issued":{"date-parts":[["2009"]]},"note":"cited By 281","page":"1299-1313","title":"Patch alignment for dimensionality reduction","type":"article-journal","volume":"21"},"uris":["http://www.mendeley.com/documents/?uuid=e99f8ae3-847d-4f9e-96c5-299c6beb0964"]}],"mendeley":{"formattedCitation":"[10]","plainTextFormattedCitation":"[10]","previouslyFormattedCitation":"[10]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[10] have shown that learning the distance metric based on the class discriminative information usually shows better performance than using the classical Euclidean distance.Supervised distance metric learning itself could be divided into the two categories of local and global approaches. An approach is to learn a global distance metric from the training data in order to satisfy the constraints between all data-pairs simultaneously  ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Xing","given":"E P","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Ng","given":"A Y","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Jordan","given":"M I","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Russell","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2002"]]},"note":"cited By 774","title":"Distance metric learning, with application to clustering with side-information","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=0ad58555-0974-4518-8897-a90b6a5fdb84"]},{"id":"ITEM-2","itemData":{"author":[{"dropping-particle":"","family":"Zhang","given":"Z","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Kwok","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Yeung","given":"D","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Proceedings of the IJCAI","id":"ITEM-2","issued":{"date-parts":[["0"]]},"note":"cited By 1","title":"Parametric distance metric learning with label information","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=7ec32585-79db-42d9-8294-fd4883ebdb49"]}],"mendeley":{"formattedCitation":"[5], [11]","plainTextFormattedCitation":"[5], [11]","previouslyFormattedCitation":"[5], [11]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[5], [11]. The most expressive work in this field is Xing’s ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Xing","given":"E P","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Ng","given":"A Y","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Jordan","given":"M I","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Russell","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2002"]]},"note":"cited By 774","title":"Distance metric learning, with application to clustering with side-information","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=0ad58555-0974-4518-8897-a90b6a5fdb84"]}],"mendeley":{"formattedCitation":"[11]","plainTextFormattedCitation":"[11]","previouslyFormattedCitation":"[11]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[11] algorithm which learns a distance metric in the global scale where the distances between the data-pairs are in turn minimized and maximized under the equivalence and inequivalence constraints, respectively. Equivalence and inequivalence constraints may conflict when the data from different classes have multiple distributions. Thus, it is hard to satisfy the whole constrains in the global scale. In order to confront with this phenomenon, local distance metric learning approaches, which take account of the local constraints, are introduced ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Goldberger","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Roweis","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Hinton","given":"G","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Salakhutdinov","given":"R","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2005"]]},"note":"cited By 334","title":"Neighbourhood components analysis","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=cb615e11-7eb7-4344-a52c-40af61aebf1b"]},{"id":"ITEM-2","itemData":{"author":[{"dropping-particle":"","family":"Sugiyama","given":"M","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Journal of Machine Learning Research","id":"ITEM-2","issued":{"date-parts":[["2007"]]},"note":"cited By 580","page":"1027-1061","title":"Dimensionality reduction of multimodal labeled data by local fisher discriminant analysis","type":"article-journal","volume":"8"},"uris":["http://www.mendeley.com/documents/?uuid=40f06db5-ddad-4dd6-9e2c-81fec2128023"]},{"id":"ITEM-3","itemData":{"author":[{"dropping-particle":"","family":"Weinberger","given":"K Q","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Saul","given":"L K","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Journal of Machine Learning Research","id":"ITEM-3","issued":{"date-parts":[["2009"]]},"note":"cited By 983","page":"207-244","title":"Distance metric learning for large margin nearest neighbor classification","type":"article-journal","volume":"10"},"uris":["http://www.mendeley.com/documents/?uuid=59c6e0f9-47ae-4f28-8548-14c0eeeffc2d"]}],"mendeley":{"formattedCitation":"[12]–[14]","plainTextFormattedCitation":"[12]–[14]","previouslyFormattedCitation":"[12]–[14]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[12]–[14]. These local algorithms only consider the pairwise constraints while avoiding the conflicting ones.The aforementioned approaches try to present one single metric for all instances of the data. However, learning only single metric may have the deficiencies like: (1) is barely probable to find a metric appropriate for all the training data; (2) a local metric may not be immune to noisy data; (3) a local metric cannot be used in the multi-modal problems. Therefore, it is recommended to use different metrics for multiple distributions of the training data ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Domeniconi","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Gunopulos","given":"D","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2002"]]},"note":"cited By 21","title":"Adaptive nearest neighbor classification using support vector machines","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=6c661769-afac-4dbc-ae2e-ce81c009d3f6"]},{"id":"ITEM-2","itemData":{"DOI":"10.1109/34.506411","author":[{"dropping-particle":"","family":"Hastie","given":"T","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Tibshirani","given":"R","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Pattern Analysis and Machine Intelligence","id":"ITEM-2","issue":"6","issued":{"date-parts":[["1996"]]},"note":"cited By 502","page":"607-616","title":"Discriminant adaptive nearest neighbor classification","type":"article-journal","volume":"18"},"uris":["http://www.mendeley.com/documents/?uuid=9da1843e-3c63-4571-a0c1-f084fb433862"]},{"id":"ITEM-3","itemData":{"author":[{"dropping-particle":"","family":"Weinberger","given":"K Q","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Saul","given":"L K","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Journal of Machine Learning Research","id":"ITEM-3","issued":{"date-parts":[["2009"]]},"note":"cited By 983","page":"207-244","title":"Distance metric learning for large margin nearest neighbor classification","type":"article-journal","volume":"10"},"uris":["http://www.mendeley.com/documents/?uuid=59c6e0f9-47ae-4f28-8548-14c0eeeffc2d"]}],"mendeley":{"formattedCitation":"[4], [14], [15]","plainTextFormattedCitation":"[4], [14], [15]","previouslyFormattedCitation":"[4], [14], [15]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[4], [14], [15].Generally, supervised distance metric learning could be divided into the two groups of local and global approaches. The local methods could also be subcategorized into the single-metric and multiple-metric approaches. The global methods try to keep the similar samples as close and dissimilar samples as far as possible. Xing’s algorithm ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Xing","given":"E P","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Ng","given":"A Y","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Jordan","given":"M I","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Russell","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2002"]]},"note":"cited By 774","title":"Distance metric learning, with application to clustering with side-information","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=0ad58555-0974-4518-8897-a90b6a5fdb84"]}],"mendeley":{"formattedCitation":"[11]","plainTextFormattedCitation":"[11]","previouslyFormattedCitation":"[11]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[11] is a good representative of global approaches which optimizes some equal and inequality constraints simultaneously using the convex optimization methods.The advantage of using the global approaches is in their ability to capture the distributions from different classes when all the samples belonging to the same class do not obey the same distribution. However, the global approaches may not be able to learn the optimal distance metric when data have multimodal distributions.Local approaches use the neighborhood information to cope with the multimodal distribution problems. Local Fisher Discriminant Analysis (LFDA) ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Sugiyama","given":"M","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Journal of Machine Learning Research","id":"ITEM-1","issued":{"date-parts":[["2007"]]},"note":"cited By 580","page":"1027-1061","title":"Dimensionality reduction of multimodal labeled data by local fisher discriminant analysis","type":"article-journal","volume":"8"},"uris":["http://www.mendeley.com/documents/?uuid=40f06db5-ddad-4dd6-9e2c-81fec2128023"]}],"mendeley":{"formattedCitation":"[13]","plainTextFormattedCitation":"[13]","previouslyFormattedCitation":"[13]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[13], according to the local information, give more weight to the pairwise neighborhood constraints. Yang et al. ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Yang","given":"L","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Jin","given":"R","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Sukthankar","given":"R","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Liu","given":"Y","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Proceedings of the National Conference on Artificial Intelligence","id":"ITEM-1","issued":{"date-parts":[["2006"]]},"note":"cited By 59","page":"543-548","title":"An efficient algorithm for local distance metric learning","type":"paper-conference","volume":"1"},"uris":["http://www.mendeley.com/documents/?uuid=5f956f29-3e42-4d91-945e-d200aa163a36"]}],"mendeley":{"formattedCitation":"[3]","plainTextFormattedCitation":"[3]","previouslyFormattedCitation":"[3]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[3], proposed a probabilistic approach to optimize the local pairwise constraints. Goldberger et al. ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Goldberger","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Roweis","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Hinton","given":"G","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Salakhutdinov","given":"R","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2005"]]},"note":"cited By 334","title":"Neighbourhood components analysis","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=cb615e11-7eb7-4344-a52c-40af61aebf1b"]}],"mendeley":{"formattedCitation":"[12]","plainTextFormattedCitation":"[12]","previouslyFormattedCitation":"[12]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[12] utilized a stochastic variant of the KNN classifier to calculate the leave-one-out classification error.Statement of the problemDimensionality reduction (DR) approaches try to find a low dimensional representation of the data in order to satisfy some goals. Size reduction of the feature vectors for data compression (from the unsupervised perspective) as well as avoiding the curse of dimensionality (from the supervised view) are two main objectives of the dimensionality reduction approaches. However, problems happen when the number of data-points is not sufficient to cover the whole initial high dimensional space. Data visualization is one other goal of the DR approaches, in which the DR is used to project the high dimensional data onto a space with at most two or three dimensions in a way which is comprehensible and visualizable. In data classification application, the DR methods could be used to find a low-dimensional manifold on which the data with the same label are compact, while the data from different classes are discriminant with respect to each other, which itself improves the classification accuracy.DML has been one of the fields of research for many years, the main purpose of which is to properly define the criteria of similarity (or dissimilarity). Thus, DML methods are used to define an appropriate distance metric that reflects the similarity or dissimilarity in each application. The goal of DML is to find a metric function of pairs of data with real values ​​in such a way that the data points with the same label be as close to each other and the data with the dissimilar labels become as separated from each other as possible. Here, the goal is to learn a transformation, that is capable of mapping the data points to the manifold space with the least change in the locality and neighborhood of the points with respect to each other.Importance and necessity of doing this researchThe proposed method in this research tries to cover three of the challenges and gaps in distance learning standard methods. Therefore, the importance and necessity of this research will be in three ways:1.Neighborhood Preservation: In this research, we try to learn the distance metric in such a way that the locality as well as local neighborhoods between similar points are preserved as much as possible and only dissimilar data pairs get separated from each other.2.Unbalanced data: The proposed method in this research is such that distance learning is done in such a way that for each data point the number of similar and dissimilar data points are equal.3.Independence of the problem: The proposed method tries to do the DML in such a way that can be used for any application, whether learning with supervision or without supervision.In this research, the proposed method tries to cover a triple of the challenges in the distance metric learning dimensionality reduction. This research is important as it tries to learn the distance metric in such a way that after transformation, which is done by the learned metric function, the data from the same class are as close and the data from different classes are as far from each other as possible. Besides, nowadays many of the real-world datasets are found to be imbalanced in terms of the number the points associated to different classes. Thus, the proposed method tries to learn the distance metric with respect to this phenomenon. Furthermore, one other goal of the proposed method is to learn the distance metric in a way that it could be used in any application, independent from the presence or absence of the labeling information.A summary of the proposed methodIn this study, we have attempted to learn a low-dimensional manifold out of the data in the initial space. Then, similar, dissimilar and irrelevant data-points are found based their local neighborhood on the manifold. Consequently, based on these neighborhood relationships which are found on the manifold and based the coordinates of the data points on the initial space, distance metric learning is done using a Mahalanobis distance metric learning approach.Thesis structureThe remainder of this thesis is organized as follows. Chapter 2 primarily deals with the concept of distance metric learning and dimensionality reduction followed by some discussion on the different manifold learning approaches. The proposed method will be introduced in Chapter 3. Chapter 4 describes the experimental setup and analyzes its performance and summarizes its results and finally, Chapter 5 discusses the main findings and concludes this study besides giving some directions for future studies.Literature reviewIntroductionDistance metric plays a key role in the success of many machine learning algorithms. For example, the classification techniques such as the k-nearest neighbors (k-NN) ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TIT.1967.1053964","author":[{"dropping-particle":"","family":"Cover","given":"T M","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Hart","given":"P E","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Information Theory","id":"ITEM-1","issue":"1","issued":{"date-parts":[["1967"]]},"note":"cited By 4627","page":"21-27","title":"Nearest Neighbor Pattern Classification","type":"article-journal","volume":"13"},"uris":["http://www.mendeley.com/documents/?uuid=29b39867-1ecb-4665-829b-16e9a46fdb5c"]}],"mendeley":{"formattedCitation":"[16]","plainTextFormattedCitation":"[16]","previouslyFormattedCitation":"[16]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[16] and the clustering approaches like k-means algorithm are highly dependent on the applied metric in order to model the structural models between the input data. A tangible example in this field could be the visual object recognition problem. Lots of the applications in machine learning could be considered as implicit distance metric learning approaches which are capable of learning the similarities and dissimilarities between visual input objects. This typical example includes classification ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/ICCV.2005.171","author":[{"dropping-particle":"","family":"Winn","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Criminisi","given":"A","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Minka","given":"T","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Proceedings of the IEEE International Conference on Computer Vision","id":"ITEM-1","issued":{"date-parts":[["2005"]]},"note":"cited By 442","page":"1800-1807","title":"Object categorization by learned universal visual dictionary","type":"paper-conference","volume":"II"},"uris":["http://www.mendeley.com/documents/?uuid=2d8bb881-4f78-4758-adbf-190e47bbea43"]}],"mendeley":{"formattedCitation":"[17]","plainTextFormattedCitation":"[17]","previouslyFormattedCitation":"[17]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[17] and  Content-based image retrieval ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/34.895972","author":[{"dropping-particle":"","family":"Smeulders","given":"A W M","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Worring","given":"M","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Santini","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Gupta","given":"A","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Jain","given":"R","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Pattern Analysis and Machine Intelligence","id":"ITEM-1","issue":"12","issued":{"date-parts":[["2000"]]},"note":"cited By 4027","page":"1349-1380","title":"Content-based image retrieval at the end of the early years","type":"article-journal","volume":"22"},"uris":["http://www.mendeley.com/documents/?uuid=4c0b403b-697f-41b1-b160-7198ec5bcc58"]}],"mendeley":{"formattedCitation":"[18]","plainTextFormattedCitation":"[18]","previouslyFormattedCitation":"[18]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[18] where a distance metric is required to discriminate between different classes of related or irrelevant objects.In this chapter we will touch upon some basic ideas about the distance metric learning and dimensionality reduction approaches. Then we will discuss about some of the most promising approaches in this discipline and finally we will conclude this chapter with a short review on each of the triple of the distance metric learning approaches as supervised, unsupervised and the semi-supervised.Definitions of the problem and literature review (dimensionality reduction and manifold learning problems)Dimensionality reductionDimensionality reduction or feature extraction has been widely used in data mining, computer vision, and pattern recognition ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TNNLS.2015.2422994","ISBN":"2013032915202","ISSN":"2162-237X","author":[{"dropping-particle":"","family":"Lai","given":"Zhihui","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Wong","given":"Wai Keung","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Xu","given":"Yong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Yang","given":"Jian","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"David","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Neural Networks and Learning Systems","id":"ITEM-1","issue":"4","issued":{"date-parts":[["2016","4"]]},"page":"723-735","title":"Approximate Orthogonal Sparse Embedding for Dimensionality Reduction","type":"article-journal","volume":"27"},"uris":["http://www.mendeley.com/documents/?uuid=460f8b04-f717-466d-9470-677eb86dcb81"]}],"mendeley":{"formattedCitation":"[19]","plainTextFormattedCitation":"[19]","previouslyFormattedCitation":"[19]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[19]. Classical dimensionality reduction methods, such as the Principal Component Analysis (PCA) ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1162/jocn.1991.3.1.71","ISSN":"0898-929X","author":[{"dropping-particle":"","family":"Turk","given":"Matthew","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Pentland","given":"Alex","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Journal of Cognitive Neuroscience","id":"ITEM-1","issue":"1","issued":{"date-parts":[["1991","1"]]},"page":"71-86","title":"Eigenfaces for Recognition","type":"article-journal","volume":"3"},"uris":["http://www.mendeley.com/documents/?uuid=471739ec-2b2b-48dd-8995-3d23e5fc95ae"]},{"id":"ITEM-2","itemData":{"DOI":"10.1109/34.41390","ISBN":"0162-8828 VO - 12","ISSN":"01628828","abstract":"The use of natural symmetries (mirror images) in a well-defined\\nfamily of patterns (human faces) is discussed within the framework of\\nthe Karhunen-Loeve expansion. This results in an extension of the data\\nand imposes even and odd symmetry on the eigenfunctions of the\\ncovariance matrix, without increasing the complexity of the calculation.\\nThe resulting approximation of faces projected from outside of the data\\nset onto this optimal basis is improved on average","author":[{"dropping-particle":"","family":"Kirby","given":"M.","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Sirovich","given":"L.","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Pattern Analysis and Machine Intelligence","id":"ITEM-2","issue":"1","issued":{"date-parts":[["1990"]]},"page":"103-108","title":"Application of the Karhunen-Loeve procedure for the characterization of human faces","type":"article-journal","volume":"12"},"uris":["http://www.mendeley.com/documents/?uuid=22f9f174-c4b9-491d-90d1-be8dd9bd4543"]},{"id":"ITEM-3","itemData":{"DOI":"10.1364/JOSAA.4.000519","ISBN":"doi:10.1364/JOSAA.4.000519","ISSN":"1084-7529","PMID":"3572578","abstract":"A method is presented for the representation of (pictures of) faces. Within a specified framework the representation is ideal. This results in the characterization of a face, to within an error bound, by a relatively low-dimensional vector. The method is illustrated in detail by the use of an ensemble of pictures taken for this purpose.","author":[{"dropping-particle":"","family":"Sirovich","given":"L","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Kirby","given":"M","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Journal of the Optical Society of America. A, Optics and image science","id":"ITEM-3","issue":"3","issued":{"date-parts":[["1987"]]},"page":"519-524","title":"Low-dimensional procedure for the characterization of human faces.","type":"article-journal","volume":"4"},"uris":["http://www.mendeley.com/documents/?uuid=f92d4191-ed0e-43ca-b449-3d0f55d1a32f"]}],"mendeley":{"formattedCitation":"[20]–[22]","plainTextFormattedCitation":"[20]–[22]","previouslyFormattedCitation":"[20]–[22]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[20]–[22] as well as Linear Resolution Analysis (LDA) ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TCSVT.2003.818352","ISSN":"1051-8215","abstract":"This work is a continuation and extension of our previous research where Kernel Fisher Discriminant Analysis (KFDA), a combination of the kernel trick with Fisher Linear Discriminant Analysis (FLDA), was introduced to represent facial features for face recognition. The main contributions of this work are three-fold to further improve the performance of KFDA. First, a new kernel function, called the Cosine kernel, is proposed to increase the discriminating capability of the original polynomial kernel function. Second, a geometry-based feature vector selection scheme is adopted to reduce the computational complexity of KFDA. Third, a variant of the nearest feature line classifier is employed to further enhance the recognition performance as it can produce virtual samples to make up for the shortage of training samples. Experiments are carried out on a mixed database with 125 persons and 970 images and demonstrate the effectiveness of improvements.","author":[{"dropping-particle":"","family":"Liu","given":"Qingshan","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Lu","given":"Hanqing","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Ma","given":"Songde","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Circuits and Systems","id":"ITEM-1","issue":"1","issued":{"date-parts":[["2004"]]},"page":"42-49","title":"Improving Kernel Fisher Discriminant Analysis for Face Recognition","type":"article-journal","volume":"14"},"uris":["http://www.mendeley.com/documents/?uuid=c40e9edc-83b7-4fbb-b7ec-1f4ef07ce801"]},{"id":"ITEM-2","itemData":{"DOI":"10.1198/016214502760047131","ISBN":"0162-1459","ISSN":"0162-1459","PMID":"20815477","abstract":"Cluster analysis is the automated search for groups of related observations in a dataset. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures, and most clustering methods available in commercial software are also of this ... \\n","author":[{"dropping-particle":"","family":"Fraley","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Raftery","given":"a E","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Journal of the American Statistical Association","id":"ITEM-2","issue":"458","issued":{"date-parts":[["2002"]]},"page":"611-631","title":"Model-based clustering, discriminant analysis, and density estimation","type":"article-journal","volume":"97"},"uris":["http://www.mendeley.com/documents/?uuid=bc949f76-3d23-46ff-b033-9be93580e6d9"]},{"id":"ITEM-3","itemData":{"DOI":"10.1007/BFb0015522","author":[{"dropping-particle":"","family":"Belhumeur","given":"P","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Hespanha","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Kriegman","given":"D","non-dropping-particle":"","parse-names":false,"suffix":""}],"id":"ITEM-3","issue":"7","issued":{"date-parts":[["1997"]]},"page":"711-720","title":"Eigenfaces Vs. Fisherfaces: Recognition Using Class Specific Linear Projection","type":"article-journal","volume":"19"},"uris":["http://www.mendeley.com/documents/?uuid=e47ac72d-95ed-4ab4-a228-eb26f92ddfaf"]}],"mendeley":{"formattedCitation":"[23]–[25]","plainTextFormattedCitation":"[23]–[25]","previouslyFormattedCitation":"[23]–[25]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[23]–[25] and their modified methods ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TNNLS.2012.2234134","ISSN":"2162-237X","author":[{"dropping-particle":"","family":"Jun Li","given":"","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Dacheng Tao","given":"","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Neural Networks and Learning Systems","id":"ITEM-1","issue":"3","issued":{"date-parts":[["2013","3"]]},"page":"485-497","title":"Simple Exponential Family PCA","type":"article-journal","volume":"24"},"uris":["http://www.mendeley.com/documents/?uuid=482274e4-d4b0-4937-ad89-549b499af82b"]},{"id":"ITEM-2","itemData":{"DOI":"10.1109/TPAMI.2008.70","author":[{"dropping-particle":"","family":"Tao","given":"Dacheng","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Li","given":"Xuelong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Member","given":"Senior","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Wu","given":"Xindong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Member","given":"Senior","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"TIANJIN UNIVERSITY. DOWNLOADED ON DECEMBER 8, 2009 AT 04:33 FROM IEEE XPLORE. RESTRICTIONS APPLY. YUAN ET AL.: BINARY SPARSE NONNEGATIVE MATRIX FACTORIZATION 777","id":"ITEM-2","issue":"2","issued":{"date-parts":[["2009"]]},"page":"260-274","title":"Geometric Mean for Subspace Selection","type":"article-journal","volume":"31"},"uris":["http://www.mendeley.com/documents/?uuid=17f7dac8-e329-3f72-90c7-6afc35a58246"]},{"id":"ITEM-3","itemData":{"DOI":"10.1109/TIP.2012.2211372","ISSN":"10577149","PMID":"22875249","abstract":"Principal component analysis (PCA) computes a succinct data representation by converting the data to a few new variables while retaining maximum variation. However, the new variables are difficult to interpret, because each one is combined with all of the original input variables and has obscure semantics. Under the umbrella of Bayesian data analysis, this paper presents a new prior to explicitly regularize combinations of input variables. In particular, the prior penalizes pair-wise products of the coefficients of PCA and encourages a sparse model. Compared to the commonly used l1regularizer, the proposed prior encourages the sparsity pattern in the resultant coefficients to be consistent with the intrinsic groups in the original input variables. Moreover, the proposed prior can be explained as recovering a robust estimation of the covariance matrix for PCA. The proposed model is suited for analyzing visual data, where it encourages the output variables to correspond to meaningful parts in the data. We demonstrate the characteristics and effectiveness of the proposed technique through experiments on both synthetic and real data.","author":[{"dropping-particle":"","family":"Li","given":"Jun","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Tao","given":"Dacheng","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Image Processing","id":"ITEM-3","issue":"12","issued":{"date-parts":[["2012"]]},"page":"4830-4843","title":"On preserving original variables in Bayesian PCA with application to image analysis","type":"article-journal","volume":"21"},"uris":["http://www.mendeley.com/documents/?uuid=c6090e67-c50d-4c40-b290-ed44c450789d"]}],"mendeley":{"formattedCitation":"[26]–[28]","plainTextFormattedCitation":"[26]–[28]","previouslyFormattedCitation":"[26]–[28]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[26]–[28], are simple and effective and have been widely used in various fields such as face recognition, palm recognition, etc. However, classical methods such as PCA and LDA focus only on the global structure of a dataset in order to reduce the dimensionality.Roweis and Saul ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1126/science.290.5500.2323","ISBN":"00368075","ISSN":"00368075","PMID":"11125150","author":[{"dropping-particle":"","family":"Roweis","given":"S. T.","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Science","id":"ITEM-1","issue":"5500","issued":{"date-parts":[["2000","12","22"]]},"page":"2323-2326","title":"Nonlinear Dimensionality Reduction by Locally Linear Embedding","type":"article-journal","volume":"290"},"uris":["http://www.mendeley.com/documents/?uuid=af9f1543-af3f-46b7-98b9-b00224ecee28"]}],"mendeley":{"formattedCitation":"[29]","plainTextFormattedCitation":"[29]","previouslyFormattedCitation":"[29]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[29] and Tenenbaum et al. ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1126/science.290.5500.2319","ISSN":"00368075","author":[{"dropping-particle":"","family":"Tenenbaum","given":"Joshua B","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Science","id":"ITEM-1","issue":"5500","issued":{"date-parts":[["2000","12","22"]]},"page":"2319-2323","title":"A Global Geometric Framework for Nonlinear Dimensionality Reduction","type":"article-journal","volume":"290"},"uris":["http://www.mendeley.com/documents/?uuid=02b734f1-8992-4f7e-b47f-a816f01489eb"]}],"mendeley":{"formattedCitation":"[30]","plainTextFormattedCitation":"[30]","previouslyFormattedCitation":"[30]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[30] state that the images of different objects are placed on a low-dimensional manifold that is laid in a high-dimensional space. To identify the intrinsic geometry of a data set, manifold learning methods have been widely used, the most popular of which are LLE, ISOMAP, and Laplacian eigenmap ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1162/089976603321780317","ISBN":"0899-7667","ISSN":"0899-7667","PMID":"12816577","abstract":"One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed.","author":[{"dropping-particle":"","family":"Belkin","given":"Mikhail","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Niyogi","given":"Partha","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Neural Computation","id":"ITEM-1","issue":"6","issued":{"date-parts":[["2003"]]},"page":"1373-1396","title":"Laplacian Eigenmaps for Dimensionality Reduction and Data Representation","type":"article-journal","volume":"15"},"uris":["http://www.mendeley.com/documents/?uuid=42a9d17c-7f95-4951-ad5d-427c88d2fea1"]}],"mendeley":{"formattedCitation":"[31]","plainTextFormattedCitation":"[31]","previouslyFormattedCitation":"[31]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[31]. Based on these nonlinear methods, many linear dimensional reduction methods based on manifold learning for feature extraction have been proposed. Among these methods, Neighborhood Preserving Embedding (NPE) ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/ICCV.2005.167","ISBN":"0-7695-2334-X","ISSN":"1550-5499","abstract":"Recently there has been a lot of interest in geometrically motivated approaches to data analysis in high dimensional spaces. We consider the case where data is drawn from sampling a probability distribution that has support on or near a submanifold of Euclidean space. In this paper, we propose a novel subspace learning algorithm called neighborhood preserving embedding (NPE). Different from principal component analysis (PCA) which aims at preserving the global Euclidean structure, NPE aims at preserving the local neighborhood structure on the data manifold. Therefore, NPE is less sensitive to outliers than PCA. Also, comparing to the recently proposed manifold learning algorithms such as Isomap and locally linear embedding, NPE is defined everywhere, rather than only on the training data points. Furthermore, NPE may be conducted in the original space or in the reproducing kernel Hilbert space into which data points are mapped. This gives rise to kernel NPE. Several experiments on face database demonstrate the effectiveness of our algorithm","author":[{"dropping-particle":"","family":"Xiaofei He","given":"","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Deng Cai","given":"","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Shuicheng Yan","given":"","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Hong-Jiang Zhang","given":"","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1","id":"ITEM-1","issued":{"date-parts":[["2005"]]},"page":"1208-1213 Vol. 2","title":"Neighborhood preserving embedding","type":"article-journal","volume":"2"},"uris":["http://www.mendeley.com/documents/?uuid=9830bbb2-ccfe-41e1-9361-1cf6bf5a5b12"]}],"mendeley":{"formattedCitation":"[32]","plainTextFormattedCitation":"[32]","previouslyFormattedCitation":"[32]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[32], Orthogonal Neighborhood-Preserving Projection (ONPP) ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TPAMI.2007.1131","ISBN":"0769522785","ISSN":"01628828","PMID":"17934224","abstract":"This paper considers the problem of dimensionality reduction by orthogonal projection techniques. The main feature of the proposed techniques is that they attempt to preserve both the intrinsic neighborhood geometry of the data samples and the global geometry. In particular we propose a method, named Orthogonal Neighborhood Preserving Projections, which works by first building an \"affinity\" graph for the data, in a way that is similar to the method of Locally Linear Embedding (LLE). However, in contrast with the standard LLE where the mapping between the input and the reduced spaces is implicit, ONPP employs an explicit linear mapping between the two. As a result, handling new data samples becomes straightforward, as this amounts to a simple linear transformation. We show how to define kernel variants of ONPP, as well as how to apply the method in a supervised setting. Numerical experiments are reported to illustrate the performance of ONPP and to compare it with a few competing methods.","author":[{"dropping-particle":"","family":"Kokiopoulou","given":"Effrosyni","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Saad","given":"Yousef","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Pattern Analysis and Machine Intelligence","id":"ITEM-1","issue":"12","issued":{"date-parts":[["2007"]]},"page":"2143-2156","title":"Orthogonal neighborhood preserving projections: A projection-based dimensionality reduction technique","type":"article-journal","volume":"29"},"uris":["http://www.mendeley.com/documents/?uuid=ff1b1dcc-59ca-4de3-a3a4-2cebed624a31"]}],"mendeley":{"formattedCitation":"[33]","plainTextFormattedCitation":"[33]","previouslyFormattedCitation":"[33]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[33], Locality Preserving Projections (LPP) ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1.1.19.9400","ISBN":"0262201526","ISSN":"10495258","PMID":"25246403","abstract":"Many problems in information processing involve some form of dimen- sionality reduction. In this paper, we introduce Locality Preserving Pro- jections (LPP). These are linear projective maps that arise by solving a variational problem that optimally preserves the neighborhood structure of the data set. LPP should be seen as an alternative to Principal Com- ponent Analysis (PCA) – a classical linear technique that projects the data along the directions of maximal variance. When the high dimen- sional data lies on a low dimensional manifold embedded in the ambient space, the Locality Preserving Projections are obtained by finding the optimal linear approximations to the eigenfunctions of the Laplace Bel- trami operator on the manifold. As a result, LPP shares many of the data representation properties of nonlinear techniques such as Laplacian Eigenmaps or Locally Linear Embedding. Yet LPP is linear and more crucially is defined everywhere in ambient space rather than just on the training data points. This is borne out by illustrative examples on some high dimensional data sets.","author":[{"dropping-particle":"","family":"He","given":"Xiaofei","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Niyogi","given":"Partha","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Neural information processing systems","id":"ITEM-1","issued":{"date-parts":[["2004"]]},"page":"153","title":"Locality preserving projections","type":"article-journal","volume":"16"},"uris":["http://www.mendeley.com/documents/?uuid=d18376d1-e0ab-4ce0-85de-a19086d716be"]}],"mendeley":{"formattedCitation":"[34]","plainTextFormattedCitation":"[34]","previouslyFormattedCitation":"[34]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[34] can be considered as methods that preserve the local geometric structure of the data and map them onto the manifold space by applying a simple linear estimation to maintain nonlinear maps.Figure  STYLEREF 1 \s ‎2 SEQ Figure \* ARABIC \s 1 1. An example of Manifold learning while maintaining a structural neighborhood and manifold learning with lower dimensions embedded in the original space with higher dimensions  ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"URL":"https://www.eecis.udel.edu/~zhou/Research.html","accessed":{"date-parts":[["2017","10","8"]]},"id":"ITEM-1","issued":{"date-parts":[["0"]]},"title":"Yin Zhou's Home Page @ University of Delaware","type":"webpage"},"uris":["http://www.mendeley.com/documents/?uuid=a791f47d-c089-3990-8647-1e072dcf5ad6"]}],"mendeley":{"formattedCitation":"[35]","plainTextFormattedCitation":"[35]","previouslyFormattedCitation":"[35]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[35].LLE-based methods have been developed in a variety of forms and thanks to their simplicity are widely usage in a variety of applications such as facial expression recognition, image prediction and retrieval, feature fusion, face recognition, gait recognition, human motion recognition, etc. ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TNNLS.2015.2422994","ISBN":"2013032915202","ISSN":"2162-237X","author":[{"dropping-particle":"","family":"Lai","given":"Zhihui","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Wong","given":"Wai Keung","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Xu","given":"Yong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Yang","given":"Jian","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"David","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Neural Networks and Learning Systems","id":"ITEM-1","issue":"4","issued":{"date-parts":[["2016","4"]]},"page":"723-735","title":"Approximate Orthogonal Sparse Embedding for Dimensionality Reduction","type":"article-journal","volume":"27"},"uris":["http://www.mendeley.com/documents/?uuid=460f8b04-f717-466d-9470-677eb86dcb81"]}],"mendeley":{"formattedCitation":"[19]","plainTextFormattedCitation":"[19]","previouslyFormattedCitation":"[19]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[19].Main features of distance learning algorithmsWith the exception of some basic methods, most distance learning algorithms are essentially competitive in such a way that they can achieve the state-of-the-art performance in some areas. However, each algorithm has its own inherent characteristics (such as, type of metric, ability to use unsupervised data, scalability in different dimensions, guarantee of generalization, etc.) and on the decision to choose a method that fits the type of the problem given should be considered with special attention. In this section, we examine three important features of distance learning algorithms:Learning scopeIn this section, we examine three important features of distance learning algorithms:Fully Supervised: the distance learning algorithm has full access to the training data set zi=xi, yii=1n, where each training sample zi∈Z=x×y  is consisted of an instance xi∈X and a label (or class) yi∈Y.  Y is a discrete and finite set of labels |Y| (unless noted otherwise). In practice, label information is commonly used to construct specific sets of pair/triplet constraints S,D,R, for example based on the notion of neighborhood.Weakly supervised: the algorithm has no access to the label information of any training data and only has information about constraints S, D, R. That is a meaningful approach in many applications where obtaining labeling data is costly while ancillary information is inexpensive. For example, implicit user feedback (such as clicking on search engine results), referrals between articles, or links within a network.Semi-supervised: in addition to fully and semi-supervised methods, the algorithm has access to a large number of unlabeled instances for which no additional information is available. This learning method is useful to avoid over-fitting when tagged information or ancillary information are very limited.Form of metricClearly, the form of the learned metric is a key choice. The three main families of distance learning metrics are:Linear metrics, such as the Mahalanobis distance. Their expressive power is limited, but they are easier to optimize (they often lead to convex formulation, and as a result the global optimization of the solution) and are less likely to over-fit.Nonlinear metrics: such as the X2 histogram distance. They usually lead to nonconvex formulations (due to the local optimalities) as well as overfitting, but they can maintain nonlinear variations in the data.Local metrics: in which several local metrics (linear or nonlinear, usually simultaneously) are learned to deal with complex issues such as heterogeneous data. However, they are more prone to over-fitting than global methods as the number of parameters they have to learn can be very large.Dimensionality reductionAs mentioned earlier, distance metric learning is in some cases formulated as a visualization of data into a new feature space. An attractive achievement in this case is finding lower-dimensionally projected space, which allows us to perform calculations faster as well as more compact representations of them. This is usually done by adjusting the distance matrix of the learned distance to a lower rank.A brief on distance metric learningAlthough the roots of distance metric learning can be traced back to some previous work (e.g. ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1016/C2009-0-27872-X","ISBN":"9780080478654","author":[{"dropping-particle":"","family":"Keinosuke Fukunaga","given":"","non-dropping-particle":"","parse-names":false,"suffix":""}],"id":"ITEM-1","issued":{"date-parts":[["1990"]]},"publisher":"Elsevier","title":"Introduction to Statistical Pattern Recognition","type":"book"},"uris":["http://www.mendeley.com/documents/?uuid=1aaf085e-b821-40b6-869f-1343134c44ea"]},{"id":"ITEM-2","itemData":{"author":[{"dropping-particle":"","family":"Short","given":"R","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Fukunaga","given":"Keinosuke","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE transactions on Information Theory","id":"ITEM-2","issue":"5","issued":{"date-parts":[["1981"]]},"page":"622-627","publisher":"IEEE","title":"The optimal distance measure for nearest neighbor classification","type":"article-journal","volume":"27"},"uris":["http://www.mendeley.com/documents/?uuid=1b84ab05-9c87-4e52-b826-c6222cbfb174"]}],"mendeley":{"formattedCitation":"[36], [37]","plainTextFormattedCitation":"[36], [37]","previouslyFormattedCitation":"[36], [37]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[36], [37]), distance learning was actually emerged in 2002 by the pioneering work of Xing et al. ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Xing","given":"E P","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Ng","given":"A Y","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Jordan","given":"M I","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Russell","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2002"]]},"note":"cited By 774","title":"Distance metric learning, with application to clustering with side-information","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=0ad58555-0974-4518-8897-a90b6a5fdb84"]}],"mendeley":{"formattedCitation":"[11]","plainTextFormattedCitation":"[11]","previouslyFormattedCitation":"[11]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[11] that formulates as a convex optimization problem.The purpose of learning the distance criterion of matching a function with real values, for example the distance Mahalanobis dMx, x'=x-x'TMx-x', with the problem using information that They are created by educational data. Most methods learn the distance criterion (which here is the positive semi-definite matrix M) with a weakly supervised method based on pairwise or triple constraints as follows:Must-link/cannot link constraints (sometimes called positive/negative pairs):S={xi, xj:xi and xj should be similar}D={xi, xj:xi and xj should be dissimilar}Relative constrains (sometimes called training triplets):R={xi, xj,xk:xi  should be more similar to xj than xk}Mahalanobis Distance MetricThis section deals with the distance metric of the supervised Mahalanobis (complete or weak), which has gained a lot of attention due to its simplicity and excellent interpretability due to linear projection. Here we present the Mahalanobis distance as well as the two important challenges related to this distance metric.Mahalanobis distance: This concept is derived from Mahalanobis (1936) ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Mahalanobis","given":"Prasanta Chandra","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Proceedings of the National Institute of Sciences (Calcutta)","id":"ITEM-1","issued":{"date-parts":[["1936"]]},"page":"49-55","title":"On the generalized distance in statistics","type":"article-journal","volume":"2"},"uris":["http://www.mendeley.com/documents/?uuid=17222ec1-0dd9-490a-91a6-3a2e9afb60b7"]}],"mendeley":{"formattedCitation":"[38]","plainTextFormattedCitation":"[38]","previouslyFormattedCitation":"[38]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[38] and basically refers to the measurement of distances that includes the correlation between features:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 1dmahax, x=x-xTΩ-1x-x,Where x and x are random vectors of the same distribution as the covariance matrix Ω. With a slight change in the terms that are common in the distance metric learning literature, we will actually be able to derive from the concept of Mahalanobis distance to refer to generalized square distances, which are defined as follows:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 2dMx, x'=x-x'TMx-x'Which is parameterized with M∈S+d, where S+d is a symmetric positive semi-definite cone d × d (PSD). M must ensure that dM satisfies the properties of the pseudo-distance ∀ x, x',x''∈ X,dMx, x'≥0dMx, x'=0dMx, x'=d(x',x)dMx, x''≤dx,x'+d(x',x'')Interpretation: It should be noted that when M is the same matrix, we obtain the Euclidean distance. Otherwise, M can be expressed byLTL, where L∈RK×d where k is the rank of the matrix M. We can also write dMx, x' as follows:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 3dMx, x'=x-x'TMx-x'=x-x'TLTLx-x'=Lx-Lx'TLx-Lx'Thus, a Mahalanobis distance can be implicitly expressed as a calculation of the Euclidean distance after the linear representation of the data defined by the M conversion matrix. It should be noted that M is a low-order matrix, in other words, rankM=r<d, so it provides a linear conversion of information into a space with smaller dimensions r. Thus, it allows us to get a more concise representation of cheaper distance information and calculations, especially when the main feature space is large. These excellent features testify to the great attractiveness of the Mahalanobis distance, which is used in many distance learning applications.Challenges: There are two major challenges in learning the Mahalanobis distance standard. The first challenge is to maintain M∈S+d somehow during the optimization process. A simple way to do this is to use the illustrated rotation method, which is between a gradient step and an imaging step on the PSD cone by setting the negative eigenvalues ​​to zero. But this method is expensive for high-dimensional problems, because the analysis of eigenvalues ​​will be compared to O(d3). The second challenge is learning a low-order matrix (which, as mentioned earlier, involves an illustration of a smaller space) instead of a full-order matrix. Unfortunately, optimizing M against rank constraint or NP-regulation is difficult and therefore cannot be done effectively.Distance metric learning algorithmsIn this section, we review some of the distance learning methods that have been introduced recently. We divide these algorithms into unsupervised, supervised, or semi-supervised categories based on the supervised information they use. In this research, we use X=x1, x2,…,xn∈Rd×n to represent the data matrix. xi∈Rd is the i vector of the data vector. It should be noted that in the following demonstration we will use D to represent the distance measurement we want to learn.Although there are many distance learning standard algorithms, almost all of them optimize the following objective function. In which L(D) is a supervised term containing regulatory information, U(D) is an unsupervised term and interacts only with data indicators. λ1≥0, λ2≥0 are balancing parameters. With this in mind, we will discuss the details of algorithms and how to formulate them. Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 4JD=λ1LD+λ2U(D)Unsupervised distance metric learning approachesUnsupervised methods of distance metric learning do not require any supervision information. In other words, they perform distance metric learning only by having a matrix X, so that a discriminative or geometric optimality or is obtained. In  REF _Ref84701225 \h Equation ‎24, unsupervised methods J (D) are optimized by considering λ1 = 0. Based on the properties of our existing unsupervised methods, compare them with Table 2 1. Unsupervised distance learning algorithms. We classify in which distance learning algorithms are divided into linear and nonlinear categories based on the linearity or nonlinearity of their imagery. The concepts of learning the local and global distance criterion also depend on whether the algorithm applies some constraints to the local or global data structure.Table  STYLEREF 1 \s ‎2 SEQ Table \* ARABIC \s 1 1. Unspervised distance metric learning algorithms.LocalGlobalLinearLPP ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"He","given":"X","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Niyogi","given":"P","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2004"]]},"note":"cited By 678","title":"Locality preserving projections","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=95a5e1cd-6301-49a5-af8c-61aebf48403b"]}],"mendeley":{"formattedCitation":"[39]","plainTextFormattedCitation":"[39]","previouslyFormattedCitation":"[39]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[39]PCA ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Jolliffe","given":"Ian T","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Principal component analysis","id":"ITEM-1","issued":{"date-parts":[["2002"]]},"note":"From Duplicate 3 (No Title - Jolliffe, I T)\n\ncited By 16275","page":"150-166","publisher":"Springer","title":"Principal component analysis and factor analysis","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=d8d75efb-dde1-4814-8023-886422f7e788"]},{"id":"ITEM-2","itemData":{"DOI":"10.1109/TNN.2011.2161772","author":[{"dropping-particle":"","family":"Wang","given":"F","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhao","given":"B","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Neural Networks","id":"ITEM-2","issue":"9","issued":{"date-parts":[["2011"]]},"note":"cited By 11","page":"1446-1456","title":"Unsupervised large margin discriminative projection","type":"article-journal","volume":"22"},"uris":["http://www.mendeley.com/documents/?uuid=0b5b24df-f052-48fe-8c36-33655e28a947"]}],"mendeley":{"formattedCitation":"[40], [41]","plainTextFormattedCitation":"[40], [41]","previouslyFormattedCitation":"[40], [41]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[40], [41]NonlinearLE ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Belkin","given":"M","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Niyogi","given":"P","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2002"]]},"note":"cited By 1017","page":"585-591","title":"Laplacian eigenmaps and spectral techniques for embedding and clustering","type":"article-journal","volume":"14"},"uris":["http://www.mendeley.com/documents/?uuid=e7d4cba5-c077-44d4-962a-085b9bca54fe"]}],"mendeley":{"formattedCitation":"[42]","plainTextFormattedCitation":"[42]","previouslyFormattedCitation":"[42]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[42], LLE ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1126/science.290.5500.2323","ISBN":"00368075","ISSN":"00368075","PMID":"11125150","author":[{"dropping-particle":"","family":"Roweis","given":"S. T.","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Science","id":"ITEM-1","issue":"5500","issued":{"date-parts":[["2000","12","22"]]},"page":"2323-2326","title":"Nonlinear Dimensionality Reduction by Locally Linear Embedding","type":"article-journal","volume":"290"},"uris":["http://www.mendeley.com/documents/?uuid=af9f1543-af3f-46b7-98b9-b00224ecee28"]}],"mendeley":{"formattedCitation":"[29]","plainTextFormattedCitation":"[29]","previouslyFormattedCitation":"[29]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[29], Isomap ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1126/science.290.5500.2319","abstract":"Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human Brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs - 30,000 auditory nerve fibers or 106 optic nerve fibers - a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.","author":[{"dropping-particle":"","family":"Tenenbaum","given":"J.B.","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Silva","given":"V.","non-dropping-particle":"De","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Langford","given":"J.C.","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Science","id":"ITEM-1","issue":"5500","issued":{"date-parts":[["2000"]]},"title":"A global geometric framework for nonlinear dimensionality reduction","type":"article-journal","volume":"290"},"uris":["http://www.mendeley.com/documents/?uuid=5f51b0ef-5cd0-38fa-80ac-33208133fde8"]}],"mendeley":{"formattedCitation":"[43]","plainTextFormattedCitation":"[43]","previouslyFormattedCitation":"[43]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[43], SNE ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Hinton","given":"G E","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Roweis","given":"S T","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 833–840","id":"ITEM-1","issued":{"date-parts":[["2002"]]},"note":"cited By 1","title":"Stochastic neighbor embedding. In: Advances in neural information processing systems (NIPS)","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=3fd14afe-508f-44fd-8a13-ca89adc99b28"]}],"mendeley":{"formattedCitation":"[44]","plainTextFormattedCitation":"[44]","previouslyFormattedCitation":"[44]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[44], KLPP ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"He","given":"X","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Niyogi","given":"P","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2004"]]},"note":"cited By 678","title":"Locality preserving projections","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=95a5e1cd-6301-49a5-af8c-61aebf48403b"]}],"mendeley":{"formattedCitation":"[39]","plainTextFormattedCitation":"[39]","previouslyFormattedCitation":"[39]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[39]KPCA ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Schölkopf","given":"Bernhard","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Smola","given":"Alexander J","non-dropping-particle":"","parse-names":false,"suffix":""}],"id":"ITEM-1","issued":{"date-parts":[["2002"]]},"publisher":"MIT press","title":"Learning with kernels: support vector machines, regularization, optimization, and beyond","type":"book"},"uris":["http://www.mendeley.com/documents/?uuid=56d33e72-e0ff-4443-8ec9-0663d51a93a6"]}],"mendeley":{"formattedCitation":"[45]","plainTextFormattedCitation":"[45]","previouslyFormattedCitation":"[45]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[45], KUMMP ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TNN.2011.2161772","author":[{"dropping-particle":"","family":"Wang","given":"F","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhao","given":"B","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Neural Networks","id":"ITEM-1","issue":"9","issued":{"date-parts":[["2011"]]},"note":"cited By 11","page":"1446-1456","title":"Unsupervised large margin discriminative projection","type":"article-journal","volume":"22"},"uris":["http://www.mendeley.com/documents/?uuid=0b5b24df-f052-48fe-8c36-33655e28a947"]}],"mendeley":{"formattedCitation":"[41]","plainTextFormattedCitation":"[41]","previouslyFormattedCitation":"[41]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[41]Principle Component Analysis (PCA)PCA [30] is a method that tries to extract directions from the data in such a way that the maximum variances can be achieved. Assuming that X be a zero-mean matrix (matrix with mean equal zero), then the first principal component W1 can be obtained from the following equations.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 5W1=argmaxW=1=VarWT XargmaxW=11n-1WT X XTWBy having the first k-1 principal components, the k principal of the principal component can be obtained by subtracting the first principal component k-1 from X.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 6Xk-1=X-i=1k-1WiWiTXiUsing Xk-1 as the new data set, we can obtain the k-th principal component according to the following equation.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 7Wk=argmaxW=1WT Xk-1Xk-1WFinally, we want to obtain the matrix W with the following properties:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 8maxwtrWTXXTW,   s.t. WTW=I We observe that the PCA directions continue with a sequential search for the directions on which the data changes are the most. Given that tr(WT XXTW) considers the set of variances in all directions, the desired W can be obtained by the eigen decomposition  XXT. PCA is a linear and global learning method. The distance learned between xi and xj is the Euclidean distance between WT xi and WT xj. Also, since W is explicitly learned, PCA can also be applied to out-of-sampledata.Nonlinear PCAOne of the limitations of PCA is that it is linear. In order to be able to learn the direction of distributions of nonlinear data, we can use a kernel trick ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Schölkopf","given":"Bernhard","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Smola","given":"Alexander J","non-dropping-particle":"","parse-names":false,"suffix":""}],"id":"ITEM-1","issued":{"date-parts":[["2002"]]},"publisher":"MIT press","title":"Learning with kernels: support vector machines, regularization, optimization, and beyond","type":"book"},"uris":["http://www.mendeley.com/documents/?uuid=56d33e72-e0ff-4443-8ec9-0663d51a93a6"]}],"mendeley":{"formattedCitation":"[45]","plainTextFormattedCitation":"[45]","previouslyFormattedCitation":"[45]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[45], which is a common method in machine learning and data mining, that to tries to convert the nonlinear distribution of data in the initial space to a linear distribution in the mapped feature space. Assuming this mapping is ϕ:Rd→F. F is a Reproducing Kernel Hilbert Space (RKHS), we can express this mapping as ϕ:x→k(.,x), where k (.,x) is a function in RKHS as explained in the following:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 9-〈k.,x〉=f(x)Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 10-k.,x, k.,y=k(x,y)Where, f is a function in the same RKHS. Based on the theorem presented in ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Schölkopf","given":"Bernhard","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Smola","given":"Alexander J","non-dropping-particle":"","parse-names":false,"suffix":""}],"id":"ITEM-1","issued":{"date-parts":[["2002"]]},"publisher":"MIT press","title":"Learning with kernels: support vector machines, regularization, optimization, and beyond","type":"book"},"uris":["http://www.mendeley.com/documents/?uuid=56d33e72-e0ff-4443-8ec9-0663d51a93a6"]}],"mendeley":{"formattedCitation":"[45]","plainTextFormattedCitation":"[45]","previouslyFormattedCitation":"[45]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[45], we can write each function f∈F as follows:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 11fx=i=1nαik(xi,x)AutoencoderAutoencoder is a type of neural network with a generally narrow (bottleneck) hidden layer. This network tries to reconstruct the input data in the output and generally is used for novelty detection and deep learning ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"ISBN":"9783319458229","author":[{"dropping-particle":"","family":"Handl","given":"Julia","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Hart","given":"Emma","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"López-ibáñez","given":"Peter R Lewis Manuel","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Ochoa","given":"Gabriela","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Paechter","given":"Ben","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Hutchison","given":"David","non-dropping-particle":"","parse-names":false,"suffix":""}],"id":"ITEM-1","issued":{"date-parts":[["2016"]]},"title":"Parallel Problem Solving from Nature – PPSN XIV","type":"book"},"uris":["http://www.mendeley.com/documents/?uuid=fcc8139d-9fa7-4aa3-a5d9-2f9d7789fd41"]}],"mendeley":{"formattedCitation":"[46]","plainTextFormattedCitation":"[46]","previouslyFormattedCitation":"[46]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[46]. This network initially encodes the input and then decodes it to reconstruct it in the output. The goal of the autoencoder is to reconstruct the input itself.Figure  STYLEREF 1 \s ‎2 SEQ Figure \* ARABIC \s 1 2. Diagram of an Autoencoder  ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1007/s13042-016-0550-y","ISSN":"1868808X","abstract":"Autoencoder can learn the structure of data adaptively and represent data efficiently. These properties make autoencoder not only suit huge volume and variety of data well but also overcome expensive designing cost and poor generalization. Moreover, using autoencoder in deep learning to implement feature extraction could draw better classification accuracy. However, there exist poor robustness and overfitting problems when utilizing autoencoder. In order to extract useful features, meanwhile improve robustness and overcome overfitting, we studied denoising sparse autoencoder through adding corrupting operation and sparsity constraint to traditional autoencoder. The results suggest that different autoencoders mentioned in this paper have some close relation and the model we researched can extract interesting features which can reconstruct original data well. In addition, all results show a promising approach to utilizing the proposed autoencoder to build deep models.","author":[{"dropping-particle":"","family":"Meng","given":"Lingheng","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Ding","given":"Shifei","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Xue","given":"Yu","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"International Journal of Machine Learning and Cybernetics","id":"ITEM-1","issue":"5","issued":{"date-parts":[["2017"]]},"page":"1719-1729","publisher":"Springer Berlin Heidelberg","title":"Research on denoising sparse autoencoder","type":"article-journal","volume":"8"},"uris":["http://www.mendeley.com/documents/?uuid=caaa8872-0af8-454f-9f62-e1dbb65fe5e5"]}],"mendeley":{"formattedCitation":"[47]","plainTextFormattedCitation":"[47]","previouslyFormattedCitation":"[47]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[47].The autoencoder tries to learn the function S(.) as follows:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 12SW,W',b1,b2(X)≈XIn which W,W',b1,b2 are the model parameters. W is a weighted matrix connected to the input and hidden layers and W' is the output layer weights matrix. b1 and b2 are also the bias vectors of the hidden and output layers, respectively. S. is divided into two phases. First phase, or the encoding phase is from the input to the hidden layer ( REF _Ref501201910 \h  \* MERGEFORMAT Equation ‎213) and the second phase or the decoding phase is from the hidden layer to the output ( REF _Ref84758118 \h Equation ‎214). Autoencoder finds the latent space (i.e., hidden variables) embedded in the input data, from the outputs of the hidden layer as denoted by h in  REF _Ref501201910 \h Equation ‎213.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 13h=f(W×X+b1)Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 14Y=g(W'×h+b2) In practice, we could use the tied weight W'=W to reconstruct the input X i.e., Y≈X. To do this we could use the square error ( REF _Ref501202217 \h  \* MERGEFORMAT Equation ‎215) and cross entropy loss function ( REF _Ref84764711 \h Equation ‎216).Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 15LsW,W',b1,b2;X=12Y-X2Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 16LcW,W',b1,b2;X=-[XlogY+1-Xlog1-Y] In these equations, if X is a matrix with real values then we would usually use the Least square loss function and in case the values are binary then the use of the cross-entropy loss function would be more appropriate. Y could be calculated from the combination of equations ( REF _Ref501201910 \h  \* MERGEFORMAT Equation ‎213) and ( REF _Ref84758118 \h Equation ‎214) as shown in  REF _Ref84764991 \h Equation ‎217:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 17Y=gW'×X+b1+b2)Generally, in order to control the weights' scale and to stop the overfitting the regularization term is added to the loss functions  REF _Ref501202217 \h  \* MERGEFORMAT Equation ‎215 or  REF _Ref84764711 \h Equation ‎216 where the loss function would be finally as follows.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 18LW,W',b1,b2;X=LtW,W',b1,b2;X+λ2l=1nli=1slj=1sl+1Wijl2In which, Lt shows the squared error Ls or the cross-entropy Lc. Additionally, nl shows the layer number and sl and sl+1 show the units on the lth and l+1th layer, respectively.Locality Preserving Projections (LPP)Transformations such as PCA find directions in which data is optimally distributed. In other words, PCA seeks directions in which total data changes are maximal. Another way to find the direction of imagery is to preserve the geometry and proximity of the data to the original space. An example of this type of learning is the Retention Location Imaging (LPP) method ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"He","given":"X","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Niyogi","given":"P","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2004"]]},"note":"cited By 678","title":"Locality preserving projections","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=95a5e1cd-6301-49a5-af8c-61aebf48403b"]}],"mendeley":{"formattedCitation":"[39]","plainTextFormattedCitation":"[39]","previouslyFormattedCitation":"[39]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[39].The LPP tries to find a W imaging matrix that preserves the local position of the data in the original space. In this method, the local position of the data is considered based on the similarity of the data pair {ωiji,j=1n in a neighborhood. This neighborhood is usually calculated by a Gaussian function as follows.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 19ωij=exp-xi-xj22σ2(if xi∈Nj or xj∈Ni)Here Ni and Nj are neighbors around points xi and xj, respectively. This means that we need to maintain distances in only one local neighborhood.The purpose of LPP is to solve the following optimization problem.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 20minwij:xi∈Nj or xj∈NiWTxi-WTxj2ωij=tr(WTXLXTW)s.t. WTXDXTW=I,where D∈Rn×n is a diagonal matrix with Dii=jωij. L=D-Ω is a Laplacian matrix with Ωij=ωij if xi∈Ni, otherwise Ωij=0. The optimal solution for  REF _Ref84755823 \h Equation ‎220 can be done by performing generalized special analysis on (XLX, XDX).And the direction of optimal imaging is obtained by adding k special vectors whose corresponding eigenvalues have the smallest values. LPP is therefore a linear and local method, as it only tries to preserve the local geometry of the data. The distance learned between xi and xj is the Euclidean distance between WTxi and WTxj. The computational method used in LPP is eigenvalue analysis, and with W it can also be used for off-sample data.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 21XLXTw=λXDXTwwe can also use the kernel trick to build a nonlinear LPP, in which case we need to solve the problem of parsing the following eigenvalues.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 22ΦLΦTv=λΦDΦTv ⇒ΦLΦTΦα=ΦDΦTΦα,where Φ is the data matrix in the property space after kernel mapping and we have v=Φα. Therefore, we have:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 23KLKα=KDKα⇒Ly=λDy,where y = Kα represents the embedded data after KLPP. Thus, KLPP is a local and nonlinear method.Laplacian Embedding (LE)In fact, before LPP emerged, a method called LE ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Belkin","given":"M","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Niyogi","given":"P","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2002"]]},"note":"cited By 1017","page":"585-591","title":"Laplacian eigenmaps and spectral techniques for embedding and clustering","type":"article-journal","volume":"14"},"uris":["http://www.mendeley.com/documents/?uuid=e7d4cba5-c077-44d4-962a-085b9bca54fe"]}],"mendeley":{"formattedCitation":"[42]","plainTextFormattedCitation":"[42]","previouslyFormattedCitation":"[42]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[42] was proposed that focused on embedding while maintaining a structural neighborhood. Here, too, neighborhood is defined as stated in  REF _Ref84756517 \h Equation ‎219. Assuming we want to map the data in a one-dimensional space with mapped coordinates y = [y1, y2,…, yn], then LE's goal is to obtain y by solving the following optimization problem.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 24minyi=1nyi-yj2ωij=yTLys.t. yTDy=1,where L, like LLP, is the Laplacian matrix. Because LE performs mapped coordinates without any obvious mapping, LE is a nonlinear, local method. The distance learned between xi and xj is the Euclidean distance between yi and yj. The computational method used in LE is the eigenvalue decomposition method. Given that LE performs mapped coordinates without obtaining any explicit mapping, it is simply not possible to generalize this mapping to off-sample data.Locally Linear embedding (LLE)LLE ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1126/science.290.5500.2323","ISBN":"00368075","ISSN":"00368075","PMID":"11125150","author":[{"dropping-particle":"","family":"Roweis","given":"S. T.","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Science","id":"ITEM-1","issue":"5500","issued":{"date-parts":[["2000","12","22"]]},"page":"2323-2326","title":"Nonlinear Dimensionality Reduction by Locally Linear Embedding","type":"article-journal","volume":"290"},"uris":["http://www.mendeley.com/documents/?uuid=0a0dbcfa-cef5-4e23-91c6-7fcee753cf54"]}],"mendeley":{"formattedCitation":"[29]","plainTextFormattedCitation":"[29]","previouslyFormattedCitation":"[29]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[29] is another approach to achieve the embedded space which tries to preserve the local neighborhoods of the input data. The difference between LLE and the LE ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Belkin","given":"M","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Niyogi","given":"P","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2002"]]},"note":"cited By 1017","page":"585-591","title":"Laplacian eigenmaps and spectral techniques for embedding and clustering","type":"article-journal","volume":"14"},"uris":["http://www.mendeley.com/documents/?uuid=e7d4cba5-c077-44d4-962a-085b9bca54fe"]}],"mendeley":{"formattedCitation":"[42]","plainTextFormattedCitation":"[42]","previouslyFormattedCitation":"[42]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[42] is in the way that they calculate the neighborhoods between the points. LLE is based on the assumption of linear neighborhood between the points, which assumes that each point, xii=1, 2, …, n, could be reconstructed using the location of its neighbors, Ni(i=1, 2, …, n).Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 25minωijixi-xωijxj2s.t. jωij=1 (∀ i=1, 2, …, n)In the second step, LLE tries to retrieve the mappings in a lower dimension while preserving the local relations by solving the optimization problem in  REF _Ref501269317 \h  \* MERGEFORMAT Equation ‎226.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 26minyii=1niyi-xωijyj2s.t.  i=1nyi=0,   i=1nyiyj=nILLE is also a local and non-linear method. In this approach, like LE the learned distance between xi and xj is the Euclidean distance between yi and yj. The computation method used in the LLE utilizes both quadratic programming and eigen analysis. Generalization of the LLE for the out-of-sample data is not easy as it calculates the mapped coordinates of the data directly and without calculating any explicit mappings.Isometric feature mapping (Isomap)Isomap ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1126/science.290.5500.2319","abstract":"Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human Brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs - 30,000 auditory nerve fibers or 106 optic nerve fibers - a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.","author":[{"dropping-particle":"","family":"Tenenbaum","given":"J.B.","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Silva","given":"V.","non-dropping-particle":"De","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Langford","given":"J.C.","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Science","id":"ITEM-1","issue":"5500","issued":{"date-parts":[["2000"]]},"title":"A global geometric framework for nonlinear dimensionality reduction","type":"article-journal","volume":"290"},"uris":["http://www.mendeley.com/documents/?uuid=5f51b0ef-5cd0-38fa-80ac-33208133fde8"]}],"mendeley":{"formattedCitation":"[43]","plainTextFormattedCitation":"[43]","previouslyFormattedCitation":"[43]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[43] is another approach for learning the low-dimensional spaces where the geodesic distances are devised on a weighted graph with classical scaling (metric Multidimensional Scaling ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"ISBN":"1584880945","author":[{"dropping-particle":"","family":"Cox, Trevor F; Cox","given":"Michael AA","non-dropping-particle":"","parse-names":false,"suffix":""}],"id":"ITEM-1","issued":{"date-parts":[["2000"]]},"publisher":"CRC press","title":"Multidimensional scaling","type":"book"},"uris":["http://www.mendeley.com/documents/?uuid=c5c143d7-0b3e-4ca2-ad72-48a3d46e4158"]}],"mendeley":{"formattedCitation":"[48]","plainTextFormattedCitation":"[48]","previouslyFormattedCitation":"[48]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[48]).  The main difference between the Isomap, LE and LLE is in their approach of learning the similar data-pairs. In Isomap, in addition to the similarity, the distance between the data-pairs (i.e., the dissimilarities) are first calculated, and then the classic MDS approach is used to calculate the coordinates of the mappings in a way that the pairwise distances are preserved with the best way possible.Here, the distance between the data-pairs is measured as followed. First, a connected neighborhood graph is constructed on the dataset, this graph could be weighted or unweighted. Then the geodesic distances would be the shortest path between the data-pairs. These computations could be considered as the discrete approximation of the real geodesic distances of the data-pairs on the manifold. Thus, Isomap is a nonlinear and global approach. The learned distance is measured by the Euclidean distance on the low-dimensional space. The computation method used in Isomap is Eigen decomposition. As in Isomap the mapped coordinates of the data are learned directly and without any explicit mappings; thus, like LE and LLE, it is not that easy to extend the Isomap to the out-of-sample data. Geodesic distance has been previously applied successfully for dimensionality reduction in classification and clustering application [22].Supervised distance metric learning approachesSupervised distance metric learning algorithms, which preform the learning process based on the data points and their corresponding labels, are discussed in this chapter. Referring to  REF _Ref84701225 \h Equation ‎24, the supervised approaches perform JD with λ2=0. Like the unsupervised approaches, we divide the supervised approaches to different categories based on their characteristics.Table  STYLEREF 1 \s ‎2 SEQ Table \* ARABIC \s 1 2. Supervised distnace metric learning algorithms.LocalGlobalLinearNCA ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Goldberger","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Roweis","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Hinton","given":"G","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Salakhutdinov","given":"R","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Neighbourhood Components Analysis","id":"ITEM-1","issued":{"date-parts":[["2004"]]},"note":"cited By 26","page":"513-520","title":"Neighbourhood components analysis","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=98a57fff-05cc-4469-b0af-fe21a49a1c35"]}],"mendeley":{"formattedCitation":"[49]","plainTextFormattedCitation":"[49]","previouslyFormattedCitation":"[49]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[49], ANMM ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/CVPR.2007.383124","author":[{"dropping-particle":"","family":"Wang","given":"F","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","id":"ITEM-1","issued":{"date-parts":[["2007"]]},"note":"cited By 55","title":"Feature extraction by maximizing the average neighborhood margin","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=2c3565c3-8e37-4505-8470-431d19d33fb0"]}],"mendeley":{"formattedCitation":"[50]","plainTextFormattedCitation":"[50]","previouslyFormattedCitation":"[50]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[50], LMNN ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Weinberger","given":"K Q","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Blitzer","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2005"]]},"note":"cited By 1","title":"Distance metric learning for large margin nearest neighbor classification","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=7a69abb7-0c68-41eb-8ead-44733ab3cd65"]}],"mendeley":{"formattedCitation":"[51]","plainTextFormattedCitation":"[51]","previouslyFormattedCitation":"[51]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[51]LDA ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1016/0098-3004(96)00017-9","ISBN":"9780080478654","ISSN":"00983004","PMID":"21124870","abstract":"This completely revised second edition presents an introduction to statistical pattern recognition. Pattern recognition in general covers a wide range of problems: it is applied to engineering problems, such as character readers and wave form analysis as well as to brain modeling in biology and psychology. Statistical decision and estimation, which are the main subjects of this book, are regarded as fundamental to the study of pattern recognition. This book is appropriate as a text for introductory courses in pattern recognition and as a reference book for workers in the field. Each chapter contains computer projects as well as exercises.","author":[{"dropping-particle":"","family":"Fukunaga","given":"Keinosuke","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Pattern Recognition","id":"ITEM-1","issue":"7","issued":{"date-parts":[["1990"]]},"page":"833-834","title":"Statistical Pattern Stas-tical Pattern Recognition","type":"article-journal","volume":"22"},"uris":["http://www.mendeley.com/documents/?uuid=19c17298-b536-40d7-a3cf-ebf8608e50c2"]}],"mendeley":{"formattedCitation":"[52]","plainTextFormattedCitation":"[52]","previouslyFormattedCitation":"[52]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[52], LSI ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Xing","given":"E P","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Ng","given":"A Y","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Jordan","given":"M I","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Russell","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2002"]]},"note":"cited By 774","title":"Distance metric learning, with application to clustering with side-information","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=0ad58555-0974-4518-8897-a90b6a5fdb84"]}],"mendeley":{"formattedCitation":"[11]","plainTextFormattedCitation":"[11]","previouslyFormattedCitation":"[11]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[11], ITML ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"V","family":"Davis","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Kulis","given":"B","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Jain","given":"P","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Suvrit","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Dhillon","given":"I S","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 209–216","id":"ITEM-1","issued":{"date-parts":[["2007"]]},"note":"cited By 1","title":"Information-theoretic metric learning. In: International conference on machine learning (ICML)","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=5925aef0-efcb-46e3-a11c-cff0bfd4f277"]}],"mendeley":{"formattedCitation":"[53]","plainTextFormattedCitation":"[53]","previouslyFormattedCitation":"[53]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[53], MMDA ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Kocsor","given":"A","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Kovács","given":"K","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Szepesvári","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science)","id":"ITEM-1","issued":{"date-parts":[["2004"]]},"note":"cited By 25","page":"227-238","title":"Margin maximizing discriminant analysis","type":"paper-conference","volume":"3201"},"uris":["http://www.mendeley.com/documents/?uuid=044ec6ea-5aaf-4127-b716-b505dab64f51"]}],"mendeley":{"formattedCitation":"[54]","plainTextFormattedCitation":"[54]","previouslyFormattedCitation":"[54]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[54], RCA ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Shental","given":"N","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Hertz","given":"T","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Weinshall","given":"D","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Pavel","given":"M","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 776–790","id":"ITEM-1","issued":{"date-parts":[["2002"]]},"note":"cited By 1","title":"Adjustment learning and relevant component analysis. In: Proceedings of European conference on computer vision","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=d7823fad-8300-4c0b-91e7-44cb11219159"]}],"mendeley":{"formattedCitation":"[55]","plainTextFormattedCitation":"[55]","previouslyFormattedCitation":"[55]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[55]NonlinearKANMM ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/CVPR.2007.383124","author":[{"dropping-particle":"","family":"Wang","given":"F","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","id":"ITEM-1","issued":{"date-parts":[["2007"]]},"note":"cited By 55","title":"Feature extraction by maximizing the average neighborhood margin","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=2c3565c3-8e37-4505-8470-431d19d33fb0"]}],"mendeley":{"formattedCitation":"[50]","plainTextFormattedCitation":"[50]","previouslyFormattedCitation":"[50]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[50] , KLMNN ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Weinberger","given":"K Q","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Blitzer","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2005"]]},"note":"cited By 1","title":"Distance metric learning for large margin nearest neighbor classification","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=7a69abb7-0c68-41eb-8ead-44733ab3cd65"]}],"mendeley":{"formattedCitation":"[51]","plainTextFormattedCitation":"[51]","previouslyFormattedCitation":"[51]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[51]KLDA ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Mika","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Ratsch","given":"G","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Weston","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Schölkopf","given":"B","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Müllers","given":"K R","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 41–48","id":"ITEM-1","issued":{"date-parts":[["1999"]]},"note":"cited By 1","title":"Fisher discriminant analysis with kernels. In: Neural networks for signal processing IX, 1999. proceedings of the 1999 IEEE signal processing society workshop","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=1322bacc-9bca-422f-9d92-d9238e1eee22"]}],"mendeley":{"formattedCitation":"[56]","plainTextFormattedCitation":"[56]","previouslyFormattedCitation":"[56]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[56], KMMDA ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Kocsor","given":"A","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Kovács","given":"K","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Szepesvári","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science)","id":"ITEM-1","issued":{"date-parts":[["2004"]]},"note":"cited By 25","page":"227-238","title":"Margin maximizing discriminant analysis","type":"paper-conference","volume":"3201"},"uris":["http://www.mendeley.com/documents/?uuid=044ec6ea-5aaf-4127-b716-b505dab64f51"]}],"mendeley":{"formattedCitation":"[54]","plainTextFormattedCitation":"[54]","previouslyFormattedCitation":"[54]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[54], KRCA ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Tsang","given":"I W","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Cheung","given":"P M","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Kwok","given":"J T","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 954–959","id":"ITEM-1","issued":{"date-parts":[["2005"]]},"note":"cited By 1","title":"Kernel relevant component analysis for distance metric learning. In: In IEEE International joint conference on neural networks (IJCNN)","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=bd80d13e-a39c-48ff-ac65-24a4ca8df600"]}],"mendeley":{"formattedCitation":"[57]","plainTextFormattedCitation":"[57]","previouslyFormattedCitation":"[57]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[57]Linear discriminant analysis (LDA)LDA ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1016/0098-3004(96)00017-9","ISBN":"9780080478654","ISSN":"00983004","PMID":"21124870","abstract":"This completely revised second edition presents an introduction to statistical pattern recognition. Pattern recognition in general covers a wide range of problems: it is applied to engineering problems, such as character readers and wave form analysis as well as to brain modeling in biology and psychology. Statistical decision and estimation, which are the main subjects of this book, are regarded as fundamental to the study of pattern recognition. This book is appropriate as a text for introductory courses in pattern recognition and as a reference book for workers in the field. Each chapter contains computer projects as well as exercises.","author":[{"dropping-particle":"","family":"Fukunaga","given":"Keinosuke","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Pattern Recognition","id":"ITEM-1","issue":"7","issued":{"date-parts":[["1990"]]},"page":"833-834","title":"Statistical Pattern Stas-tical Pattern Recognition","type":"article-journal","volume":"22"},"uris":["http://www.mendeley.com/documents/?uuid=19c17298-b536-40d7-a3cf-ebf8608e50c2"]}],"mendeley":{"formattedCitation":"[52]","plainTextFormattedCitation":"[52]","previouslyFormattedCitation":"[52]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[52] is one the popular supervised embedding approaches. This approach, searches for the directions where the data belonging to different classes are discriminated in the best way possible. To be more precise, with the assumption that the data are from C different classes, LDA defines the compactness and separation matrices as follows:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 27C=1Cc1ncxi∈cxi-xcxi-xcTEquation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 28S=1Ccxc-xxc-xTThe goal of LDA is to find W which could be calculated by solving the following equation:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 29minWTW=Itr(WTCW)tr(WTSW)By extending the numerator and denominator of  REF _Ref501291151 \h  \* MERGEFORMAT Equation ‎229, it could be seen that the numerator corresponds to the sum of the distances between data points and its class center after the mapping, and the denominator corresponds to sum of the distances between the center of each class and the total mean of the data after projection. Therefore, by minimizing  REF _Ref501291151 \h  \* MERGEFORMAT Equation ‎229 the inter-class scatter increases and at the same time the intra-class scatter decreases. As it is hard to solve  REF _Ref501291151 \h  \* MERGEFORMAT Equation ‎229, some researchers ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1016/S0167-8655(02)00207-6","author":[{"dropping-particle":"","family":"Guo","given":"Y.-F.","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Li","given":"S.-J.","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Yang","given":"J.-Y.","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Shu","given":"T.-T.","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Wu","given":"L.-D.","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Pattern Recognition Letters","id":"ITEM-1","issue":"1-3","issued":{"date-parts":[["2003"]]},"note":"cited By 75","page":"147-158","title":"A generalized Foley-Sammon transform based on generalized fisher discriminant criterion and its application to face recognition","type":"article-journal","volume":"24"},"uris":["http://www.mendeley.com/documents/?uuid=fe5f8da4-80af-460b-bf8b-060a2e4350f7"]},{"id":"ITEM-2","itemData":{"DOI":"10.1109/TNN.2009.2015760","author":[{"dropping-particle":"","family":"Jia","given":"Y","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Nie","given":"F","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Neural Networks","id":"ITEM-2","issue":"4","issued":{"date-parts":[["2009"]]},"note":"cited By 137","page":"729-735","title":"Trace ratio problem revisited","type":"article-journal","volume":"20"},"uris":["http://www.mendeley.com/documents/?uuid=9fc0c430-c26a-45bb-8fad-ccea68132db2"]}],"mendeley":{"formattedCitation":"[58], [59]","plainTextFormattedCitation":"[58], [59]","previouslyFormattedCitation":"[58], [59]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[58], [59] have conducted some research on this problem. LDA is a linear global approach. The learned distance between xi and xj is the Euclidean distance between WTxi and WTxj. The generalization of LDA to the out-of-sample data is easy as it learns the transformation matrix W explicitly through eigenvalue decomposition.Relevant Component Analysis (RCA)RCA ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Shental","given":"N","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Hertz","given":"T","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Weinshall","given":"D","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Pavel","given":"M","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 776–790","id":"ITEM-1","issued":{"date-parts":[["2002"]]},"note":"cited By 1","title":"Adjustment learning and relevant component analysis. In: Proceedings of European conference on computer vision","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=d7823fad-8300-4c0b-91e7-44cb11219159"]}],"mendeley":{"formattedCitation":"[55]","plainTextFormattedCitation":"[55]","previouslyFormattedCitation":"[55]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[55] is another distance metric learning method that uses data pairwise constraints. The RCA's goal is to find a mapping that reinforces related variances and eliminates the unrelated ones. Here, variances are just sample variances. We assume that data variances are correlated with a specific task, if removing these variances from the data (on average) worsens the clustering or retrieval results. They will be irrelevant if they are stored in the data but not correlated with a specific task ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Shental","given":"N","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Hertz","given":"T","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Weinshall","given":"D","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Pavel","given":"M","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 776–790","id":"ITEM-1","issued":{"date-parts":[["2002"]]},"note":"cited By 1","title":"Adjustment learning and relevant component analysis. In: Proceedings of European conference on computer vision","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=d7823fad-8300-4c0b-91e7-44cb11219159"]}],"mendeley":{"formattedCitation":"[55]","plainTextFormattedCitation":"[55]","previouslyFormattedCitation":"[55]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[55]. We also define small clusters and call them chunklets, which are connected components derived from must-links. The steps that the RCA includes are:Build chunklets based on must-link constraints, so that the data inside each chunklet is paired with must-link constraints.Assuming p there is a point in k chunklet, where chunklet j contains the points xjii=1nj and its mean mj. RCA Calculates the weighted covariance matrix inside the chunklet.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 30C=1pjkinxji-mjxji-mjTCalculation of the whitening transformation W=C12, and apply it to the original data: x=Wx. Intermittent use of inverse C as a precision matrix of a generalized Mahalanobis distance.Therefore, RCA is a global and linear method. It is easy to generalize the RCA to the out-of-sampledatapoints, since this method clearly learns the mapping matrix W. The computational method used in RCA is also eigen analysis. Information Theoretic Metric Learning (ITML)The objective function based on information theory is an approach to developing a supervised distance criterion. An example of this type of approach is ITML [53]. Assuming we have a generalized Mahalanobis spaced parameterized by the M_0 precision matrix, an M set of must-link constraints and a set of cannot cannot-link constraints. ITML solves the following optimization problem:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 31minM≥0dlogdet(M, M0)s.t. xi-xjTMxi-xj≥l, xi, xj∈∁xu-xvTMxu-xv≤u, xu, xv∈Mdlogdet is the LogDet divergence, also called Stein loss. It can be shown that Stein loss is a constant-scale constant loss function for which the neutral estimator of the least uniform changes is also an equivalent minimum error estimator ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"V","family":"Davis","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Kulis","given":"B","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Jain","given":"P","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Suvrit","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Dhillon","given":"I S","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 209–216","id":"ITEM-1","issued":{"date-parts":[["2007"]]},"note":"cited By 1","title":"Information-theoretic metric learning. In: International conference on machine learning (ICML)","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=5925aef0-efcb-46e3-a11c-cff0bfd4f277"]}],"mendeley":{"formattedCitation":"[53]","plainTextFormattedCitation":"[53]","previouslyFormattedCitation":"[53]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[53]. The authors in ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"V","family":"Davis","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Kulis","given":"B","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Jain","given":"P","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Suvrit","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Dhillon","given":"I S","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 209–216","id":"ITEM-1","issued":{"date-parts":[["2007"]]},"note":"cited By 1","title":"Information-theoretic metric learning. In: International conference on machine learning (ICML)","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=5925aef0-efcb-46e3-a11c-cff0bfd4f277"]}],"mendeley":{"formattedCitation":"[53]","plainTextFormattedCitation":"[53]","previouslyFormattedCitation":"[53]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[53] also present a Bergman illustration method for problem solving. ITML is a global, linear method. The distance learned is the distance of the Mahalanobis from the M accuracy matrix. Because the M accuracy matrix, which is used to estimate distance between data pairs, is learned, ITML generalization to off-sample data is easy. Also, the computational method used in ITML is Bergman illustration.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 32dlogdetM, M0=trMM0-1-logdetMM0-1-nNeighborhood Component Analysis (NCA)Unlike global learning methods such as LDA, NCA ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Goldberger","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Roweis","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Hinton","given":"G","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Salakhutdinov","given":"R","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Neighbourhood Components Analysis","id":"ITEM-1","issued":{"date-parts":[["2004"]]},"note":"cited By 26","page":"513-520","title":"Neighbourhood components analysis","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=98a57fff-05cc-4469-b0af-fe21a49a1c35"]}],"mendeley":{"formattedCitation":"[49]","plainTextFormattedCitation":"[49]","previouslyFormattedCitation":"[49]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[49] is a local method for supervised distance learning. In this method, each point xi selects another point xj with the probability of pij, and assigns its class label based on the selected point. NCA defines the probability of selecting a point as a neighbor as follows. Under this random selection rule, the NCA calculates the probability that point i will be classified correctly.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 33pij=exp⁡(-WTxi-WTxj2)k≠i(-WTxi-WTxk2)Where Li={j|li=lj} is a set of points in the same class as point i.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 34pi=j∈LipijThe goal of the NCA is to maximize the number of correctly classified points as follows:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 35JW=ipi=ij∈LipijIn ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Goldberger","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Roweis","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Hinton","given":"G","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Salakhutdinov","given":"R","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Neighbourhood Components Analysis","id":"ITEM-1","issued":{"date-parts":[["2004"]]},"note":"cited By 26","page":"513-520","title":"Neighbourhood components analysis","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=98a57fff-05cc-4469-b0af-fe21a49a1c35"]}],"mendeley":{"formattedCitation":"[49]","plainTextFormattedCitation":"[49]","previouslyFormattedCitation":"[49]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[49], the authors have proposed a truncated gradient descent method to minimize J (W). NCA is a local and linear approach. The distance learned between xi and xj is the Euclidean distance between WTxi and WTxj. Generalizing the NCA to the out-of-sample data is easy, because the mapping matrix W is explicitly learned, and the computational method used is the eigen decomposition.Discriminative Least Squares Regression (DLSR)discriminative least square approach proposed in ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TNNLS.2012.2212721","ISSN":"2162237X","PMID":"24808069","abstract":"This paper presents a framework of discriminative least squares regression (LSR) for multiclass classification and feature selection. The core idea is to enlarge the distance between different classes under the conceptual framework of LSR. First, a technique called ε-dragging is introduced to force the regression targets of different classes moving along opposite directions such that the distances between classes can be enlarged. Then, the ε-draggings are integrated into the LSR model for multiclass classification. Our learning framework, referred to as discriminative LSR, has a compact model form, where there is no need to train two-class machines that are independent of each other. With its compact form, this model can be naturally extended for feature selection. This goal is achieved in terms of L2,1 norm of matrix, generating a sparse learning model for feature selection. The model for multiclass classification and its extension for feature selection are finally solved elegantly and efficiently. Experimental evaluation over a range of benchmark datasets indicates the validity of our method.","author":[{"dropping-particle":"","family":"Xiang","given":"Shiming","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Nie","given":"Feiping","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Meng","given":"Gaofeng","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Pan","given":"Chunhong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"Changshui","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Neural Networks and Learning Systems","id":"ITEM-1","issue":"11","issued":{"date-parts":[["2012"]]},"page":"1738-1754","title":"Discriminative least squares regression for multiclass classification and feature selection","type":"article-journal","volume":"23"},"uris":["http://www.mendeley.com/documents/?uuid=3225d760-6f75-470d-872b-09eb5a4ad3ce"]}],"mendeley":{"formattedCitation":"[60]","plainTextFormattedCitation":"[60]","previouslyFormattedCitation":"[60]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[60] is a framework for computing the least square regression (LSR) for multiclass classification. The main goal of this approach is to enlarge the distances between different classes under the framework of the LSR. To do so, [26] has utilized a method called the ϵ-dragging to push the regression objective of different classes back in different directions in a way that the distance between different classes is increased. With the assumption of having n training samples {(xi,yi)}i=1nin c(≥2) classes, where xi is a datapoint in Rm and yi∈{1, 2, …,c} is the label of xi. The main goal of the DLSR is to learn the following linear function:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 36y=WTx+tNote that an arbitrary set of c independent vectors in Rc is capable of identifying c classes independently. Thus, 0/1 class label vectors cloud be used as the regression objective for the multiclass classification. In other words, for the jth class, j=1, 2,…,c, fj=0,…,0,1,0,…,0T∈Rc could be de fined by making the jth element equal to one in a way that for n training examples we would have:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 37fyi≈WTxi+t, i=1, 2, …,n,Where, W is a transformation matrix in Rm×c and t is a translation vector in Rc.In order to develop a compressed optimization method for multiclass classification, assume that B∈Rn×c be a constant matrix where the ith row and the jth column and is defined as follows:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 38Bij=+1,  if yi=j      -1,            otherwise.From the geometrical viewpoint, each element in B corresponds to a dragging direction. In other words, “+1” indicates the dragging towards the positive direction, whereas “-1” shows the dragging in the negative direction. By performing the mentioned dragging on each element of Y and recording this epsilon with matrix M, we would have the following equation:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 39XW+entT-(Y+B⊙M)≈0Where ⊙ indicates the Hadamard (or elementwise) multiplication and isen=1, 1, …,1T∈Rn a vector of ones.Now by obtaining the regularized framework of the LSR, we would have the following learning model:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 40minW,t,MXW+entT-Y-B⊙MF2+λWF2,Where, λ is a positive regularization term and .F indicates the Fresenius norm.By adding the term B⊙M in  REF _Ref501344353 \h  \* MERGEFORMAT Equation ‎240 which is related to the ϵ-dragging for enlarging the inter-class distances, this model could be used for a constrained optimization problem. Based the convex optimization theory, the convexity of  REF _Ref501344353 \h  \* MERGEFORMAT Equation ‎240 could be easily justified ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TNNLS.2012.2212721","ISSN":"2162237X","PMID":"24808069","abstract":"This paper presents a framework of discriminative least squares regression (LSR) for multiclass classification and feature selection. The core idea is to enlarge the distance between different classes under the conceptual framework of LSR. First, a technique called ε-dragging is introduced to force the regression targets of different classes moving along opposite directions such that the distances between classes can be enlarged. Then, the ε-draggings are integrated into the LSR model for multiclass classification. Our learning framework, referred to as discriminative LSR, has a compact model form, where there is no need to train two-class machines that are independent of each other. With its compact form, this model can be naturally extended for feature selection. This goal is achieved in terms of L2,1 norm of matrix, generating a sparse learning model for feature selection. The model for multiclass classification and its extension for feature selection are finally solved elegantly and efficiently. Experimental evaluation over a range of benchmark datasets indicates the validity of our method.","author":[{"dropping-particle":"","family":"Xiang","given":"Shiming","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Nie","given":"Feiping","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Meng","given":"Gaofeng","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Pan","given":"Chunhong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"Changshui","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Neural Networks and Learning Systems","id":"ITEM-1","issue":"11","issued":{"date-parts":[["2012"]]},"page":"1738-1754","title":"Discriminative least squares regression for multiclass classification and feature selection","type":"article-journal","volume":"23"},"uris":["http://www.mendeley.com/documents/?uuid=3225d760-6f75-470d-872b-09eb5a4ad3ce"]}],"mendeley":{"formattedCitation":"[60]","plainTextFormattedCitation":"[60]","previouslyFormattedCitation":"[60]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[60] and on this basis it would have one unique answer. For more details on the DLSR algorithm, one could refer to ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TNNLS.2012.2212721","ISSN":"2162237X","PMID":"24808069","abstract":"This paper presents a framework of discriminative least squares regression (LSR) for multiclass classification and feature selection. The core idea is to enlarge the distance between different classes under the conceptual framework of LSR. First, a technique called ε-dragging is introduced to force the regression targets of different classes moving along opposite directions such that the distances between classes can be enlarged. Then, the ε-draggings are integrated into the LSR model for multiclass classification. Our learning framework, referred to as discriminative LSR, has a compact model form, where there is no need to train two-class machines that are independent of each other. With its compact form, this model can be naturally extended for feature selection. This goal is achieved in terms of L2,1 norm of matrix, generating a sparse learning model for feature selection. The model for multiclass classification and its extension for feature selection are finally solved elegantly and efficiently. Experimental evaluation over a range of benchmark datasets indicates the validity of our method.","author":[{"dropping-particle":"","family":"Xiang","given":"Shiming","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Nie","given":"Feiping","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Meng","given":"Gaofeng","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Pan","given":"Chunhong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"Changshui","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Neural Networks and Learning Systems","id":"ITEM-1","issue":"11","issued":{"date-parts":[["2012"]]},"page":"1738-1754","title":"Discriminative least squares regression for multiclass classification and feature selection","type":"article-journal","volume":"23"},"uris":["http://www.mendeley.com/documents/?uuid=3225d760-6f75-470d-872b-09eb5a4ad3ce"]}],"mendeley":{"formattedCitation":"[60]","plainTextFormattedCitation":"[60]","previouslyFormattedCitation":"[60]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[60]. The ϵ-dragging method is applied as one of the key ideas in our proposed approach.Semi-supervised distance metric learningThe main purpose of semi-supervised methods is to learn the distance criterion on the data when only monitoring information is provided for a small part of the data. These algorithms use both observational and unsupervised data in the learning process. Therefore, a simple method for constructing a semi-supervised algorithm using an object such as  REF _Ref84701225 \h Equation ‎24 with the values λ1≠0, λ2≠0, in which case U(D) can be applied to all data in some way. Unsupervised and L(D) can also be made on the part of the labeled data. Finally, some constraints can be applied to the distance learning criteria to balance the two parts.  REF _Ref84773185 \h Table ‎23, shows the classification of some semi-supervised methods along with their characteristics.Table  STYLEREF 1 \s ‎2 SEQ Table \* ARABIC \s 1 3. Semi-supervised distance metric learning algorithms.LocalGlobalLinearLRML ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/CVPR.2008.4587351","author":[{"dropping-particle":"","family":"Hoi","given":"S C H","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Liu","given":"W","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Chang","given":"S.-F.","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR","id":"ITEM-1","issued":{"date-parts":[["2008"]]},"note":"cited By 86","title":"Semi-supervised distance metric learning for collaborative image retrieval","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=b27b211c-e1c1-4de6-ada0-6e207dff95c8"]}],"mendeley":{"formattedCitation":"[61]","plainTextFormattedCitation":"[61]","previouslyFormattedCitation":"[61]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[61]LRML ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/CVPR.2008.4587351","author":[{"dropping-particle":"","family":"Hoi","given":"S C H","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Liu","given":"W","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Chang","given":"S.-F.","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR","id":"ITEM-1","issued":{"date-parts":[["2008"]]},"note":"cited By 86","title":"Semi-supervised distance metric learning for collaborative image retrieval","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=b27b211c-e1c1-4de6-ada0-6e207dff95c8"]}],"mendeley":{"formattedCitation":"[61]","plainTextFormattedCitation":"[61]","previouslyFormattedCitation":"[61]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[61], CMM ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Wang","given":"F","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Chen","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Li","given":"T","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 1457–1458","id":"ITEM-1","issued":{"date-parts":[["2008"]]},"note":"cited By 1","title":"Semi-supervised metric learning by maximizing constraint margin. In: Proceedings of the 17th ACM conference on information and knowledge management","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=c2d5a482-8591-41d1-809a-347b90a1b51d"]}],"mendeley":{"formattedCitation":"[62]","plainTextFormattedCitation":"[62]","previouslyFormattedCitation":"[62]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[62]NonlinearKLRML ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/CVPR.2008.4587351","author":[{"dropping-particle":"","family":"Hoi","given":"S C H","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Liu","given":"W","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Chang","given":"S.-F.","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR","id":"ITEM-1","issued":{"date-parts":[["2008"]]},"note":"cited By 86","title":"Semi-supervised distance metric learning for collaborative image retrieval","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=b27b211c-e1c1-4de6-ada0-6e207dff95c8"]}],"mendeley":{"formattedCitation":"[61]","plainTextFormattedCitation":"[61]","previouslyFormattedCitation":"[61]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[61], SSDM ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Yang","given":"X","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Fu","given":"H","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zha","given":"H","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Barlow","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 1065–1072","id":"ITEM-1","issued":{"date-parts":[["2006"]]},"note":"cited By 1","title":"Semi-supervised nonlinear dimensionality reduction. In: 23rd International conference on machine learning","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=3021a99e-192f-44f7-b091-864734b720c2"]}],"mendeley":{"formattedCitation":"[63]","plainTextFormattedCitation":"[63]","previouslyFormattedCitation":"[63]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[63],MPCK-means ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Bilenko","given":"M","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Basu","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Proceedings of the twenty-first international conference on Machine learning","id":"ITEM-1","issued":{"date-parts":[["2004"]]},"note":"cited By 1","page":"11-18","title":"Mooney RJ (2004) Integrating constraints and metric learning in semi-supervised clustering","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=946124eb-c6e6-4baf-942c-d38d18bf09ac"]}],"mendeley":{"formattedCitation":"[64]","plainTextFormattedCitation":"[64]","previouslyFormattedCitation":"[64]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[64]KLRML ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/CVPR.2008.4587351","author":[{"dropping-particle":"","family":"Hoi","given":"S C H","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Liu","given":"W","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Chang","given":"S.-F.","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR","id":"ITEM-1","issued":{"date-parts":[["2008"]]},"note":"cited By 86","title":"Semi-supervised distance metric learning for collaborative image retrieval","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=b27b211c-e1c1-4de6-ada0-6e207dff95c8"]}],"mendeley":{"formattedCitation":"[61]","plainTextFormattedCitation":"[61]","previouslyFormattedCitation":"[61]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[61], KCMM ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Wang","given":"F","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Chen","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Li","given":"T","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 1457–1458","id":"ITEM-1","issued":{"date-parts":[["2008"]]},"note":"cited By 1","title":"Semi-supervised metric learning by maximizing constraint margin. In: Proceedings of the 17th ACM conference on information and knowledge management","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=c2d5a482-8591-41d1-809a-347b90a1b51d"]}],"mendeley":{"formattedCitation":"[62]","plainTextFormattedCitation":"[62]","previouslyFormattedCitation":"[62]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[62], SSDM ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Yang","given":"X","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Fu","given":"H","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zha","given":"H","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Barlow","given":"J","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 1065–1072","id":"ITEM-1","issued":{"date-parts":[["2006"]]},"note":"cited By 1","title":"Semi-supervised nonlinear dimensionality reduction. In: 23rd International conference on machine learning","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=3021a99e-192f-44f7-b091-864734b720c2"]}],"mendeley":{"formattedCitation":"[63]","plainTextFormattedCitation":"[63]","previouslyFormattedCitation":"[63]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[63]Laplacian Regularized Metric Learning (LRML)LRML ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/CVPR.2008.4587351","author":[{"dropping-particle":"","family":"Hoi","given":"S C H","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Liu","given":"W","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Chang","given":"S.-F.","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR","id":"ITEM-1","issued":{"date-parts":[["2008"]]},"note":"cited By 86","title":"Semi-supervised distance metric learning for collaborative image retrieval","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=b27b211c-e1c1-4de6-ada0-6e207dff95c8"]}],"mendeley":{"formattedCitation":"[61]","plainTextFormattedCitation":"[61]","previouslyFormattedCitation":"[61]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[61] is a semi-supervised method of distance learning. LRML uses the LPP formulation ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"He","given":"X","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Niyogi","given":"P","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2004"]]},"note":"cited By 678","title":"Locality preserving projections","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=95a5e1cd-6301-49a5-af8c-61aebf48403b"]}],"mendeley":{"formattedCitation":"[39]","plainTextFormattedCitation":"[39]","previouslyFormattedCitation":"[39]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[39] for the unsupervised term U(D), and for the supervised term the ANMM method ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/CVPR.2007.383124","author":[{"dropping-particle":"","family":"Wang","given":"F","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","id":"ITEM-1","issued":{"date-parts":[["2007"]]},"note":"cited By 55","title":"Feature extraction by maximizing the average neighborhood margin","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=2c3565c3-8e37-4505-8470-431d19d33fb0"]}],"mendeley":{"formattedCitation":"[50]","plainTextFormattedCitation":"[50]","previouslyFormattedCitation":"[50]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[50]. Shows the optimization problem that LRML is trying to solve. Where the smoothing term is defined as follows.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 41minMt+γ1t2-γ2t3s.t. t1≤tM≥0,where M=WWT. Considerable terms including compaction and dispersion are:Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 42t1=i,jWTxi-WTxj2ωij=tr(WTXLXTW)=tr(XLXTM)Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 43t2=(xi, xj)∈MWTxi-WTxj2= trM(xi, xj)∈Mxi-xjxi, xjTin which, M and C are must-link and cannot-link sets, respectively.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 44t3=(xi, xj)∈CWTxi-WTxj2=trM(xi, xj)∈Cxi-xjxi, xjTIn ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/CVPR.2008.4587351","author":[{"dropping-particle":"","family":"Hoi","given":"S C H","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Liu","given":"W","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Chang","given":"S.-F.","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR","id":"ITEM-1","issued":{"date-parts":[["2008"]]},"note":"cited By 86","title":"Semi-supervised distance metric learning for collaborative image retrieval","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=b27b211c-e1c1-4de6-ada0-6e207dff95c8"]}],"mendeley":{"formattedCitation":"[61]","plainTextFormattedCitation":"[61]","previouslyFormattedCitation":"[61]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[61] a semi-deterministic programming method for solving the problem of  REF _Ref84780156 \h Equation ‎241 is presented. LRML is a combination of local (unsupervised) and global (supervised) and linear methods. The distance learned is the distance of the Mahalanobis to the accuracy matrix M. Therefore, since the M accuracy matrix is learned, LRML can also be generalized to off-sample data. The computational method used in this method is quadratic programming.Constraint Margin Maximization (CRM)Similarly, CMM ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Wang","given":"F","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Chen","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Li","given":"T","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 1457–1458","id":"ITEM-1","issued":{"date-parts":[["2008"]]},"note":"cited By 1","title":"Semi-supervised metric learning by maximizing constraint margin. In: Proceedings of the 17th ACM conference on information and knowledge management","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=c2d5a482-8591-41d1-809a-347b90a1b51d"]}],"mendeley":{"formattedCitation":"[62]","plainTextFormattedCitation":"[62]","previouslyFormattedCitation":"[62]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[62] uses PCA as its unsupervised term in its objective function and ANMM for the observable term as well as LRML. Thus, the CMM optimization problem will be as follows:Where the unsupervised term t4=tr(WTXΣXTW) is the same as the PCA target. It should be noted that before applying CMM, the average of all data must be zero. The goal of CMM is to maximize data point variations while meeting the limitations of data pairs in mapped space. The authors in ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Wang","given":"F","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Chen","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Li","given":"T","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 1457–1458","id":"ITEM-1","issued":{"date-parts":[["2008"]]},"note":"cited By 1","title":"Semi-supervised metric learning by maximizing constraint margin. In: Proceedings of the 17th ACM conference on information and knowledge management","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=c2d5a482-8591-41d1-809a-347b90a1b51d"]}],"mendeley":{"formattedCitation":"[62]","plainTextFormattedCitation":"[62]","previouslyFormattedCitation":"[62]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[62] have shown that the optimal W is obtained by the standard eigenvalue decomposition process. Also shown in ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Wang","given":"F","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Chen","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"C","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Li","given":"T","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 1457–1458","id":"ITEM-1","issued":{"date-parts":[["2008"]]},"note":"cited By 1","title":"Semi-supervised metric learning by maximizing constraint margin. In: Proceedings of the 17th ACM conference on information and knowledge management","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=c2d5a482-8591-41d1-809a-347b90a1b51d"]}],"mendeley":{"formattedCitation":"[62]","plainTextFormattedCitation":"[62]","previouslyFormattedCitation":"[62]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[62] is how to obtain a kernel-based version for dealing with nonlinear data. The distance learned between xi and xj is the Euclidean distance between WTxi and WTxj. Generalization of CMM is easy for out-of-sample data, as the mapping matrix W is ​​explicitly learned, and the computational method used is eigen analysis.Equation  STYLEREF 1 \s ‎2 SEQ Equation \* ARABIC \s 1 45maxWt4-γ1t2-γ2t3s.t. WTW=I,Where the unsupervised term t4=tr(WTXΣXTW) is the PCA target function. It should be noted that all data must have a mean of zero before applying CMM. The goal of CMM is to maximize data changes in the mapped space while meeting binary constraints.Summarization and conclusion of this chapterIn this chapter, different methods of learning distance criteria and dimensional reduction are examined and classified into three categories, which are briefly described as follows:• Unsupervised methods whose learning is based on unlabeled data, all of which formulate the learning process as an optimization problem, in which the objective function can be based on spatial geometry or information theory. In all of these methods except PCA, there are release parameters such as kernel parameters for kernel-based methods, neighborhood size, or scaling parameters that must be determined in advance. From the point of view of generalization to non-sample data (data that does not have a training set), linear and core-based methods are preferable to other methods, because these methods can be directly and indirectly. Learn matrix mapping and illustration. But for methods such as Isomap, LLE, and LE that implicitly learn mapped coordinates, additional work is needed to learn the mapping matrix ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Bengio","given":"Y","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Paiement","given":"J.-F.","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Vincent","given":"P","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Delalleau","given":"O","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Roux","given":"N","non-dropping-particle":"Le","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Ouimet","given":"M","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Advances in Neural Information Processing Systems","id":"ITEM-1","issued":{"date-parts":[["2004"]]},"note":"cited By 360","title":"Out-of-sample extensions for LLE, Isomap, MDS, Eigenmaps, and spectral clustering","type":"paper-conference"},"uris":["http://www.mendeley.com/documents/?uuid=d17ffe57-1d4a-4c97-a044-d3a9df1945e8"]}],"mendeley":{"formattedCitation":"[65]","plainTextFormattedCitation":"[65]","previouslyFormattedCitation":"[65]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[65].• Supervised methods, in which learning is done on labeled training data, similar to unsupervised methods, require the initial adjustment of free parameters, and most of them involve some heavy computational processes such as parsing. Are special or semi-definitive programming. The ITML method is an exception to this, as it uses Bergman's imaging strategy, which can increase computational efficiency ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1007/s10618-014-0356-z","ISBN":"1061801403","ISSN":"13845810","abstract":"Distance metric learning is a fundamental problem in data mining and knowledge discovery.Many representative data mining algorithms, such as k-nearest neighbor classifier, hierarchical clustering and spectral clustering, heavily rely on the underlying distance metric for correctly measuring relations among input data. In recent years, many studies have demonstrated, either theoretically or empirically, that learning a good distance metric can greatly improve the performance of classification, clustering and retrieval tasks. In this survey, we overview existing distance metric learning approaches according to a common framework. Specifically, depending on the available supervision information during the distance metric learning process, we categorize each distance metric learning algorithm as supervised, unsupervised or semi-supervised.We compare those different types of metric learning methods, point out their strength and limitations. Finally, we summarize open challenges in distance metric learning and propose future directions for distance metric learning.","author":[{"dropping-particle":"","family":"Wang","given":"Fei","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Sun","given":"Jimeng","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Data Mining and Knowledge Discovery","id":"ITEM-1","issue":"2","issued":{"date-parts":[["2014"]]},"number-of-pages":"534-564","title":"Survey on distance metric learning and dimensionality reduction in data mining","type":"book","volume":"29"},"uris":["http://www.mendeley.com/documents/?uuid=59f68543-9151-478c-ad96-bb120d81de5b"]}],"mendeley":{"formattedCitation":"[66]","plainTextFormattedCitation":"[66]","previouslyFormattedCitation":"[66]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[66].• Semi-supervised methods, in which the algorithm accesses a large number of unlabeled samples for which no additional information is available, can be used as a combination of supervised methods and Considered without an observer. These methods can be used when the supervised information on the data is very limited and sparse. However, not all regulatory information is necessarily useful for distance learning. Accordingly, some researchers have conducted research on the usefulness of supervised information ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Davidson","given":"I","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Wagstaff","given":"K L","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Basu","given":"S","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","id":"ITEM-1","issued":{"date-parts":[["2006"]]},"note":"cited By 75","page":"115-126","title":"Measuring constraint-set utility for partitional clustering algorithms","type":"article-journal","volume":"4213 LNAI"},"uris":["http://www.mendeley.com/documents/?uuid=d3a932e7-6390-457a-be1c-c17bc64b82f7"]}],"mendeley":{"formattedCitation":"[67]","plainTextFormattedCitation":"[67]","previouslyFormattedCitation":"[67]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[67]. Research has also been done on when unlabeled data can be useful ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"author":[{"dropping-particle":"","family":"Singh","given":"A","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Nowak","given":"R D","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhu","given":"X","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"pp 1513–1520","id":"ITEM-1","issued":{"date-parts":[["2008"]]},"note":"cited By 1","title":"Unlabeled data: now it helps, now it doesn’t. In: Advances in neural information processing systems","type":"article-journal"},"uris":["http://www.mendeley.com/documents/?uuid=2126dc6b-f73d-4330-a5b0-1b2eeed88a40"]}],"mendeley":{"formattedCitation":"[68]","plainTextFormattedCitation":"[68]","previouslyFormattedCitation":"[68]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[68]. In practice, the balance between regulatory data usage and unlabeled data will vary depending on the data distribution infrastructure. And as far as we know, there are no universal rules for optimizing parameters.MethodologyIntroductionThis chapter will describe the proposed method of distance metric learning in detail. The proposed method tries to learn the distance metric in way that the structures between the data-points are preserved as much as possible. In this approach, in order to encounter with the problem of the imbalanced distributions of different classes, for each given data point, two neighborhoods are created, each of which consisting of the data with similar and dissimilar labels to the given data point, respectively. The proposed method tries to preserve the spatial locality of the similar data in relation with each other and to push back the dissimilar data from each data-point. On this basis, and with respect to the fact that the number of the data points in the similar neighborhood is equal to the number of points in the dissimilar neighborhood, the problem of the imbalanced data distributions could also be covered.Proposed methodAs it can be seen in  REF _Ref84170916 \h Figure ‎31, in the proposed method, in order to increase the manifold and distance metric learning speed besides reaching a feasible amount of system memory on today’s computers, first the number of the training data is down-sampled, otherwise the size of the similarity matrix would as big and bulky that it could not be implemented and, as a result, the execution of the proposed method would not be possible. In order to encounter with such big data, a uniform random sampling of the training data is preformed through which the share of each class in training samples will be remained intact. The down-sampling factor is considered to be 0.1 of all samples.After sample reduction, manifold learning is conducted on these data using one of the manifold learning approaches in order to extract the local neighborhoods of the nodes based on their adjacencies on the manifold. Consequently, based on these extracted local neighborhoods, two neighborhoods are created for each given data point. As it can be seen in  REF _Ref84170944 \h Figure ‎32, one of the created neighborhoods is dedicated to the data with the same label whereas the other neighborhood consists of the dissimilar neighbors to the given data point. Other data points are regarded as so called unrelated set. Finally, as it is depicted in  REF _Ref84170957 \h Figure ‎33, distance metric learning based on the initial coordinates of the given data point in ambient space and with respect to the similarity and dissimilarity relations thanks to constructed similar and dissimilar neighborhoods is conducted in a way that the similar data points to the given point would be more close to it than the other dissimilar points. Figure  STYLEREF 1 \s ‎3 SEQ Figure \* ARABIC \s 1 1. Overall process of the proposed method.Figure  STYLEREF 1 \s ‎3 SEQ Figure \* ARABIC \s 1 2. The local patch consisting of the dissimilar neighbors.Figure  STYLEREF 1 \s ‎3 SEQ Figure \* ARABIC \s 1 3. The local neighborhoods after distance metric learning with the proposed approach.Data neighborhood structures after distance metric learningAfter discriminating the similar and dissimilar neighborhoods, as well as the unrelated data points which are not contained in either of similar and dissimilar neighborhoods, they are ordered as shown in  REF _Ref84171013 \h Figure ‎34, based on their distance to the given data point and the following relation vector is created.UiDiSixiFigure  STYLEREF 1 \s ‎3 SEQ Figure \* ARABIC \s 1 4. The representation of data points after manifold embedding and similarity calculation.In  REF _Ref84171013 \h Figure ‎34, xi shows the given data point, Si shows the faraway points from xi  with the similar class labels and, Di shows the neighbors with the dissimilar data and Ui indicates the unrelated data which are not included in either of the similar and dissimilar sets with respect to the given data point. In the other words, if a data point is not a member of either of their similar or dissimilar neighborhoods, it is said to be unrelated.At this stage one of the distance metric learning methods e.g., the Mahalanobis distance, could be used. In the proposed framework, we have adopted the Discrete Least Square Regression (DLSR), proposed in ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TNNLS.2012.2212721","ISSN":"2162237X","PMID":"24808069","abstract":"This paper presents a framework of discriminative least squares regression (LSR) for multiclass classification and feature selection. The core idea is to enlarge the distance between different classes under the conceptual framework of LSR. First, a technique called ε-dragging is introduced to force the regression targets of different classes moving along opposite directions such that the distances between classes can be enlarged. Then, the ε-draggings are integrated into the LSR model for multiclass classification. Our learning framework, referred to as discriminative LSR, has a compact model form, where there is no need to train two-class machines that are independent of each other. With its compact form, this model can be naturally extended for feature selection. This goal is achieved in terms of L2,1 norm of matrix, generating a sparse learning model for feature selection. The model for multiclass classification and its extension for feature selection are finally solved elegantly and efficiently. Experimental evaluation over a range of benchmark datasets indicates the validity of our method.","author":[{"dropping-particle":"","family":"Xiang","given":"Shiming","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Nie","given":"Feiping","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Meng","given":"Gaofeng","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Pan","given":"Chunhong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"Changshui","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Neural Networks and Learning Systems","id":"ITEM-1","issue":"11","issued":{"date-parts":[["2012"]]},"page":"1738-1754","title":"Discriminative least squares regression for multiclass classification and feature selection","type":"article-journal","volume":"23"},"uris":["http://www.mendeley.com/documents/?uuid=3225d760-6f75-470d-872b-09eb5a4ad3ce"]}],"mendeley":{"formattedCitation":"[60]","plainTextFormattedCitation":"[60]","previouslyFormattedCitation":"[60]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[60] and modified the approach in it in order to be compatible with the proposed distance metric learning. Having the above similar/dissimilar/unrelated sets the proposed approach can be formulated as the following optimization problem as inspired from [26]:Equation  STYLEREF 1 \s ‎3 SEQ Equation \* ARABIC \s 1 1minXW+entT-Y-B⊙MF2+λWF2In our proposed method, X∈Rn×m is the input data matrix, W∈Rm×n is the transformation matrix to the similarity space (resulted from the distance metric learning) and en=1, 1, …,1T∈Rn is a vector consisting of ones. Also, Y∈Rn×n and B∈Rn×n are two constant matrices each of which are in the ith row and the jth column as follows:Equation  STYLEREF 1 \s ‎3 SEQ Equation \* ARABIC \s 1 2Yi,j=1,  if li=j , j∈Psi0,                   otherwise Equation  STYLEREF 1 \s ‎3 SEQ Equation \* ARABIC \s 1 3Bi,j=+1,     if li=j , j∈Psi-1,     if li=j , j∈Pdi0,                   otherwiseWhere Psi and Pdi show the similar and dissimilar sets for each given data point. In other words, each element  Yi,j will be equal to one in case that the jth data point which has the same label as i be in the furthest neighborhood of the given it. Also, matrix B shows the similar/dissimilar/unrelated set (+1,-1 and 0 respectively) information as gathered from the previous stage. The other matrices and variables included in ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TNNLS.2012.2212721","ISSN":"2162237X","PMID":"24808069","abstract":"This paper presents a framework of discriminative least squares regression (LSR) for multiclass classification and feature selection. The core idea is to enlarge the distance between different classes under the conceptual framework of LSR. First, a technique called ε-dragging is introduced to force the regression targets of different classes moving along opposite directions such that the distances between classes can be enlarged. Then, the ε-draggings are integrated into the LSR model for multiclass classification. Our learning framework, referred to as discriminative LSR, has a compact model form, where there is no need to train two-class machines that are independent of each other. With its compact form, this model can be naturally extended for feature selection. This goal is achieved in terms of L2,1 norm of matrix, generating a sparse learning model for feature selection. The model for multiclass classification and its extension for feature selection are finally solved elegantly and efficiently. Experimental evaluation over a range of benchmark datasets indicates the validity of our method.","author":[{"dropping-particle":"","family":"Xiang","given":"Shiming","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Nie","given":"Feiping","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Meng","given":"Gaofeng","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Pan","given":"Chunhong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"Changshui","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Neural Networks and Learning Systems","id":"ITEM-1","issue":"11","issued":{"date-parts":[["2012"]]},"page":"1738-1754","title":"Discriminative least squares regression for multiclass classification and feature selection","type":"article-journal","volume":"23"},"uris":["http://www.mendeley.com/documents/?uuid=3225d760-6f75-470d-872b-09eb5a4ad3ce"]}],"mendeley":{"formattedCitation":"[60]","plainTextFormattedCitation":"[60]","previouslyFormattedCitation":"[60]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[60], as well as the calculations of transformation matrix, W, are done precisely based on the assumptions contained in the DLSR algorithm ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TNNLS.2012.2212721","ISSN":"2162237X","PMID":"24808069","abstract":"This paper presents a framework of discriminative least squares regression (LSR) for multiclass classification and feature selection. The core idea is to enlarge the distance between different classes under the conceptual framework of LSR. First, a technique called ε-dragging is introduced to force the regression targets of different classes moving along opposite directions such that the distances between classes can be enlarged. Then, the ε-draggings are integrated into the LSR model for multiclass classification. Our learning framework, referred to as discriminative LSR, has a compact model form, where there is no need to train two-class machines that are independent of each other. With its compact form, this model can be naturally extended for feature selection. This goal is achieved in terms of L2,1 norm of matrix, generating a sparse learning model for feature selection. The model for multiclass classification and its extension for feature selection are finally solved elegantly and efficiently. Experimental evaluation over a range of benchmark datasets indicates the validity of our method.","author":[{"dropping-particle":"","family":"Xiang","given":"Shiming","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Nie","given":"Feiping","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Meng","given":"Gaofeng","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Pan","given":"Chunhong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"Changshui","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Neural Networks and Learning Systems","id":"ITEM-1","issue":"11","issued":{"date-parts":[["2012"]]},"page":"1738-1754","title":"Discriminative least squares regression for multiclass classification and feature selection","type":"article-journal","volume":"23"},"uris":["http://www.mendeley.com/documents/?uuid=3225d760-6f75-470d-872b-09eb5a4ad3ce"]}],"mendeley":{"formattedCitation":"[60]","plainTextFormattedCitation":"[60]","previouslyFormattedCitation":"[60]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[60].As you can see in  REF _Ref84170916 \h Figure ‎31, after calculating the mapping matrix W, all the training/test data are mapped to the similarity space using the following equation.Equation  STYLEREF 1 \s ‎3 SEQ Equation \* ARABIC \s 1 4X'=X×W+en×tTWhere X'  is the transformed data matrix, showing the data mapped onto the similarity space and also t∈Rn is a translation vector according to the assumptions in ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TNNLS.2012.2212721","ISSN":"2162237X","PMID":"24808069","abstract":"This paper presents a framework of discriminative least squares regression (LSR) for multiclass classification and feature selection. The core idea is to enlarge the distance between different classes under the conceptual framework of LSR. First, a technique called ε-dragging is introduced to force the regression targets of different classes moving along opposite directions such that the distances between classes can be enlarged. Then, the ε-draggings are integrated into the LSR model for multiclass classification. Our learning framework, referred to as discriminative LSR, has a compact model form, where there is no need to train two-class machines that are independent of each other. With its compact form, this model can be naturally extended for feature selection. This goal is achieved in terms of L2,1 norm of matrix, generating a sparse learning model for feature selection. The model for multiclass classification and its extension for feature selection are finally solved elegantly and efficiently. Experimental evaluation over a range of benchmark datasets indicates the validity of our method.","author":[{"dropping-particle":"","family":"Xiang","given":"Shiming","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Nie","given":"Feiping","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Meng","given":"Gaofeng","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Pan","given":"Chunhong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"Changshui","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Neural Networks and Learning Systems","id":"ITEM-1","issue":"11","issued":{"date-parts":[["2012"]]},"page":"1738-1754","title":"Discriminative least squares regression for multiclass classification and feature selection","type":"article-journal","volume":"23"},"uris":["http://www.mendeley.com/documents/?uuid=3225d760-6f75-470d-872b-09eb5a4ad3ce"]}],"mendeley":{"formattedCitation":"[60]","plainTextFormattedCitation":"[60]","previouslyFormattedCitation":"[60]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[60].Figure  STYLEREF 1 \s ‎3 SEQ Figure \* ARABIC \s 1 5. The general process in metric learning.The objective of distance metric learning As it is shown in  REF _Ref84170944 \h Figure ‎32 and  REF _Ref84170957 \h Figure ‎33, the main objective of a distance metric learning algorithm, is to learn the parameters of the metric which are best suited for the constraints in such a way that it is the best approximation of the distance embedded between the data points. Distance metric learning is commonly expressed as an optimization problem, as the general form below:Equation  STYLEREF 1 \s ‎3 SEQ Equation \* ARABIC \s 1 5minMlM,S,D,R+λR(M)Where lM,S,D,R is a loss function that acquires a penalty in case the training constraints are violated and RM regularizes the parameters M of the learned metric and λ≥0 is a regularization parameter.After the learning phase, the resulted function is used to improve the performance of a metric-based algorithm, which is most commonly k-Nearest Neighbors (k-NN). The main goal of using the k-NN is to preserve the symmetry in the distance metric learning phase, with the sense that, as seen in  REF _Ref84170944 \h Figure ‎32 and  REF _Ref84170957 \h Figure ‎33, the number of the similar neighbors is equal with the number of the neighbors from other classes around each data point. As a result, the supremacy of the proposed method is that it learns the distance metric in a balanced way as it uses an equal number of the similar and dissimilar data points to learn the distance metric.Advantages of the proposed methodOne of the advantages of the proposed method is that it preserves the structure of similar local data, so that the proposed method tries to locate the data that are in the vicinity of each point and on the patch of data with the same label as the data point. Have the least change of location and remove only neighboring data labeled as dissimilar to the data point. Thus, when entering test data, this data will be mapped in the new space with minimal change in location relative to their neighbors.Another advantage of the proposed method is that the distance learning criterion is balanced over similar and dissimilar data. Because in the proposed method, considering the number of identical data points on similar and dissimilar patches with each data point, an attempt has been made to learn the distance criterion by observing the balance between similar and dissimilar samples with each data point.ExperimentsIntroductionThis chapter will make a comparison between the proposed method and the Discriminative Least Squares Regression (DLSR) ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1109/TNNLS.2012.2212721","ISSN":"2162237X","PMID":"24808069","abstract":"This paper presents a framework of discriminative least squares regression (LSR) for multiclass classification and feature selection. The core idea is to enlarge the distance between different classes under the conceptual framework of LSR. First, a technique called ε-dragging is introduced to force the regression targets of different classes moving along opposite directions such that the distances between classes can be enlarged. Then, the ε-draggings are integrated into the LSR model for multiclass classification. Our learning framework, referred to as discriminative LSR, has a compact model form, where there is no need to train two-class machines that are independent of each other. With its compact form, this model can be naturally extended for feature selection. This goal is achieved in terms of L2,1 norm of matrix, generating a sparse learning model for feature selection. The model for multiclass classification and its extension for feature selection are finally solved elegantly and efficiently. Experimental evaluation over a range of benchmark datasets indicates the validity of our method.","author":[{"dropping-particle":"","family":"Xiang","given":"Shiming","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Nie","given":"Feiping","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Meng","given":"Gaofeng","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Pan","given":"Chunhong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Zhang","given":"Changshui","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"IEEE Transactions on Neural Networks and Learning Systems","id":"ITEM-1","issue":"11","issued":{"date-parts":[["2012"]]},"page":"1738-1754","title":"Discriminative least squares regression for multiclass classification and feature selection","type":"article-journal","volume":"23"},"uris":["http://www.mendeley.com/documents/?uuid=3225d760-6f75-470d-872b-09eb5a4ad3ce"]}],"mendeley":{"formattedCitation":"[60]","plainTextFormattedCitation":"[60]","previouslyFormattedCitation":"[60]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[60] and some other fundamental  methods of dimensionality reduction.DatasetIn order to evaluate the proposed method in this research the following numeric datasets which are obtained from the UCI repository of machine learning are employed.Table  STYLEREF 1 \s ‎4 SEQ Table \* ARABIC \s 1 1. The properties of the datasets.Dataset#Samples# Class# FeaturesImbalance ratioVehicle8464181.09Bupa345261.37Glass214688.44Ionosphere3512341.78Iris150341KDD4940215417528.03Monks124261New-thyroid215355Pima768281.86WDBC5692301.68Wholesale440272.09Wine1783131.47In which the imbalance ratio is the proportion of the population of the majority class to the population of the minority class which could be calculated from the following equation.Equation  STYLEREF 1 \s ‎4 SEQ Equation \* ARABIC \s 1 1RIm=nmajornminorWhere RIm is the imbalance ratio and nmajor and nminor are the population of the majority class to the minority class, respectively. Evaluation criteriaIn order to compare the proposed method with other approaches we have employed the following evaluation criteria:Accuracy or the correct rate is the proportion of the correctly classified data to the total number of the items in the dataset.Equation  STYLEREF 1 \s ‎4 SEQ Equation \* ARABIC \s 1 2ACC=TP+TNTP+TN+FP+FNSensitivity, true positive rate (TPR), recall, or the hit rate, is the proportion of the data which are correctly classified in the positive class to the total of the positive data.Equation  STYLEREF 1 \s ‎4 SEQ Equation \* ARABIC \s 1 3SEN=TPTP+FNSpecificity or true negative rate is the proportion of the negative points which are correctly classified in the negative class to the total number of the negative samples.Equation  STYLEREF 1 \s ‎4 SEQ Equation \* ARABIC \s 1 4SPC=TNTN+FPEvaluation scenarios and experimental resultsIn this chapter we will analyze and make a comparison between the performance results of the proposed method and some other well-known approaches of distance metric learning and dimensionality reduction and also the original DLSR algorithm with respect to the evaluation measures. To do this, the 10-fold cross validation is utilized. The results are based on the performance of the two k-NN classifier and SVM classifier with the RBF kernel. The accuracy of different approaches including DLSR and the proposed approach are depicted in Table 2. In these experiments the proposed method has employed different manifold learning approaches such as PCA, LDA, MDS, Isomap, LLE, Kernel PCA and Autoencoder. The experiments are performed for different latent dimensions and the best results are reported in the tables. Note that, in the following tables (d, R) respectively show the best latent dimension and the rank of the method on the corresponding dataset.Table  STYLEREF 1 \s ‎4 SEQ Table \* ARABIC \s 1 2. Accuracy comparison between different approaches versus the proposed using 10-fold cross validation and 7-NN classifier with (d,r) indicating the best latent dimensionality and the rank of the approach, respectively (AE denotes auto-encoder approach).DatasetDimensionality ReductionFeature SelectionProposed Method PCA LLE Kernel PCA AE Fisher Gini DLSR PCA LDA MDS Isomap LLE Kernel PCA AEVehicle 0.6823 (13, 9)0.6117 (17, 12) 0.2588 (9, 14)0.5294(5, 13)0.6823(17, 9)0.6705 (17, 11)0.9183 (17, 1)0.8235 (1 ,4)0.8705 (1, 3)0.8235 (1, 4)0.8235 (1, 4)0.8941 (13, 2) 0.8      (1, 8) 0.8235 (13, 4)Bupa0.5714(1, 12)0.6571 (3, 9)0.4285(5, 14)0.5714(3, 12)0.6857(5, 7) 0.7428(1, 9)0.7573(3, 2)0.6285(1, 10) 0.7714(5, 1) 0.6285(1, 10) 0.6857(1, 7) 0.7142 (3, 5) 0.7142 (1, 5) 0.7428(5, 3)Glass0.5454(1, 10)0.5454(5, 10)0.5(7, 12)0.4545(1, 13)0.7272(9, 3)0.7272(9, 3)NA0.7272(1, 3)0.7272(3, 3)0.7272(1, 3)0.7272(5, 3)0.7727(3, 1)0.7727(5, 1)0.7272(9, 3)Ionosphere0.8888(8, 11)0.8055(22, 14)0.9166(8, 5)0.8888 (15, 11)0.9166(15, 5)0.9166(15, 5)0.8694 (29, 13)0.9166 (15, 5)0.9722(1, 1)0.9166 (15, 5)0.9444 (15, 4)0.9722 (29, 1)0.9722(1, 1)0.9166 (29, 5)Iris1 (1, 1)1(2, 1)1(2, 1)1 (1, 1)1(2, 1)1(2, 1)NA1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)KDD0.9879(1, 10)0.9839(28, 12)0.7915(10, 14)0.9819(37, 13)0.9919(10, 6)0.9919(19, 6)0.9901(28, 9)0.9939(10, 2)0.9939 (10, 10)0.9939(1, 2)0.9939(1, 2)0.9959(19, 1)0.9939(10, 2)0.9919(1, 6)Monks0.8333 (5, 8)0.9166 (5, 4)0.8333 (3, 8)0.5      (1, 14)0.5833 (3, 12)0.5833 (3, 12)0.7916(5, 11)1(5, 1)0.8333(3, 8)1(5, 1)0.9166(5, 4)0.9166(3, 4)0.9166(3, 4)1(3, 1)New-thyroid0.9545(5, 1)0.9090(3, 3)0.5909(1, 13)0.9090 (3, 3)0.9090(1, 3)0.9090(1, 3)NA0.9090(1, 3)0.9090(1, 3)0.9090(1, 3)0.9090(1, 3)0.9090(1, 3)0.9545(5, 1)0.9090(1, 3)Pima0.7532(3, 10)0.7142(7, 12)0.6493(3, 14)0.6883(7, 13)0.7922(1, 2)0.7922(1, 2)0.7597(1, 9)0.7792(5, 4)0.8051(1, 1)0.7792(5, 4)0.7792(7, 4)0.7792(1, 4)0.7792(3, 4)0.7532(7, 10)WDBC0.9298(7, 9)0.9122(8, 12)0.6315 (22, 14)0.8596(8, 13)0.9298(22, 9)0.9298(22, 9)0.9807(1, 4)0.9473(1, 5)0.9473(1, 5)0.9473(1, 5)0.9473(1, 5)0.9824(15, 1)0.9824(1, 1)0.9824(1, 1)Wine0.7777(4, 9)0.9444(13, 8)0.3888(1, 13)0.7222(1, 12)0.7777(13, 9)0.7777(13, 9)NA1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(4, 1)1(1, 1)Wholesale1(3, 1)0.9545(3, 11)0.6818(3, 13)0.7045(5, 12)1(5, 1)1(7, 1)NA1(1, 1)0.9772(1, 8)1(1, 1)1(3, 1)1(7, 1)0.9772(1, 8)0.9772(1, 8)Average rank7.66911.2510.835.675.57.143.333.083.333.252.083.083.92As it can be seen in  REF _Ref84786982 \h Table ‎42, from the total of 12 experiments on different datasets, the proposed method of distance metric learning using the LLE, Kernel PCA and LDA approaches for manifold learning has gained the first rank on 7, 6 and 5 datasets, respectively. While, under the same circumstances the other methods such as the pure manifold learning, feature selection and the DLSR have achieved the best accuracy only in one experiment which is still equal to the result of the proposed method.Therefore, from total of 12 experiments, the proposed framework, has totally gained the first rank, whereas the base approaches have the first rank only in one experiment which is a testimony of the absolute excellence of the proposed approach from the accuracy viewpoint using 7-NN classifier.Also, with respect to the fact that among different manifold learning methods combined with DML, LLE has gained the maximum rank, it could be concluded that this approach has got the best performance in finding the structural neighborhoods in comparison with the other manifold learning approaches in terms of the accuracy using the 7-NN classifier.Table  STYLEREF 1 \s ‎4 SEQ Table \* ARABIC \s 1 3. Sensitivity comparison between different approaches versus the proposed using 10-fold cross validation and 7-NN classifier with (d,r) indicating the best dimensionality and the rank of the approach, respectively (AE denotes auto-encoder approach).DatasetDimensionality ReductionFeature SelectionProposed Method PCA LLEKernel PCAAE Fisher Gini DLSR PCA LDA MDS  Isomap LLE Kernel PCAAEVehicle 0.95 (13, 9) 0.75(4, 14) 1(1, 1) 0.95(9, 9) 0.9846(5, 7) 0.9692(17, 8)1(13, 1) 0.95(1, 9)1 (1, 1) 0.95(1, 9) 0.95(5, 9)1(1, 1)1(1, 1)1(13, 1)Bupa0.4667(3, 13)0.6666(3, 6)1(1, 1)0.5333(5, 9)0.75(5, 4)0.8(1, 2)0.7616 (3,3)0.5333 (1, 9)0.7333 (5, 5)0.5333(1, 9)0.5333 (1, 9) 0.6 (3, 7)0.4667 (1, 13)0.6     (3, 7)Glass0.8571(1, 8)0.8571(1, 8)0.5(7, 13)0.7142(1, 12)0.8571(5, 8)0.8571(5, 8) NA1(3, 1)1(5, 1)1(3, 1)1(1, 1)1(3, 1)1(3, 1)1(1, 1)Ionosphere0.9565(8, 10)0.9565 (22, 10)0.9130(1, 14)0.9522 (15, 10)1(1, 1)1(1, 1)0.9913 (29, 9)1(15, 1)1(1, 1)1(15, 1)1(1, 1)1(1, 1)0.9522(1, 10)1(29, 1)Iris1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)NA1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)KDD1(10, 1)1(10, 1)1(10, 1)1(19, 1)0.9919(10, 14)0.9974(10, 12)0.9971(19, 13)1(10, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(10, 1)1(1, 1)Monks1(5, 1)1(1, 1)0.8333(3, 12)1(3, 1)1(5, 1)1(1, 1)0.7333(5, 13)1(5, 1)0.6666(1, 14)1(5, 1)1(5, 1)1(3, 1)1(3, 1)1(3, 1)New-thyroid1(5, 1)1(1, 1)0.8666(1, 13)1(5, 1)1(1, 1)1(1, 1)NA1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)Pima 0.4444(3, 13) 0.4444 (3, 13) 1(1, 1) 0.9259 (3, 3) 0.7037 (1, 3) 0.7037 (1, 3) 0.6185(1, 5)0.5185(1, 8)0.5555(7, 6)0.5185(5, 8)0.5185(1, 8)0.5555 (1, 8)0.5185(1, 8)0.5185(5, 8)WDBC1(8, 1)1(8, 1)1(22, 1)1(1, 1)1(22, 1)1(22, 1)0.9861(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)Wine 0.85 (1, 10) 0.8333(4, 11)1(4, 1)1(4, 1)0.8333 (4, 11)0.8333 (1, 11)NA1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)Wholesale1(3, 1)0.9666(3, 11)1(3, 1)1(1, 1)1(5, 1)1(7, 1)NA1(1, 1)0.9666(1, 11)1(1, 1)1(3, 1)1(7, 1)0.9666(1, 11)1(5, 1)Average rank5.756.554.084.424.428.292.923.672.922.921.9242.08 REF _Ref84787023 \h Table ‎43, denotes the comparison between the proposed methods and other approaches of distance metric learning and dimensionally reduction in term of sensitivity. As it can be seen in  REF _Ref84787023 \h Table ‎43, from the total of 12 experiments on different datasets, the proposed method of distance metric learning using LLE, Auto-encoder and the PCA approaches of manifold learning has gained the first rank on 10, 10 and 9 datasets, respectively. Whereas, under the same circumstances from the other methods, approaches such Auto-encoder, Gini and Fisher has gained the first rank in 7, 6 and 6 experiments, respectively. Table  STYLEREF 1 \s ‎4 SEQ Table \* ARABIC \s 1 4. Specificity comparison between different approaches versus the proposed using 10-fold cross validation and 7-NN classifier with (d,r) indicating the best dimensionality and the rank of the approach, respectively (AE denotes auto-encoder approach).DatasetDimensionality ReductionFeature SelectionProposed MethodPCA0LLEKernel PCA AE Fisher Gini DLSR PCA LDA MDS Isomap  LLE Kernel PCA  AEVehicle 0.9692 (13, 11) 0.8923 (17, 13)1 (5, 1)0.8307(1, 14)0.9846(5, 10)0.9692 (17, 11)1(17, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)Bupa 0.7 (1, 11)0.65 (3, 13)0(1, 14)0.7(1, 11)0.75(5, 8)0.8(1, 3)0.7564(3, 7)0.75(3, 8)0.8(1, 3)0.75(3, 8)0.8(1, 3)0.8(3, 3)0.9(1, 1)0.9(5, 1)Glass0.6666 (7, 13)0.7333(3, 10)0.8(1, 1)0.8(5, 1)0.7333(1, 10)0.7333(1, 10)NA0.8(1, 1)0.8(3, 1)0.8(1, 1)0.8(3, 1)0.8(1, 1)0.8(5, 1)0.8(1, 1)Ionosphere0.7692(8, 8)0.6153(15, 14)1(8, 1)0.7692(15, 8)0.8461(15, 4)0.8461(15, 4)0.7076(8, 13)0.7692(1, 8)0.9230(1, 2)0.7692(1, 8)0.8461(15, 4)0.9230(29, 2)0.8461 (1, 4)0.7692(1, 8)Iris1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)NA1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)KDD0.9906(10, 4)0.9439(10, 13)0.0373 (10, 14)1(37, 1)1(10, 1)0.9906(10, 4)0.9953(37, 3)0.9906(10, 4)0.9906(1, 4)0.9906(1, 4)0.9906(1, 4)0.9906(1, 4)0.9906(1, 4)0.9906(1, 4)Monks0.8333(3, 9)0(1, 14)0.8333(1, 9)1(3, 1)0.6666(5, 12)0.6666(5, 12)0.85(5, 8)1(1, 1)1(1, 1)1(3, 1)1(1, 1)0.8333(1, 9)1(1, 1)1(1, 1)New-thyroid0.8571(3, 2)0.7142(3, 7)0.8666(1, 1)0.8571(5, 2)0.8571(3, 2)0.8571(3, 2)NA0.7142(1, 7)0.7142(1, 7)0.7142(1, 7)0.7142(1, 7)0.7142(1, 7)0.8571(5, 2)0.7142(1, 7)Pima0.92(5, 5)0.9(7, 12)1(3, 11)0.82(7, 14)0.92(3, 5)0.92(3, 5)0.836(7, 13)0.92(5, 5)0.96(1, 2)0.92(5, 5)0.92(7, 5)0.94(7, 3)0.92(3, 5)0.94(7, 3)WDBC1(1, 1)0.7619(1, 13)1(1)0.7619(8, 13)0.8095(8, 11)0.8095(8, 11)0.9714(1, 3)0.8571(1, 7)0.8571(1, 7)0.8571(1, 7)0.8571(1, 7)0.9523(15, 4)0.9523(15, 4)0.9523(1, 4)Wine1(4, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)NA1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)Wholesale1(3, 1)0.9285(3, 12)1(1, 1)0.0714(5, 13)1(5, 1)1(5, 1)NA1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)Average rank5.5810.253.836.675.55.426.863.752.583.7533.012.172.75In a comparison between the proposed method and the base methods in terms of the specificity according to  REF _Ref84787081 \h Table ‎44, from the total of the 12 experiments, thanks to the data mapping on the manifold learnt by the kernel PCA, LDA, and Auto-encoder methods, the proposed method has gained the first rank in 7, 6, and 7 experiments, respectively. Yet, under the same circumstances out of the base methods of manifold learning, the methods of Kernel PCA, Gini index, and Fisher have earned the first rank in 5, 3, and 4 experiments, respectively.Also, the proposed methods have gained a lower on average rank than the base approaches. Also, we can claim kernel PCA as the best approach in finding the neighborhoods on the manifold in terms of the specificity by using the 7-NN classifier.Table  STYLEREF 1 \s ‎4 SEQ Table \* ARABIC \s 1 5. Accuracy comparison between different approaches versus the proposed using 10-fold cross validation and SVM classifier with (d,r) indicating the best latent dimensionality and the rank of the approach, respectively (AE denotes auto-encoder approach).DatasetDimensionality ReductionFeature SelectionProposed MethodPCALLEKernel PCAAEFisherGiniPCALDAMDSIsomapLLEKernel PCAAEVehicle0.4588(1, 9)0.4470(17, 10)0.2588(1, 13)0.3411(13, 11)0.4941(5, 8)0.3058(5, 12)0.8(5, 6)0.8235 (1, 2)0.8(5, 6)0.8117 (5, 4)0.8470 (1, 1)0.8117 (9, 4)0.8235 (13, 2)Bupa0.6(3, 9)0.5714(1, 11)0.5714(1, 11)0.5714(1, 11)0.6(3, 9)0.7714(1, 1)0.7142 (1, 2)0.6857(5, 5)0.7142 (1, 2)0.7142(1, 2)0.6857(1, 5)0.6571(5, 8)0.6857(3, 5)Glass0.5909(7, 1)0.5454(9, 2)0.5454(9, 2)0.3181(9, 13)0.5454(7, 2)0.4545(5, 12)0.5238(1, 5)0.5238(1, 5)0.5238(1, 5)0.5238(1, 5)0.5238(3, 5)0.5238(3, 5)0.5238(1, 5)Ionosphere0.9444(15, 7)0.7222 (22, 12)0.9166 (22, 10)0.6388 (8, 13)0.9722(8, 1)0.9444(8, 7)0.9722(15, 1)0.9722(1, 1)0.9722(15, 1)0.9166(8, 10)0.9722(15, 1)0.9444(1, 7)0.9722(22, 1)Iris1(1, 1)1(2, 1)0.9333(2, 11)0.6666(1, 13)1(1, 1)1(1, 1)1(1, 1)0.9333(1, 11)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)KDD0.9779(1, 9)0.9839(19, 8)0.7855(1, 13)0.9338(37, 12)0.9378(19, 11)0.9679(1, 10)0.9939(1, 3)0.9939(10, 3)0.9939(1, 3)0.9919(1, 7)0.9959(1, 1)0.9939(10, 3)0.9959(1, 1)Monks0.8333(3, 5)0.5(1, 13)0.8333(5, 5)0.75(1, 11)0.9166(5, 2)0.9166(5, 2)0.8333(3, 5)0.75(1, 11)0.8333(3, 5)0.9166(5, 2)0.8333(3, 5)0.8333(3, 5)1(3, 1)New-thyroid0.7272(1, 12)0.8636(5, 9)0.6818(1, 13)0.8181(5, 11)0.9090(1, 6)0.9090(1, 6)0.9545(5, 1)0.8636(1, 9)0.9545(5, 1)0.9545(5, 1)0.9545(5, 1)0.9545(5, 1)0.9090(3, 6)Pima0.6883(1, 10)0.6623(7, 11)0.6493(1, 12)0.6493(1, 12)0.7272(1, 8)0.7272(1, 8)0.7532(3, 2)0.7662(1, 1)0.7532(3, 2)0.7532(3, 2)0.7532(3, 2)0.7532(1, 2)0.7532(3, 2)WDBC0.6491(1, 8)0.8947 (29, 11)0.6315(1, 12)0.6315(1, 12)0.8596(1, 10)0.8771(1, 9)0.9649 (1, 4)0.9649(1, 4)0.9649(1, 4)0.9649(1, 4)0.9824(15, 1)0.9824(1, 1)0.9824(15, 1)Wine0.4888(1, 11)0.8888(7, 8)0.3888(7, 12)0.3333(1, 13)0.8888(1, 8)0.5555(4, 10)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)1(1, 1)Wholesale0.6818(1, 9)0.7954 (5, 8)0.6818(1, 9)0.6818(1, 9)0.6818(1, 9)0.6818(1, 9)1(3, 1)0.9772 (1, 3)1(3, 1)0.9772(1, 3)0.9772(1, 3)0.9772(1, 3)0.9772(1, 3)Average rank7.838.4110.2511.756.257.252.664.662.663.52.253.412.41Comparing the proposed method with the base approaches in terms of the accuracy using the SVM classifier according to  REF _Ref84787106 \h Table ‎45, out of the total of 12 experiments, the proposed method using the LLE, Auto-encoder and the PCA, has gained the first rank in 6, 6, and 5 experiments respectively. Whereas, under the same circumstances, from the base approaches the Fisher, Gini, and PCA have received the first rank in 2, 2, and 2 experiments, respectively.Totally, out of the 12 experiments, the proposed approaches have earned the first rank in 10 experiments. As you can see, the proposed methods have generally a better average ranking in comparison with the base approaches. Having these observations, we can announce LLE as the best approach in finding the neighborhoods on the manifold in terms of accuracy and using SVM as the classifier.Representation of the data after reductionFor better visualization of the achievements of the approach after feature mapping and reduction, the data distribution after using the proposed approach is plotted in a 2D space and compared with the original distribution after feature selection and data distribution after DLSR. The plots are shown in Fig. 5.  Data nameOriginal 2D viewDLSRPCA + DLSRBupaIonosphereIrisMonksNew-thyroidWDBCWholesaleWineFigure  STYLEREF 1 \s ‎4 SEQ Figure \* ARABIC \s 1 1. Data distribution visualization after the reduction to a new 2D space using different approaches.In the above illustrations, PCA is applied as the manifold learning approach, so that the discriminative nature of the approach does not affect the final distribution. As seen in the illustrations, the proposed method discriminates the data from different classes far better than the traditional DLSR approach. Evaluations on the KDD dataIn this chapter we will specifically compare the average results with respect to the confusion matrix of the proposed algorithm and DSLR [25] on KDD dataset which has the highest imbalance ratio among the other datasets studied in this research. The results are shown in Tables 4-6 to 4-13.Note that for the sake of computational facility, as the number of samples in KDD dataset is too great, we have conducted an under sampling before executing the process, i.e., we have only sampled one hundredth of each in the KDD dataset, except the U2R class which is the minority class.In the following tables,  REF _Ref84787322 \h Table ‎46 denotes DLSR approach results while the others denote the results of the proposed framework using different methods in the manifold learning phase. As seen in Tables 4-7 to 4-13, the integer average values for the number of samples correctly classified to each of the classes testifies the predictability and class-wisely equal performance of the proposed methods which signifies the robustness of the approach independent from the fold on which it is tested. Whereas, under the same conditions DLSR method, shown in  REF _Ref84787322 \h Table ‎46, has the non-integer average values for the average number of samples classified to each class in its confusion matrix. This observation on the KDDCup dataset which suffers from high imbalance ratio is the main achievement of the proposed framework on this dataset. However, as can be concluded from these experiments the recall rate of the proposed approach is higher than the DSLR method especially on minority classes (i.e., R2L and U2R).Table  STYLEREF 1 \s ‎4 SEQ Table \* ARABIC \s 1 6. The average confusion matrix of the 10-fold cross validation using DLSR approach on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 28).Class nameDOSNormalProbeR2LU2RAccuracyDOS391.10.600.20.10.997704Normal0.296.500.300.994845Probe0.11.22.7000.675R2L0.10.400.40.10.4U2R0.11.100.43.40.68Table  STYLEREF 1 \s ‎4 SEQ Table \* ARABIC \s 1 7. The average confusion matrix of the 10-fold cross validation using the proposed PCA+DSLR dimension reduction on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 10).Class nameDOSNormalProbeR2LU2RAccuracyDOS39200001Normal1950100.979381Probe004001R2L000101U2R010040.8Table  STYLEREF 1 \s ‎4 SEQ Table \* ARABIC \s 1 8. The average confusion matrix of the 10-fold cross validation using the proposed LDA+DLSR dimension reduction on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 28).Class nameDOSNormalProbeR2LU2RAccuracyDOS39200001Normal1950100.979381Probe004001R2L000101U2R010040.8Table  STYLEREF 1 \s ‎4 SEQ Table \* ARABIC \s 1 9. The average confusion matrix of the 10-fold cross validation using the proposed MDS+DLSR dimension reduction on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 1).Class nameDOSNormalProbeR2LU2RAccuracyDOS39200001Normal1950100.979381Probe004001R2L000101U2R010040.8Table  STYLEREF 1 \s ‎4 SEQ Table \* ARABIC \s 1 10. The average confusion matrix of the 10-fold cross validation using the proposed Isomap+DLSR dimension reduction on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 1).Class nameDOSNormalProbeR2LU2RAccuracyDOS39200001Normal1950100.979381Probe004001R2L000101U2R010040.8 REF _Ref84804163 \h Table ‎411 shows the average confusion matrix of the 10-fold cross validation using the proposed LLE+DLSR dimension reduction on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 1).Table  STYLEREF 1 \s ‎4 SEQ Table \* ARABIC \s 1 11. The average confusion matrix of the 10-fold cross validation using the proposed LLE+DLSR dimension reduction on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 1).Class nameDOSNormalProbeR2LU2RAccuracyDOS39200001Normal1950100.979381Probe004001R2L000101U2R010040.8Table  STYLEREF 1 \s ‎4 SEQ Table \* ARABIC \s 1 12. The average confusion matrix of the 10-fold cross validation using the proposed KPCA+DLSR dimension reduction on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 1).Class nameDOSNormalProbeR2LU2RAccuracyDOS39200001Normal1950100.979381Probe004001R2L000101U2R010040.8Table  STYLEREF 1 \s ‎4 SEQ Table \* ARABIC \s 1 13. The average confusion matrix of the 10-fold cross validation using the proposed Autoencoder+DLSR approach on KDD dataset using 7-NN classifier for the best latent dimensionality (i.e. 1).Class nameDOSNormalProbeR2LU2RAccuracyDOS39200001Normal1950100.979381Probe004001R2L000101U2R010040.8Also,  REF _Ref84787397 \h Table ‎414 shows the best results of the proposed approach which is achieved using SVM classifier and Autoencoder as the manifold learning approach. This experiment is also performed using 10-fold cross validation. The same results as above are observed in the following table while the classification accuracy of the approach in different classes (even the minority classes) is considerably high.Table  STYLEREF 1 \s ‎4 SEQ Table \* ARABIC \s 1 14. The average confusion matrix of the 10-fold cross validation using the proposed Autoencoder+DLSR approach on KDD dataset using the SVM classifier for the best latent dimensionality (i.e. 9).Class nameDOSNormalProbeR2LU2RAccuracyDOS3200001Normal0970001Probe004001R2L000101U2R010040.8To have a better representation of the achievements of the proposed approach on KDD dataset compared with some recent and the state of the art approaches, we also had a class-wise comparison between the best accuracy results of the proposed method and the TVCPSO ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1016/j.neucom.2016.03.031","ISSN":"18728286","abstract":"Many organizations recognize the necessities of utilizing sophisticated tools and systems to protect their computer networks and reduce the risk of compromising their information. Although many machine-learning-based data classification algorithm has been proposed in network intrusion detection problem, each of them has its own strengths and weaknesses. In this paper, we propose an effective intrusion detection framework by using a new adaptive, robust, precise optimization method, namely, time-varying chaos particle swarm optimization (TVCPSO) to simultaneously do parameter setting and feature selection for multiple criteria linear programming (MCLP) and support vector machine (SVM). In the proposed methods, a weighted objective function is provided, which takes into account trade-off between the maximizing the detection rate and minimizing the false alarm rate, along with considering the number of features. Furthermore, to make the particle swarm optimization algorithm faster in searching the optimum and avoid the search being trapped in local optimum, chaotic concept is adopted in PSO and time varying inertia weight and time varying acceleration coefficient is introduced. The performance of proposed methods has been evaluated by conducting experiments with the NSL-KDD dataset, which is derived and modified from well-known KDD cup 99 data sets. The empirical results show that the proposed method performs better in terms of having a high detection rate and a low false alarm rate when compared with the obtained results using all features.","author":[{"dropping-particle":"","family":"Hosseini Bamakan","given":"Seyed Mojtaba","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Wang","given":"Huadong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Yingjie","given":"Tian","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Shi","given":"Yong","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Neurocomputing","id":"ITEM-1","issued":{"date-parts":[["2016"]]},"page":"90-102","publisher":"Elsevier","title":"An effective intrusion detection framework based on MCLP/SVM optimized by time-varying chaos particle swarm optimization","type":"article-journal","volume":"199"},"uris":["http://www.mendeley.com/documents/?uuid=c40637ad-e552-4ba0-8a01-4d6cd4130fcf"]}],"mendeley":{"formattedCitation":"[69]","plainTextFormattedCitation":"[69]","previouslyFormattedCitation":"[69]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[69] and CANN ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1016/j.knosys.2015.01.009","ISSN":"09507051","abstract":"The aim of an intrusion detection systems (IDS) is to detect various types of malicious network traffic and computer usage, which cannot be detected by a conventional firewall. Many IDS have been developed based on machine learning techniques. Specifically, advanced detection approaches created by combining or integrating multiple learning techniques have shown better detection performance than general single learning techniques. The feature representation method is an important pattern classifier that facilitates correct classifications, however, there have been very few related studies focusing how to extract more representative features for normal connections and effective detection of attacks. This paper proposes a novel feature representation approach, namely the cluster center and nearest neighbor (CANN) approach. In this approach, two distances are measured and summed, the first one based on the distance between each data sample and its cluster center, and the second distance is between the data and its nearest neighbor in the same cluster. Then, this new and one-dimensional distance based feature is used to represent each data sample for intrusion detection by a k-Nearest Neighbor (k-NN) classifier. The experimental results based on the KDD-Cup 99 dataset show that the CANN classifier not only performs better than or similar to k-NN and support vector machines trained and tested by the original feature representation in terms of classification accuracy, detection rates, and false alarms. I also provides high computational efficiency for the time of classifier training and testing (i.e., detection).","author":[{"dropping-particle":"","family":"Lin","given":"Wei Chao","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Ke","given":"Shih Wen","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Tsai","given":"Chih Fong","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Knowledge-Based Systems","id":"ITEM-1","issue":"1","issued":{"date-parts":[["2015"]]},"page":"13-21","publisher":"Elsevier B.V.","title":"CANN: An intrusion detection system based on combining cluster centers and nearest neighbors","type":"article-journal","volume":"78"},"uris":["http://www.mendeley.com/documents/?uuid=90e9a7a8-f07b-4412-9722-a7b4914db394"]}],"mendeley":{"formattedCitation":"[70]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[70] approaches on the KDD dataset. The TVCSPO ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1016/j.neucom.2016.03.031","ISSN":"18728286","abstract":"Many organizations recognize the necessities of utilizing sophisticated tools and systems to protect their computer networks and reduce the risk of compromising their information. Although many machine-learning-based data classification algorithm has been proposed in network intrusion detection problem, each of them has its own strengths and weaknesses. In this paper, we propose an effective intrusion detection framework by using a new adaptive, robust, precise optimization method, namely, time-varying chaos particle swarm optimization (TVCPSO) to simultaneously do parameter setting and feature selection for multiple criteria linear programming (MCLP) and support vector machine (SVM). In the proposed methods, a weighted objective function is provided, which takes into account trade-off between the maximizing the detection rate and minimizing the false alarm rate, along with considering the number of features. Furthermore, to make the particle swarm optimization algorithm faster in searching the optimum and avoid the search being trapped in local optimum, chaotic concept is adopted in PSO and time varying inertia weight and time varying acceleration coefficient is introduced. The performance of proposed methods has been evaluated by conducting experiments with the NSL-KDD dataset, which is derived and modified from well-known KDD cup 99 data sets. The empirical results show that the proposed method performs better in terms of having a high detection rate and a low false alarm rate when compared with the obtained results using all features.","author":[{"dropping-particle":"","family":"Hosseini Bamakan","given":"Seyed Mojtaba","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Wang","given":"Huadong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Yingjie","given":"Tian","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Shi","given":"Yong","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Neurocomputing","id":"ITEM-1","issued":{"date-parts":[["2016"]]},"page":"90-102","publisher":"Elsevier","title":"An effective intrusion detection framework based on MCLP/SVM optimized by time-varying chaos particle swarm optimization","type":"article-journal","volume":"199"},"uris":["http://www.mendeley.com/documents/?uuid=c40637ad-e552-4ba0-8a01-4d6cd4130fcf"]}],"mendeley":{"formattedCitation":"[69]","plainTextFormattedCitation":"[69]","previouslyFormattedCitation":"[69]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[69] approach is designed to propose a framework for the intrusion detection using an adaptive, robust with precise optimization novel approach called the Time-varying chaos particle swarm optimization which is used for concurrent parameter setting and feature selection for the multiple criteria linear programming (MCLP) and the SVM classification. In this approach a weighted objective function is used which handles a tradeoff between the detection rate maximization and false alarm rate minimization, by considering the number of features. Furthermore, in this approach, in order to make the particle swarm optimization faster in finding the global optimal point and avoid the local optima, the chaos is concept is adopted in the PSO and the time varying inertia weight and the time varying acceleration coefficient is introduced.CANN ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1016/j.knosys.2015.01.009","ISSN":"09507051","abstract":"The aim of an intrusion detection systems (IDS) is to detect various types of malicious network traffic and computer usage, which cannot be detected by a conventional firewall. Many IDS have been developed based on machine learning techniques. Specifically, advanced detection approaches created by combining or integrating multiple learning techniques have shown better detection performance than general single learning techniques. The feature representation method is an important pattern classifier that facilitates correct classifications, however, there have been very few related studies focusing how to extract more representative features for normal connections and effective detection of attacks. This paper proposes a novel feature representation approach, namely the cluster center and nearest neighbor (CANN) approach. In this approach, two distances are measured and summed, the first one based on the distance between each data sample and its cluster center, and the second distance is between the data and its nearest neighbor in the same cluster. Then, this new and one-dimensional distance based feature is used to represent each data sample for intrusion detection by a k-Nearest Neighbor (k-NN) classifier. The experimental results based on the KDD-Cup 99 dataset show that the CANN classifier not only performs better than or similar to k-NN and support vector machines trained and tested by the original feature representation in terms of classification accuracy, detection rates, and false alarms. I also provides high computational efficiency for the time of classifier training and testing (i.e., detection).","author":[{"dropping-particle":"","family":"Lin","given":"Wei Chao","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Ke","given":"Shih Wen","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Tsai","given":"Chih Fong","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Knowledge-Based Systems","id":"ITEM-1","issue":"1","issued":{"date-parts":[["2015"]]},"page":"13-21","publisher":"Elsevier B.V.","title":"CANN: An intrusion detection system based on combining cluster centers and nearest neighbors","type":"article-journal","volume":"78"},"uris":["http://www.mendeley.com/documents/?uuid=90e9a7a8-f07b-4412-9722-a7b4914db394"]}],"mendeley":{"formattedCitation":"[70]","plainTextFormattedCitation":"[70]","previouslyFormattedCitation":"[70]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[70], proposes an approach called the cluster center and nearest neighbor. In this approach, two distances are measured and aggregated, the first one is based on the distance between each data sample and its cluster center, and the second one is based on the distance between each data point and its nearest neighbor form the same cluster. This new and one-dimensional representation of data points is used for the intrusion detection by a KNN classifier. REF _Ref84787436 \h Table ‎415 shows the results of the mentioned approaches in comparison with the proposed approach. As seen in  REF _Ref84787436 \h Table ‎415, the proposed method as is specified in Table 14, performs considerably better in terms of accuracy in comparison with the recently proposed methods ADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1016/j.knosys.2015.01.009","ISSN":"09507051","abstract":"The aim of an intrusion detection systems (IDS) is to detect various types of malicious network traffic and computer usage, which cannot be detected by a conventional firewall. Many IDS have been developed based on machine learning techniques. Specifically, advanced detection approaches created by combining or integrating multiple learning techniques have shown better detection performance than general single learning techniques. The feature representation method is an important pattern classifier that facilitates correct classifications, however, there have been very few related studies focusing how to extract more representative features for normal connections and effective detection of attacks. This paper proposes a novel feature representation approach, namely the cluster center and nearest neighbor (CANN) approach. In this approach, two distances are measured and summed, the first one based on the distance between each data sample and its cluster center, and the second distance is between the data and its nearest neighbor in the same cluster. Then, this new and one-dimensional distance based feature is used to represent each data sample for intrusion detection by a k-Nearest Neighbor (k-NN) classifier. The experimental results based on the KDD-Cup 99 dataset show that the CANN classifier not only performs better than or similar to k-NN and support vector machines trained and tested by the original feature representation in terms of classification accuracy, detection rates, and false alarms. I also provides high computational efficiency for the time of classifier training and testing (i.e., detection).","author":[{"dropping-particle":"","family":"Lin","given":"Wei Chao","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Ke","given":"Shih Wen","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Tsai","given":"Chih Fong","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Knowledge-Based Systems","id":"ITEM-1","issue":"1","issued":{"date-parts":[["2015"]]},"page":"13-21","publisher":"Elsevier B.V.","title":"CANN: An intrusion detection system based on combining cluster centers and nearest neighbors","type":"article-journal","volume":"78"},"uris":["http://www.mendeley.com/documents/?uuid=90e9a7a8-f07b-4412-9722-a7b4914db394"]},{"id":"ITEM-2","itemData":{"DOI":"10.1016/j.neucom.2016.03.031","ISSN":"18728286","abstract":"Many organizations recognize the necessities of utilizing sophisticated tools and systems to protect their computer networks and reduce the risk of compromising their information. Although many machine-learning-based data classification algorithm has been proposed in network intrusion detection problem, each of them has its own strengths and weaknesses. In this paper, we propose an effective intrusion detection framework by using a new adaptive, robust, precise optimization method, namely, time-varying chaos particle swarm optimization (TVCPSO) to simultaneously do parameter setting and feature selection for multiple criteria linear programming (MCLP) and support vector machine (SVM). In the proposed methods, a weighted objective function is provided, which takes into account trade-off between the maximizing the detection rate and minimizing the false alarm rate, along with considering the number of features. Furthermore, to make the particle swarm optimization algorithm faster in searching the optimum and avoid the search being trapped in local optimum, chaotic concept is adopted in PSO and time varying inertia weight and time varying acceleration coefficient is introduced. The performance of proposed methods has been evaluated by conducting experiments with the NSL-KDD dataset, which is derived and modified from well-known KDD cup 99 data sets. The empirical results show that the proposed method performs better in terms of having a high detection rate and a low false alarm rate when compared with the obtained results using all features.","author":[{"dropping-particle":"","family":"Hosseini Bamakan","given":"Seyed Mojtaba","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Wang","given":"Huadong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Yingjie","given":"Tian","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Shi","given":"Yong","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Neurocomputing","id":"ITEM-2","issued":{"date-parts":[["2016"]]},"page":"90-102","publisher":"Elsevier","title":"An effective intrusion detection framework based on MCLP/SVM optimized by time-varying chaos particle swarm optimization","type":"article-journal","volume":"199"},"uris":["http://www.mendeley.com/documents/?uuid=c40637ad-e552-4ba0-8a01-4d6cd4130fcf"]}],"mendeley":{"formattedCitation":"[69], [70]","plainTextFormattedCitation":"[69], [70]","previouslyFormattedCitation":"[69], [70]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[69], [70] on the KDD dataset. Other than an improvement on majority classes such as DOS and Normal, the proposed approach is highly efficient in identifying the minority classes such as R2L and U2R which is the main drawback of previous approaches on these datasets.Table  STYLEREF 1 \s ‎4 SEQ Table \* ARABIC \s 1 15. A comparison between the accuracy of the proposed method and some other recent works on different classes of the KDD based on the 10-fold cross validation.Class nameProposed Method using AE and SVMCANNADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1016/j.knosys.2015.01.009","ISSN":"09507051","abstract":"The aim of an intrusion detection systems (IDS) is to detect various types of malicious network traffic and computer usage, which cannot be detected by a conventional firewall. Many IDS have been developed based on machine learning techniques. Specifically, advanced detection approaches created by combining or integrating multiple learning techniques have shown better detection performance than general single learning techniques. The feature representation method is an important pattern classifier that facilitates correct classifications, however, there have been very few related studies focusing how to extract more representative features for normal connections and effective detection of attacks. This paper proposes a novel feature representation approach, namely the cluster center and nearest neighbor (CANN) approach. In this approach, two distances are measured and summed, the first one based on the distance between each data sample and its cluster center, and the second distance is between the data and its nearest neighbor in the same cluster. Then, this new and one-dimensional distance based feature is used to represent each data sample for intrusion detection by a k-Nearest Neighbor (k-NN) classifier. The experimental results based on the KDD-Cup 99 dataset show that the CANN classifier not only performs better than or similar to k-NN and support vector machines trained and tested by the original feature representation in terms of classification accuracy, detection rates, and false alarms. I also provides high computational efficiency for the time of classifier training and testing (i.e., detection).","author":[{"dropping-particle":"","family":"Lin","given":"Wei Chao","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Ke","given":"Shih Wen","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Tsai","given":"Chih Fong","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Knowledge-Based Systems","id":"ITEM-1","issue":"1","issued":{"date-parts":[["2015"]]},"page":"13-21","publisher":"Elsevier B.V.","title":"CANN: An intrusion detection system based on combining cluster centers and nearest neighbors","type":"article-journal","volume":"78"},"uris":["http://www.mendeley.com/documents/?uuid=90e9a7a8-f07b-4412-9722-a7b4914db394"]}],"mendeley":{"formattedCitation":"[70]","plainTextFormattedCitation":"[70]","previouslyFormattedCitation":"[70]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[70]TVCPSO-MCLPADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1016/j.neucom.2016.03.031","ISSN":"18728286","abstract":"Many organizations recognize the necessities of utilizing sophisticated tools and systems to protect their computer networks and reduce the risk of compromising their information. Although many machine-learning-based data classification algorithm has been proposed in network intrusion detection problem, each of them has its own strengths and weaknesses. In this paper, we propose an effective intrusion detection framework by using a new adaptive, robust, precise optimization method, namely, time-varying chaos particle swarm optimization (TVCPSO) to simultaneously do parameter setting and feature selection for multiple criteria linear programming (MCLP) and support vector machine (SVM). In the proposed methods, a weighted objective function is provided, which takes into account trade-off between the maximizing the detection rate and minimizing the false alarm rate, along with considering the number of features. Furthermore, to make the particle swarm optimization algorithm faster in searching the optimum and avoid the search being trapped in local optimum, chaotic concept is adopted in PSO and time varying inertia weight and time varying acceleration coefficient is introduced. The performance of proposed methods has been evaluated by conducting experiments with the NSL-KDD dataset, which is derived and modified from well-known KDD cup 99 data sets. The empirical results show that the proposed method performs better in terms of having a high detection rate and a low false alarm rate when compared with the obtained results using all features.","author":[{"dropping-particle":"","family":"Hosseini Bamakan","given":"Seyed Mojtaba","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Wang","given":"Huadong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Yingjie","given":"Tian","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Shi","given":"Yong","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Neurocomputing","id":"ITEM-1","issued":{"date-parts":[["2016"]]},"page":"90-102","publisher":"Elsevier","title":"An effective intrusion detection framework based on MCLP/SVM optimized by time-varying chaos particle swarm optimization","type":"article-journal","volume":"199"},"uris":["http://www.mendeley.com/documents/?uuid=c40637ad-e552-4ba0-8a01-4d6cd4130fcf"]}],"mendeley":{"formattedCitation":"[69]","plainTextFormattedCitation":"[69]","previouslyFormattedCitation":"[69]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[69]TVCPSO-SVMADDIN CSL_CITATION {"citationItems":[{"id":"ITEM-1","itemData":{"DOI":"10.1016/j.neucom.2016.03.031","ISSN":"18728286","abstract":"Many organizations recognize the necessities of utilizing sophisticated tools and systems to protect their computer networks and reduce the risk of compromising their information. Although many machine-learning-based data classification algorithm has been proposed in network intrusion detection problem, each of them has its own strengths and weaknesses. In this paper, we propose an effective intrusion detection framework by using a new adaptive, robust, precise optimization method, namely, time-varying chaos particle swarm optimization (TVCPSO) to simultaneously do parameter setting and feature selection for multiple criteria linear programming (MCLP) and support vector machine (SVM). In the proposed methods, a weighted objective function is provided, which takes into account trade-off between the maximizing the detection rate and minimizing the false alarm rate, along with considering the number of features. Furthermore, to make the particle swarm optimization algorithm faster in searching the optimum and avoid the search being trapped in local optimum, chaotic concept is adopted in PSO and time varying inertia weight and time varying acceleration coefficient is introduced. The performance of proposed methods has been evaluated by conducting experiments with the NSL-KDD dataset, which is derived and modified from well-known KDD cup 99 data sets. The empirical results show that the proposed method performs better in terms of having a high detection rate and a low false alarm rate when compared with the obtained results using all features.","author":[{"dropping-particle":"","family":"Hosseini Bamakan","given":"Seyed Mojtaba","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Wang","given":"Huadong","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Yingjie","given":"Tian","non-dropping-particle":"","parse-names":false,"suffix":""},{"dropping-particle":"","family":"Shi","given":"Yong","non-dropping-particle":"","parse-names":false,"suffix":""}],"container-title":"Neurocomputing","id":"ITEM-1","issued":{"date-parts":[["2016"]]},"page":"90-102","publisher":"Elsevier","title":"An effective intrusion detection framework based on MCLP/SVM optimized by time-varying chaos particle swarm optimization","type":"article-journal","volume":"199"},"uris":["http://www.mendeley.com/documents/?uuid=c40637ad-e552-4ba0-8a01-4d6cd4130fcf"]}],"mendeley":{"formattedCitation":"[69]","plainTextFormattedCitation":"[69]","previouslyFormattedCitation":"[69]"},"properties":{"noteIndex":0},"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}[69]DOS10.99680.98640.9884Normal10.97040.97590.9913Probe10.87610.87900.8929R2L10.57020.75080.6784U2R0.80.3850.59620.4038Average0.960.75970.837660.79096Conclusion and Future WorkIntroductionIn this research a novel method for distance metric learning with the aim of preserving the local neighborhoods between similar data points and also covering data imbalance problem has been proposed and the implementation steps and its experimental results in comparison with other distance metric learning and dimensionality reduction algorithms has been evaluated. In the proposed method, it has been tried to first learn the neighborhoods between the data points based on their neighborhood relations on the manifold. For each data point, two neighborhoods with same number of members consisting of the similar and dissimilar data points to the given point are created. Consequently, distance metric learning is performed with the goal of making the similar points nearer to the given data point and to push back the dissimilar data away from it. Finally, thanks to learned transformation matrix, data are mapped to the similarity space and then the classification is preformed using k-NN and SVM classifiers. The evaluations are performed on 12 datasets with different sizes and imbalance ratio specially the KDD, which resulted in significant results based on the three criteria of accuracy, sensitivity and specificity.Future workIn future we would like to have a study on different approaches of data sampling specially the graph-based prototype selection approaches which preserve the local structures of the data. Besides, as for the graph prototype selection, we need to calculate the appropriate distance between different graphs in order to select the most expressive ones. Therefore, another area that we could invest on in the future is to study on the effect of using different graph editing distances on the graph-based prototype selection.An analysis on the selection of the most appropriate manifold learning approaches (as different manifold learning approaches result in differently manifolds) could have an extensive impact on the improvement of the learned distance metric. To do this, we would like to have a study on the effect of using the deep neural networks e.g., convolutional neural networks and generative adversarial networks as the learning approaches and analyze their results in comparison with the existing manifold learning approaches. ReferencesADDIN Mendeley Bibliography CSL_BIBLIOGRAPHY [1]A. Bellet, A. Habrard, and M. Sebban, “A Survey on Metric Learning for Feature Vectors and Structured Data,” arXiv Prepr. arXiv1306.6709, p. 57, 2013, doi: 10.1073/pnas.0809777106.[2]C. Domeniconi, J. Peng, and D. Gunopulos, “Locally adaptive metric nearest-neighbor classification,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 24, no. 9, pp. 1281–1285, 2002, doi: 10.1109/TPAMI.2002.1033219.[3]L. Yang, R. Jin, R. Sukthankar, and Y. Liu, “An efficient algorithm for local distance metric learning,” in Proceedings of the National Conference on Artificial Intelligence, 2006, vol. 1, pp. 543–548.[4]C. Domeniconi and D. Gunopulos, “Adaptive nearest neighbor classification using support vector machines,” 2002.[5]Z. Zhang, J. Kwok, and D. Yeung, “Parametric distance metric learning with label information,” Proc. IJCAI.[6]P. Schneider, K. Bunte, H. Stiekema, B. Hammer, T. Villmann, and M. Biehl, “Regularization in matrix relevance learning,” IEEE Trans. Neural Networks, vol. 21, no. 5, pp. 831–840, 2010, doi: 10.1109/TNN.2010.2042729.[7]D. Tao, X. Li, X. Wu, and S. J. Maybank, “General tensor discriminant analysis and Gabor features for gait recognition,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 10, pp. 1700–1715, 2007, doi: 10.1109/TPAMI.2007.1096.[8]K. Q. Weinberger and L. K. Saul, “Fast solvers and efficient implementations for distance metric learning,” in Proceedings of the 25th International Conference on Machine Learning, 2008, pp. 1160–1167.[9]Y. Mu, W. Ding, D. Tao, and T. F. Stepinski, “Biologically inspired model for crater detection,” in Proceedings of the International Joint Conference on Neural Networks, 2011, pp. 2487–2494, doi: 10.1109/IJCNN.2011.6033542.[10]T. Zhang, D. Tao, X. Li, and J. Yang, “Patch alignment for dimensionality reduction,” IEEE Trans. Knowl. Data Eng., vol. 21, no. 9, pp. 1299–1313, 2009, doi: 10.1109/TKDE.2008.212.[11]E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell, “Distance metric learning, with application to clustering with side-information,” 2002.[12]J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov, “Neighbourhood components analysis,” 2005.[13]M. Sugiyama, “Dimensionality reduction of multimodal labeled data by local fisher discriminant analysis,” J. Mach. Learn. Res., vol. 8, pp. 1027–1061, 2007.[14]K. Q. Weinberger and L. K. Saul, “Distance metric learning for large margin nearest neighbor classification,” J. Mach. Learn. Res., vol. 10, pp. 207–244, 2009.[15]T. Hastie and R. Tibshirani, “Discriminant adaptive nearest neighbor classification,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 18, no. 6, pp. 607–616, 1996, doi: 10.1109/34.506411.[16]T. M. Cover and P. E. Hart, “Nearest Neighbor Pattern Classification,” IEEE Trans. Inf. Theory, vol. 13, no. 1, pp. 21–27, 1967, doi: 10.1109/TIT.1967.1053964.[17]J. Winn, A. Criminisi, and T. Minka, “Object categorization by learned universal visual dictionary,” in Proceedings of the IEEE International Conference on Computer Vision, 2005, vol. II, pp. 1800–1807, doi: 10.1109/ICCV.2005.171.[18]A. W. M. Smeulders, M. Worring, S. Santini, A. Gupta, and R. Jain, “Content-based image retrieval at the end of the early years,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, no. 12, pp. 1349–1380, 2000, doi: 10.1109/34.895972.[19]Z. Lai, W. K. Wong, Y. Xu, J. Yang, and D. Zhang, “Approximate Orthogonal Sparse Embedding for Dimensionality Reduction,” IEEE Trans. Neural Networks Learn. Syst., vol. 27, no. 4, pp. 723–735, Apr. 2016, doi: 10.1109/TNNLS.2015.2422994.[20]M. Turk and A. Pentland, “Eigenfaces for Recognition,” J. Cogn. Neurosci., vol. 3, no. 1, pp. 71–86, Jan. 1991, doi: 10.1162/jocn.1991.3.1.71.[21]M. Kirby and L. Sirovich, “Application of the Karhunen-Loeve procedure for the characterization of human faces,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 12, no. 1, pp. 103–108, 1990, doi: 10.1109/34.41390.[22]L. Sirovich and M. Kirby, “Low-dimensional procedure for the characterization of human faces.,” J. Opt. Soc. Am. A., vol. 4, no. 3, pp. 519–524, 1987, doi: 10.1364/JOSAA.4.000519.[23]Q. Liu, H. Lu, and S. Ma, “Improving Kernel Fisher Discriminant Analysis for Face Recognition,” IEEE Trans. Circuits Syst., vol. 14, no. 1, pp. 42–49, 2004, doi: 10.1109/TCSVT.2003.818352.[24]C. Fraley and  a E. Raftery, “Model-based clustering, discriminant analysis, and density estimation,” J. Am. Stat. Assoc., vol. 97, no. 458, pp. 611–631, 2002, doi: 10.1198/016214502760047131.[25]P. Belhumeur, J. Hespanha, and D. Kriegman, “Eigenfaces Vs. Fisherfaces: Recognition Using Class Specific Linear Projection,” vol. 19, no. 7, pp. 711–720, 1997, doi: 10.1007/BFb0015522.[26]Jun Li and Dacheng Tao, “Simple Exponential Family PCA,” IEEE Trans. Neural Networks Learn. Syst., vol. 24, no. 3, pp. 485–497, Mar. 2013, doi: 10.1109/TNNLS.2012.2234134.[27]D. Tao, X. Li, S. Member, X. Wu, and S. Member, “Geometric Mean for Subspace Selection,” TIANJIN Univ. DOWNLOADED DECEMBER 8, 2009 0433 FROM IEEE XPLORE. Restrict. APPLY. YUAN AL. Bin. SPARSE NONNEGATIVE MATRIX FACTORIZATION 777, vol. 31, no. 2, pp. 260–274, 2009, doi: 10.1109/TPAMI.2008.70.[28]J. Li and D. Tao, “On preserving original variables in Bayesian PCA with application to image analysis,” IEEE Trans. Image Process., vol. 21, no. 12, pp. 4830–4843, 2012, doi: 10.1109/TIP.2012.2211372.[29]S. T. Roweis, “Nonlinear Dimensionality Reduction by Locally Linear Embedding,” Science (80-. )., vol. 290, no. 5500, pp. 2323–2326, Dec. 2000, doi: 10.1126/science.290.5500.2323.[30]J. B. Tenenbaum, “A Global Geometric Framework for Nonlinear Dimensionality Reduction,” Science (80-. )., vol. 290, no. 5500, pp. 2319–2323, Dec. 2000, doi: 10.1126/science.290.5500.2319.[31]M. Belkin and P. Niyogi, “Laplacian Eigenmaps for Dimensionality Reduction and Data Representation,” Neural Comput., vol. 15, no. 6, pp. 1373–1396, 2003, doi: 10.1162/089976603321780317.[32]Xiaofei He, Deng Cai, Shuicheng Yan, and Hong-Jiang Zhang, “Neighborhood preserving embedding,” Tenth IEEE Int. Conf. Comput. Vis. Vol. 1, vol. 2, pp. 1208-1213 Vol. 2, 2005, doi: 10.1109/ICCV.2005.167.[33]E. Kokiopoulou and Y. Saad, “Orthogonal neighborhood preserving projections: A projection-based dimensionality reduction technique,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 12, pp. 2143–2156, 2007, doi: 10.1109/TPAMI.2007.1131.[34]X. He and P. Niyogi, “Locality preserving projections,” Neural Inf. Process. Syst., vol. 16, p. 153, 2004, doi: 10.1.1.19.9400.[35]“Yin Zhou’s Home Page @ University of Delaware.” https://www.eecis.udel.edu/~zhou/Research.html (accessed Oct. 08, 2017).[36]Keinosuke Fukunaga, Introduction to Statistical Pattern Recognition. Elsevier, 1990.[37]R. Short and K. Fukunaga, “The optimal distance measure for nearest neighbor classification,” IEEE Trans. Inf. Theory, vol. 27, no. 5, pp. 622–627, 1981.[38]P. C. Mahalanobis, “On the generalized distance in statistics,” Proc. Natl. Inst. Sci., vol. 2, pp. 49–55, 1936.[39]X. He and P. Niyogi, “Locality preserving projections,” 2004.[40]I. T. Jolliffe, “Principal component analysis and factor analysis,” Princ. Compon. Anal., pp. 150–166, 2002.[41]F. Wang, B. Zhao, and C. Zhang, “Unsupervised large margin discriminative projection,” IEEE Trans. Neural Networks, vol. 22, no. 9, pp. 1446–1456, 2011, doi: 10.1109/TNN.2011.2161772.[42]M. Belkin and P. Niyogi, “Laplacian eigenmaps and spectral techniques for embedding and clustering,” Adv. Neural Inf. Process. Syst., vol. 14, pp. 585–591, 2002.[43]J. B. Tenenbaum, V. De Silva, and J. C. Langford, “A global geometric framework for nonlinear dimensionality reduction,” Science (80-. )., vol. 290, no. 5500, 2000, doi: 10.1126/science.290.5500.2319.[44]G. E. Hinton and S. T. Roweis, “Stochastic neighbor embedding. In: Advances in neural information processing systems (NIPS),” pp 833–840, 2002.[45]B. Schölkopf and A. J. Smola, Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2002.[46]J. Handl, E. Hart, P. R. L. M. López-ibáñez, G. Ochoa, B. Paechter, and D. Hutchison, Parallel Problem Solving from Nature – PPSN XIV. 2016.[47]L. Meng, S. Ding, and Y. Xue, “Research on denoising sparse autoencoder,” Int. J. Mach. Learn. Cybern., vol. 8, no. 5, pp. 1719–1729, 2017, doi: 10.1007/s13042-016-0550-y.[48]M. A. Cox, Trevor F; Cox, Multidimensional scaling. CRC press, 2000.[49]J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov, “Neighbourhood components analysis,” Neighb. Components Anal., pp. 513–520, 2004.[50]F. Wang and C. Zhang, “Feature extraction by maximizing the average neighborhood margin,” 2007, doi: 10.1109/CVPR.2007.383124.[51]K. Q. Weinberger and J. Blitzer, “Distance metric learning for large margin nearest neighbor classification,” Adv. Neural Inf. Process. Syst., 2005.[52]K. Fukunaga, “Statistical Pattern Stas-tical Pattern Recognition,” Pattern Recognit., vol. 22, no. 7, pp. 833–834, 1990, doi: 10.1016/0098-3004(96)00017-9.[53]J. V Davis, B. Kulis, P. Jain, S. Suvrit, and I. S. Dhillon, “Information-theoretic metric learning. In: International conference on machine learning (ICML),” pp 209–216, 2007.[54]A. Kocsor, K. Kovács, and C. Szepesvári, “Margin maximizing discriminant analysis,” in Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science), 2004, vol. 3201, pp. 227–238.[55]N. Shental, T. Hertz, D. Weinshall, and M. Pavel, “Adjustment learning and relevant component analysis. In: Proceedings of European conference on computer vision,” pp 776–790, 2002.[56]S. Mika, G. Ratsch, J. Weston, B. Schölkopf, and K. R. Müllers, “Fisher discriminant analysis with kernels. In: Neural networks for signal processing IX, 1999. proceedings of the 1999 IEEE signal processing society workshop,” pp 41–48, 1999.[57]I. W. Tsang, P. M. Cheung, and J. T. Kwok, “Kernel relevant component analysis for distance metric learning. In: In IEEE International joint conference on neural networks (IJCNN),” pp 954–959, 2005.[58]Y.-F. Guo, S.-J. Li, J.-Y. Yang, T.-T. Shu, and L.-D. Wu, “A generalized Foley-Sammon transform based on generalized fisher discriminant criterion and its application to face recognition,” Pattern Recognit. Lett., vol. 24, no. 1–3, pp. 147–158, 2003, doi: 10.1016/S0167-8655(02)00207-6.[59]Y. Jia, F. Nie, and C. Zhang, “Trace ratio problem revisited,” IEEE Trans. Neural Networks, vol. 20, no. 4, pp. 729–735, 2009, doi: 10.1109/TNN.2009.2015760.[60]S. Xiang, F. Nie, G. Meng, C. Pan, and C. Zhang, “Discriminative least squares regression for multiclass classification and feature selection,” IEEE Trans. Neural Networks Learn. Syst., vol. 23, no. 11, pp. 1738–1754, 2012, doi: 10.1109/TNNLS.2012.2212721.[61]S. C. H. Hoi, W. Liu, and S.-F. Chang, “Semi-supervised distance metric learning for collaborative image retrieval,” 2008, doi: 10.1109/CVPR.2008.4587351.[62]F. Wang, S. Chen, C. Zhang, and T. Li, “Semi-supervised metric learning by maximizing constraint margin. In: Proceedings of the 17th ACM conference on information and knowledge management,” pp 1457–1458, 2008.[63]X. Yang, H. Fu, H. Zha, and J. Barlow, “Semi-supervised nonlinear dimensionality reduction. In: 23rd International conference on machine learning,” pp 1065–1072, 2006.[64]M. Bilenko and S. Basu, “Mooney RJ (2004) Integrating constraints and metric learning in semi-supervised clustering,” Proc. twenty-first Int. Conf. Mach. Learn., pp. 11–18, 2004.[65]Y. Bengio, J.-F. Paiement, P. Vincent, O. Delalleau, N. Le Roux, and M. Ouimet, “Out-of-sample extensions for LLE, Isomap, MDS, Eigenmaps, and spectral clustering,” 2004.[66]F. Wang and J. Sun, Survey on distance metric learning and dimensionality reduction in data mining, vol. 29, no. 2. 2014.[67]I. Davidson, K. L. Wagstaff, and S. Basu, “Measuring constraint-set utility for partitional clustering algorithms,” Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), vol. 4213 LNAI, pp. 115–126, 2006.[68]A. Singh, R. D. Nowak, and X. Zhu, “Unlabeled data: now it helps, now it doesn’t. In: Advances in neural information processing systems,” pp 1513–1520, 2008.[69]S. M. Hosseini Bamakan, H. Wang, T. Yingjie, and Y. Shi, “An effective intrusion detection framework based on MCLP/SVM optimized by time-varying chaos particle swarm optimization,” Neurocomputing, vol. 199, pp. 90–102, 2016, doi: 10.1016/j.neucom.2016.03.031.[70]W. C. Lin, S. W. Ke, and C. F. Tsai, “CANN: An intrusion detection system based on combining cluster centers and nearest neighbors,” Knowledge-Based Syst., vol. 78, no. 1, pp. 13–21, 2015, doi: 10.1016/j.knosys.2015.01.009.AppendixResults of the SVM (the remainder)SensitivityTable  STYLEREF 1 \s ‎0 SEQ Table \* ARABIC \s 1 1. Comparison of the results for the sensitivity criterion using the SVM classifier.DatasetDimensionality ReductionFeature SelectionProposed MethodPCALLEKernel PCAAuto-endoderFisherGiniPCALDAMDSIsomapLLEKernel PCAAuto-encoderVehicle0.5(d=1)0.8(d=5)0(d=1)0.65(d=1)0.65(d=1)0.3(d=1)0.9(d=5)0.95(d=13)0.9(d=5)0.9(d=5)0.95(d=5)0.9(d=1)0.95(d=9)Bupa0.066667(d=3)0.066667(d=1)0(d=1)0(d=1)0.166667(d=1)0.733333(d=1)0.466667(d=1)0.4(d=1)0.466667(d=1)0.466667(d=1)0.4(d=1)0.333333(d=5)0.4(d=3)Glass0.857143(d=1)1(d=1)0.857143(d=9)1(d=9)0.857143(d=7)0.857143(d=7)1(d=1)1(d=1)1(d=1)1(d=1)1(d=3)1(d=1)1(d=1)Ionosphere0.913043(d=15)1(d=1)0.869565(d=15)1(d=9)1(d=1)1(d=1)0.956522(d=1)1(d=1)0.956522(d=1)0.913043(d=1)1(d=15)0.956522(d=1)0.956522(d=1)Iris1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)0.8(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)KDD1(d=1)1(d=1)1(d=1)1(d=1)1(d=10)1(d=19)1(d=1)0.997449(d=1)1(d=1)0.997449(d=1)1(d=1)0.997449(d=1)0.997449(d=1)Monks0.666667(d=1)1(d=1)0.833333(d=3)0.666667(d=1)0.833333(d=5)0.833333(d=5)1(d=1)0.666667(d=3)1(d=5)1(d=5)1(d=3)1(d=5)1(d=3)New-thyroid1(d=3)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)Pima0.296296(d=1)0.037037(d=7)0(d=7)0(d=7)0.296296(d=1)0.296296(d=1)0.407407(d=3)0.444444(d=1)0.407407(d=3)0.407407(d=3)0.407407(d=3)0.407407(d=3)0.407407(d=3)WDBC1(d=1)1(d=1)1(d=1)1(d=1)1(d=8)1(d=8)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)Wine0.333333(d=1)1(d=4)0.388889(d=7)1(d=7)0.666667(d=1)0.333333(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)Wholesale1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=3)0.966667(d=1)1(d=3)1(d=3)1(d=3)0.966667(d=1)0.966667(d=1)Table  STYLEREF 1 \s ‎0 SEQ Table \* ARABIC \s 1 2. Ranking of the results for the sensitivity criterion based on the average of the 10-fold cross-validation.DatasetDimensionality ReductionFeature SelectionProposed Method k-NNPCALLEKernel PCAAuto-endoderFisherGiniPCALDAMDSIsomapLLEKernel PCAAuto-encoderVehicle1181399124144141Bupa10101212912522585Glass10110110101111111Ionosphere1111311171711177Iris11111111311111KDD11111111011011010Monks1118118811111111New-thyroid1111111111111Pima8111212882122222WDBC1111111111111Wine12111110121111111Wholesale1111111111111111Average Rank6.53.16666666774.33333333354.751.9166666674.751.91666666731.41666666743.5SpecificityTable  STYLEREF 1 \s ‎0 SEQ Table \* ARABIC \s 1 3. Comparison of the results for the specificity criterion using the SVM classifier.DatasetDimensionality ReductionFeature SelectionProposed MethodPCALLEKernel PCAAuto-endoderFisherGiniPCALDAMDSIsomapLLEKernel PCAAuto-encoderVehicle1(d=5)0.784615(d=1)0(d=1)0.65(d=1)1(d=9)1(d=5)0.984615(d=1)1(d=1)0.984615(d=1)0.984615(d=1)0.984615(d=1)0.984615(d=1)0.984615(d=1)Bupa1(d=3)0.95(d=1)1(d=1)1(d=1)1(d=5)1(d=3)0.9(d=1)0.9(d=5)0.9(d=1)0.9(d=1)0.9(d=1)0.9(d=1)0.9(d=1)Glass0.666667(d=7)0.933333(d=1)0.666667(d=1)1(d=3)0.733333(d=5)0.733333(d=5)0.666667(d=5)0.666667(d=5)0.666667(d=5)0.666667(d=1)0.666667(d=7)0.666667(d=3)0.666667(d=1)Ionosphere1(d=15)0.153846(d=8)1(d=1)0(d=1)1(d=15)1(d=22)1(d=8)0.923077(d=1)1(d=8)0.923077(d=8)0.923077(d=1)0.923077(d=1)1(d=8)Iris1(d=1)1(d=2)1(d=1)1(d=2)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)KDD0.96729(d=1)0.943925(d=19)0.009346(d=10)0.925234(d=19)1(d=19)1(d=10)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)Monks1(d=1)0(d=1)0.833333(d=1)0.833333(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)0.833333(d=1)1(d=1)1(d=3)New-thyroid0.571429(d=1)0.571429(d=5)0(d=1)0.428571(d=5)0.857143(d=3)0.857143(d=3)0.857143(d=5)0.714286(d=5)0.857143(d=5)0.857143(d=5)0.857143(d=1)0.857143(d=1)0.857143(d=3)Pima1(d=1)1(d=1)1(d=1)1(d=1)1(d=3)1(d=3)0.94(d=1)0.96(d=7)0.94(d=1)0.94(d=1)0.94(d=1)0.94(d=1)0.94(d=1)WDBC0.047619(d=1)0.714286(d=29)0(d=1)0(d=1)0.714286(d=1)0.761905(d=1)0.904762(d=1)0.904762(d=1)0.904762(d=1)0.904762(d=1)1(d=15)0.952381(d=1)0.952381(d=1)Wine1(d=4)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)Wholesale0(d=1)0.357143(d=5)0(d=1)0(d=1)0 (d=1)0 (d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)Table  STYLEREF 1 \s ‎0 SEQ Table \* ARABIC \s 1 4. Ranking of the results for the specificity criterion based on the average of the 10-fold cross-validation.DatasetDimensionality ReductionFeature SelectionProposed Method k-NNPCALLEKernel PCAAuto-endoderFisherGiniPCALDAMDSIsomapLLEKernel PCAAuto-encoderVehicle1111312115155555Bupa1611117777777Glass5251335555555Ionosphere112113111818881Iris1111111111111KDD10111312111111111Monks11310101111111011New-thyroid10101312111911111Pima1111118788888WDBC1191212984444122Wine1111111111111Wholesale9899991111111Average Rank4.3333337.0833336.6666677.0833332.52.41666733.83333333.5833334.0833333.4166672.833333results of the k-NN+SAccuracyFigure  STYLEREF 1 \s ‎0 SEQ Figure \* ARABIC \s 1 1. Comparison of the averagre accuracy of the proposed method on 10-fold settings based on the neighborhoods on the manifold learnt by autoenocders and DLSR on the vehicle dataset using the k-NN+S classifier.Table  STYLEREF 1 \s ‎0 SEQ Table \* ARABIC \s 1 5. Comparison of the results for the accuracy criterion using the 7-NN classifier.DatasetDimensionality ReductionProposed Method k-NN+SPCALLEKernel PCAAuto-endoderPCALDAMDSIsomapLLEKernel PCAAuto-encoderVehicle0.682353(d=13)0.611765 (d=17)0.258824(d=9)0.529412(d=5)0.8(d=1)0.776471(d=1)0.8(d=1)0.811765(d=5)0.811765(d=9)0.764706(d=13)0.788235(d=17)Bupa0.571429(d=1)0.657143 (d=3)0.428571(d=5)0.571429(d=3)0.685714(d=1)0.742857(d=3)0.685714(d=1)0.685714(d=1)0.685714(d=1)0.714286(d=1)0.714286(d=5)Glass0.545455(d=1)0.545455(d=5)0.5(d=7)0.454545(d=1)0.681818(d=3)0.636364(d=1)0.681818(d=3)0.681818(d=3)0.681818(d=7)0.727273(d=1)0.727273(d=1)Ionosphere0.888889(d=8)0.805556(d=22)0.916667(d=8)0.888889(d=15)0.861111(d=8)0.916667(d=1)0.888889(d=22)0.861111(d=1)0.916667(d=8)0.916667(d=1)0.888889(d=1)Iris1 (d=1)1(d=2)1(d=2)1 (d=1)0.8(d=2)0.666667(d=1)0.8(d=2)0.8(d=3)0.866667(d=3)0.8(d=1)0.733333(d=2)KDD0.987976(d=1)0.983968(d=28)0.791583(d=10)0.981964(d=37)0.785571(d=1)0.785571(d=1)0.785571(d=1)0.785571(d=1)0.785571(d=1)0.785571(d=1)0.785571(d=1)Monks0.833333(d=5)0.916667(d=5)0.833333(d=3)0.5(d=1)0.916667(d=5)0.75(d=75)0.916667(d=5)0.666667(d=1)0.833333(d=3)0.916667(d=3)0.916667(d=3)New-thyroid0.954545(d=5)0.909091(d=3)0.590909(d=1)0.909091(d=3)0.909091(d=1)0.909091(d=1)0.909091(d=1)0.909091(d=1)0.909091(d=1)0.909091(d=1)0.909091(d=1)Pima0.753247(d=3)0.714286(d=7)0.649351(d=3)0.688312(d=7)0.753247(d=1)0.766234(d=1)0.753247(d=1)0.74026(d=1)0.753247(d=1)0.753247(d=1)0.753247(d=3)WDBC0.929825(d=7)0.912281(d=8)0.631579(d=22)0.859649(d=8)0.982456(d=1)0.982456(d=1)0.982456(d=1)0.982456(d=1)0.982456(d=1)0.929825(d=8)0.929825(d=1)Wine0.777778(d=4)0.944444(d=13)0.388889(d=1)0.722222(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)Wholesale1(d=3)0.954545(d=3)0.681818(d=3)0.704545(d=5)0.863636(d=1)0.863636(d=1)0.886364 (d=1)0.863636(d=1)0.909091(d=1)0.886364(d=1)0.931818(d=1)Table  STYLEREF 1 \s ‎0 SEQ Table \* ARABIC \s 1 6. Ranking of the results for the accuracy criterion based on the average of the 10-fold cross-validation using the 7-NN classifier.DatasetDimensionality ReductionProposed Method k-NN+SPCALLEKernel PCAAuto-endoderPCALDAMDSIsomapLLEKernel PCAAuto-encoderVehicle8911103631175Bupa981194144422Glass8810113733311Ionosphere511159159115Iris1111611665610KDD12435555555Monks6161119110611Newthyroid121122222222Pima2911102128222WDBC6911101111166Wine9811101111111Wholesale1211107757453Average Rank4.755.8333338.257.6666673.6666674.3333333.1666674.752.9166673.253.583333SensitivityTable  STYLEREF 1 \s ‎0 SEQ Table \* ARABIC \s 1 7. Comparison of the results for the sensitivity criterion using the 7-NN classifier.DatasetDimensionality ReductionProposed Method k-NN+SPCALLEKernel PCAAuto-endoderPCALDAMDSIsomapLLEKernel PCAAuto-encoderVehicle0.95 (d=13)0.75(d=4)1(d=1)0.95 (d=9)0.9(d=1)0.95(d=5)0.9(d=1)0.9(d=1)1(d=1)1(d=5)1(d=9)Bupa0.466667(d=3)0.666667(d=3)1(d=1)0.533333(d=5)0.4(d=1)0.4(d=3)0.4(d=1)0.4(d=1)0.4(d=1)0.466667(d=1)0.466667(d=5)Glass0.857143(d=1)0.857143(d=1)0.5(d=7)0.714286(d=1)0.571429(d=3)0.428571(d=1)0.571429(d=3)0.571429(d=3)0.571429(d=5)1(d=3)0.857143(d=1)Ionosphere0.956522(d=8)0.956522(d=22)0.913043(d=1)0.956522(d=15)0.913043(d=1)0.913043(d=1)0.913043(d=1)0.913043(d=1)0.956522(d=1)0.956522(d=1)0.913043(d=1)Iris1(d=1)1(d=1)1(d=1)1(d=1)1(d=2)0.8(d=1)1(d=2)1(d=3)1(d=2)1(d=1)1(d=2)KDD1(d=10)1(d=10)1(d=10)1(d=19)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)Monks1(d=5)1(d=1)0.833333(d=3)1(d=3)1(d=5)0.833333(d=3)1(d=5)1(d=5)0.833333(d=1)1(d=3)1(d=3)New-thyroid1(d=5)1(d=1)0.866667(d=1)1(d=5)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)Pima0.444444(d=3)0.444444(d=3)1(d=1)0.925926(d=3)0.407407(d=1)0.407407(d=1)0.407407(d=1)0.37037(d=1)0.407407(d=1)0.407407(d=1)0.407407(d=3)WDBC1(d=8)1(d=8)1(d=22)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)Wine0.85(d=1)0.833333(d=4)1(d=4)1(d=4)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)Wholesale1(d=3)0.966667(d=3)1(d=3)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=5)Table  STYLEREF 1 \s ‎0 SEQ Table \* ARABIC \s 1 8. Ranking of the results for the sensitivity criterion based on the average of the 10-fold cross-validation using the 7-NN classifier.DatasetDimensionality ReductionProposed Method k-NN+SPCALLEKernel PCAAuto-endoderPCALDAMDSIsomapLLEKernel PCAAuto-encoderVehicle511158588111Bupa42137777744Glass2210561166612Ionosphere11616666116Iris111111111111KDD11111111111Monks11911911911New-thyroid111111111111Pima331255511555WDBC11111111111Wine1011111111111Wholesale111111111111Average Rank2.5833333.8333333.6666671.9166673.254.9166673.253.752.9166671.5833332.083333SpecificityTable  STYLEREF 1 \s ‎0 SEQ Table \* ARABIC \s 1 9. Comparison of the results for the specificity criterion using the 7-NN classifier.DatasetDimensionality ReductionProposed Method k-NN+SPCALLEKernel PCAAuto-endoderPCALDAMDSIsomapLLEKernel PCAAuto-encoderVehicle0.969231 (d=13)0.892308(d=17)1 (d=5)0.830769(d=1)0.984615(d=1)0.984615(d=1)0.984615(d=1)1(d=5)0.938462(d=1)0.984615(d=1)0.984615(d=17)Bupa0.7 (d=1)0.65 (d=3)0(d=1)0.7(d=1)0.9(d=1)0.9(d=1)0.9(d=1)0.9(d=1)1(d=3)0.9(d=1)0.9(d=3)Glass0.666667 (d=7)0.733333(d=3)0.8(d=1)0.8(d=5)0.933333(d=1)0.933333(d=1)0.933333(d=1)0.933333(d=1)0.933333(d=3)0.933333(d=1)0.933333(d=9)Ionosphere0.769231(d=8)0.615385(d=15)1(d=8)0.769231(d=15)0.846154(d=22)0.923077(d=1)0.846154(d=22)0.769231(d=1)0.846154(d=8)0.846154(d=1)0.846154(d=1)Iris1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)0.9(d=1)KDD0.990654(d=10)0.943925(d=10)0.037383 (d=10)1(d=37)0(d=1)0(d=1)0(d=1)0(d=1)0(d=1)0(d=1)0(d=1)Monks0.833333(d=3)0(d=1)0.833333(d=1)1(d=3)0.833333(d=3)0.833333(d=1)0.833333(d=3)0.666667(d=3)0.833333(d=3)0.833333(d=3)0.833333(d=3)New-thyroid0.857143(d=3)0.714286(d=3)0.866667(d=1)0.857143(d=5)0.714286(d=1)0.714286(d=1)0.714286(d=1)0.714286(d=1)0.714286(d=1)0.714286(d=1)0.714286(d=1)Pima0.92(d=5)0.9(d=7)1(d=3)0.82(d=7)0.94(d=1)0.96(d=1)0.94(d=1)0.94(d=1)0.94(d=1)0.94(d=1)0.94(d=1)WDBC1(d=1)0.761905(d=1)1(d=1)0.761905(d=8)0.952381(d=1)0.952381(d=1)0.952381(d=1)0.952381(d=1)0.952381(d=1)0.809524(d=8)0.809524(d=1)Wine1(d=4)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)1(d=1)Wholesale1(d=3)0.928571(d=3)1(d=1)0.071429(d=5)0.642857(d=3)0.571429(d=3)0.642857(d=3)0.571429(d=1)0.714286(d=1)0.714286(d=1)0.857143(d=1)Table  STYLEREF 1 \s ‎0 SEQ Table \* ARABIC \s 1 10. Ranking of the results for the specificity criterion based on the average of the 10-fold cross-validation using the 7-NN classifier.DatasetDimensionality ReductionProposed Method k-NN+SPCALLEKernel PCAAuto-endoderPCALDAMDSIsomapLLEKernel PCAAuto-encoderVehicle8101113331933Bupa8101182222122Glass1110881111111Ionosphere811183238333Iris111111111111KDD23415555555Monks2112122210222New-thyroid24124444444Pima9101113233333WDBC1101103333388Wine11111111111Wholesale131117979554Average Rank4.572.756.0833332.9166672.9166672.91666743.1666673.1666673.916667Comparison of the average runtime of the k-NN, k-NN+S, and SVMTable  STYLEREF 1 \s ‎0 SEQ Table \* ARABIC \s 1 11. Comparison of the average execution time per test data on 10-fold over different datasets between the three classification methods k-NN, k-NN + S, and SVM in the proposed similarity space.ClassifierKNNk-NN+SSVMDatasetPCALDAMDSIsomapLLEKernelPCAAutoencoderPCALDAMDSIsomapLLEKernelPCAAutoencoderPCALDAMDSIsomapLLEKernelPCAAutoencoderVehicle0.0071420.0072020.0072630.0072880.0073520.0076030.0075593.82E-053.95E-054.00E-053.88E-054.38E-054.07E-053.93E-050.0035490.0037920.0037240.0039920.0034930.0032730.00312KDD0.0068960.0063150.0066760.0063040.0063170.0065730.0066613.78E-053.70E-053.82E-053.71E-053.79E-053.81E-053.70E-050.0018750.0017110.0018690.0017540.0019980.0017010.001733Bupa0.0006770.000660.0006590.0006580.0006620.0006610.000754.07E-053.89E-053.85E-053.86E-053.98E-053.84E-053.88E-050.0010350.0010610.0010320.0010150.0010620.0010760.001075Glass0.0003760.0003920.0003740.0003750.0003740.0004080.0003783.92E-054.13E-053.97E-053.98E-053.89E-054.23E-054.00E-050.0029310.0030980.0029480.0029220.0028850.0029790.00288Ionosphere0.0007110.00080.0006960.0007010.0007770.0007760.0008173.95E-054.40E-053.90E-053.87E-054.17E-054.27E-054.53E-050.0010650.0009370.0010260.0009970.0008930.0009990.001116Monks0.0002680.000230.0002550.0002490.0002380.0002450.0002424.54E-054.06E-053.98E-054.11E-053.90E-054.03E-054.01E-050.001830.0016690.0017460.0017260.0016830.0016840.001697New-thyroid0.0003870.0003810.0003820.000380.0003760.0003840.0003753.91E-053.98E-053.94E-053.99E-053.96E-053.99E-053.95E-050.0014940.0015220.0015090.0015260.0015490.0014510.001488Pima0.0058260.0057070.0057370.0057120.0057250.0057270.005723.89E-053.77E-053.86E-053.75E-053.83E-053.83E-053.81E-050.0018990.0019060.0018480.0019060.0018680.0019580.00184WDBC0.0028720.0030630.0027750.0028120.0028040.0029030.003043.84E-053.92E-053.84E-053.80E-053.84E-053.96E-054.02E-050.0010020.0009020.0009550.0009710.000970.0007250.001083Iris0.0004340.0003740.0003490.0003080.0003450.0003130.000274.45E-055.21E-054.53E-054.42E-054.69E-054.63E-053.98E-050.0021750.0021220.0021810.00210.0021350.0021680.001809Wine0.0003310.0003330.000350.0003240.0003330.0003140.0003764.01E-054.25E-054.29E-053.87E-054.13E-054.01E-054.61E-050.0017680.0018450.00180.0017450.0017750.0015380.001749Wholesale0.0009740.0009840.0010510.0009990.0009950.0009930.0010583.87E-053.91E-053.96E-053.92E-053.92E-053.91E-053.91E-050.0008460.0008320.0008540.0008510.0008480.00080.00085Average runtime0.0022410.0022030.0022140.0021760.0021920.0022420.0022714E-054.1E-053.99E-053.93E-054.04E-054.05E-054.03E-050.0017890.0017830.0017910.0017920.0017630.0016960.001703As you can see in  REF _Ref84810057 \h Table ‎011, the classification using the k-NN + S method by the proposed methods is significantly faster than the k-NN and SVM methods, because the k- method Instead of calculating the distance between each point and each other point, which is especially time consuming, especially on large data sets, NN + S uses the similarity matrix obtained by learning the distance criterion method. Uses a suggestion. Thus, another achievement of the proposed method in this research could be to provide a similarity space, which can make us needless to calculate the distance between points, and in cases where the classification speed is based on a large volume of Data is of great importance and can be very useful and efficient.